{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Head Word Final 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {}\n",
    "for run in api.runs(\n",
    "    path=\"ananthag/Head Word Final 2\"\n",
    "):\n",
    "    runs[run.config['model_name']] = run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37018"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.convert_tokens_to_ids('bite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeGenTokenizerFast(name_or_path='microsoft/phi-2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50257: AddedToken(\"                               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50258: AddedToken(\"                              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50259: AddedToken(\"                             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50260: AddedToken(\"                            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50261: AddedToken(\"                           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50262: AddedToken(\"                          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50263: AddedToken(\"                         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50264: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50265: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50266: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50267: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50268: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50269: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50270: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50271: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50272: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50273: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50274: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50275: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50276: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50277: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50278: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50279: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50280: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50281: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50282: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50283: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50284: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50285: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50286: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50287: AddedToken(\"\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50288: AddedToken(\"\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50289: AddedToken(\"\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50290: AddedToken(\"\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50291: AddedToken(\"\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50292: AddedToken(\"\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50293: AddedToken(\"\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50294: AddedToken(\"\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(4532)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.codegen.tokenization_codegen_fast.CodeGenTokenizerFast"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tok.encode(' adjust', add_prefix_space=False)\n",
    "type(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not tok.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"Hey what's your name\", \"Welcome home\", \"text\" * 20]\n",
    "tok.pad_token_id = tok.eos_token_id\n",
    "inputs_dict = tok.batch_encode_plus(inputs, padding=True, return_tensors='pt')\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "m = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "out = m(**inputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs_dict[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = inputs_dict[\"attention_mask\"]\n",
    "\n",
    "last_non_masked_idx = torch.sum(attn_mask, dim=1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.tensor(\n",
    "            [list(range(inputs.shape[1])) for i in range(inputs.shape[0])]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, position_ids_slice in enumerate(position_ids):\n",
    "    position_ids_slice[last_non_masked_idx[i] :] = position_ids_slice[\n",
    "        last_non_masked_idx[i]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(inputs.shape[1]).repeat(inputs.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[attn_mask] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected index [3, 20] to be smaller than self [3, 20] apart from dimension 0 and to be smaller size than src [3, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_non_masked_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m inputs\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected index [3, 20] to be smaller than self [3, 20] apart from dimension 0 and to be smaller size than src [3, 1]"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs.scatter_(0, 1 - attn_mask, last_non_masked_idx.unsqueeze(1))\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs * attn_mask + (~attn_mask.bool() * last_non_masked_idx.unsqueeze(1).expand(-1, inputs.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10814,   644,   338,   534,  1438,     4,     4,     4,     4,     4,\n",
       "             4,     4,     4,     4,     4,     4,     4,     4,     4,     4],\n",
       "        [14618,  1363,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [ 5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,\n",
       "          5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "from dataset import DepParseDataPickle\n",
    "\n",
    "d = dataset.HeadWordDatasetWithRelns('test', 'gpt2', 12, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2416"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.start_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58223"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.start_indices[2415]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58236, 12, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.hidden_state_cache.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.70710678],\n",
       "       [0.70710678, 1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "X = [[0, 0, 0], [1, 1, 1]]\n",
    "Y = [[1, 0, 0], [1, 1, 0]]\n",
    "cosine_similarity(Y, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ran = np.random.rand(3, 3)\n",
    "ran[np.eye(3, 3, dtype=bool)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "counter = collections.Counter()\n",
    "models = set()\n",
    "with open('surprisals_errata.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        counter[(row['file_name'], row['item_number'])] += 1\n",
    "        models.add(row['model'])\n",
    "\n",
    "len(models)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('reflexive_src_fem.json', '6'), 15),\n",
       " (('reflexive_src_fem.json', '7'), 15),\n",
       " (('reflexive_src_fem.json', '13'), 15),\n",
       " (('reflexive_src_fem.json', '16'), 15),\n",
       " (('reflexive_prep_fem.json', '1'), 15),\n",
       " (('reflexive_prep_fem.json', '2'), 15),\n",
       " (('reflexive_src_fem.json', '1'), 14),\n",
       " (('reflexive_src_fem.json', '5'), 14),\n",
       " (('reflexive_src_fem.json', '15'), 14),\n",
       " (('reflexive_src_fem.json', '19'), 14),\n",
       " (('reflexive_prep_fem.json', '17'), 14),\n",
       " (('fgd-embed3.json', '4'), 14),\n",
       " (('fgd_subject.json', '4'), 14),\n",
       " (('fgd_subject.json', '5'), 14),\n",
       " (('fgd_subject.json', '9'), 14),\n",
       " (('fgd_subject.json', '10'), 14),\n",
       " (('mvrr.json', '5'), 14),\n",
       " (('mvrr_mod.json', '5'), 14),\n",
       " (('number_orc.json', '5'), 14),\n",
       " (('number_orc.json', '17'), 14),\n",
       " (('number_prep.json', '5'), 14),\n",
       " (('number_prep.json', '17'), 14),\n",
       " (('number_src.json', '5'), 14),\n",
       " (('number_src.json', '17'), 14),\n",
       " (('reflexive_orc_fem.json', '1'), 14),\n",
       " (('reflexive_orc_fem.json', '5'), 14),\n",
       " (('reflexive_orc_fem.json', '6'), 14),\n",
       " (('reflexive_orc_fem.json', '7'), 14),\n",
       " (('reflexive_orc_fem.json', '15'), 14),\n",
       " (('reflexive_src_fem.json', '9'), 14),\n",
       " (('fgd_subject.json', '12'), 14),\n",
       " (('reflexive_src_fem.json', '2'), 13),\n",
       " (('reflexive_src_fem.json', '17'), 13),\n",
       " (('reflexive_prep_fem.json', '15'), 13),\n",
       " (('fgd-embed4.json', '15'), 13),\n",
       " (('fgd-embed4.json', '16'), 13),\n",
       " (('number_orc.json', '16'), 13),\n",
       " (('reflexive_orc_fem.json', '11'), 13),\n",
       " (('reflexive_orc_fem.json', '17'), 13),\n",
       " (('fgd_hierarchy.json', '1'), 13),\n",
       " (('reflexive_prep_masc.json', '2'), 13),\n",
       " (('mvrr_mod.json', '16'), 13),\n",
       " (('reflexive_src_masc.json', '9'), 13),\n",
       " (('reflexive_prep_fem.json', '7'), 12),\n",
       " (('reflexive_prep_fem.json', '16'), 12),\n",
       " (('fgd_subject.json', '6'), 12),\n",
       " (('mvrr.json', '14'), 12),\n",
       " (('number_orc.json', '3'), 12),\n",
       " (('reflexive_orc_fem.json', '13'), 12),\n",
       " (('fgd_pp.json', '6'), 12),\n",
       " (('mvrr.json', '20'), 12),\n",
       " (('mvrr.json', '16'), 12),\n",
       " (('fgd-embed3.json', '16'), 11),\n",
       " (('subordination.json', '12'), 11),\n",
       " (('mvrr_mod.json', '19'), 11),\n",
       " (('number_src.json', '3'), 11),\n",
       " (('reflexive_orc_fem.json', '16'), 11),\n",
       " (('reflexive_orc_fem.json', '19'), 11),\n",
       " (('npi_src_any.json', '38'), 11),\n",
       " (('fgd_subject.json', '7'), 11),\n",
       " (('fgd_subject.json', '19'), 11),\n",
       " (('fgd_hierarchy.json', '19'), 10),\n",
       " (('subordination_src-src.json', '9'), 10),\n",
       " (('reflexive_prep_fem.json', '5'), 10),\n",
       " (('fgd-embed3.json', '21'), 10),\n",
       " (('subordination.json', '9'), 10),\n",
       " (('subordination.json', '11'), 10),\n",
       " (('subordination.json', '19'), 10),\n",
       " (('fgd-embed4.json', '3'), 10),\n",
       " (('fgd-embed4.json', '4'), 10),\n",
       " (('fgd-embed4.json', '9'), 10),\n",
       " (('fgd-embed4.json', '21'), 10),\n",
       " (('reflexive_src_masc.json', '19'), 10),\n",
       " (('fgd_object.json', '16'), 10),\n",
       " (('reflexive_src_fem.json', '14'), 10),\n",
       " (('reflexive_prep_fem.json', '9'), 10),\n",
       " (('reflexive_prep_fem.json', '8'), 10),\n",
       " (('fgd_subject.json', '11'), 10),\n",
       " (('fgd_subject.json', '23'), 10),\n",
       " (('reflexive_src_fem.json', '4'), 9),\n",
       " (('reflexive_prep_fem.json', '6'), 9),\n",
       " (('reflexive_prep_fem.json', '13'), 9),\n",
       " (('fgd-embed3.json', '9'), 9),\n",
       " (('subordination.json', '16'), 9),\n",
       " (('fgd_subject.json', '1'), 9),\n",
       " (('mvrr.json', '19'), 9),\n",
       " (('reflexive_src_masc.json', '6'), 9),\n",
       " (('reflexive_src_masc.json', '11'), 9),\n",
       " (('number_src.json', '15'), 9),\n",
       " (('reflexive_orc_fem.json', '8'), 9),\n",
       " (('reflexive_orc_fem.json', '9'), 9),\n",
       " (('fgd_hierarchy.json', '4'), 9),\n",
       " (('fgd_hierarchy.json', '12'), 9),\n",
       " (('fgd-embed3.json', '6'), 9),\n",
       " (('fgd-embed3.json', '8'), 9),\n",
       " (('reflexive_prep_masc.json', '9'), 9),\n",
       " (('fgd-embed4.json', '12'), 9),\n",
       " (('fgd-embed4.json', '8'), 9),\n",
       " (('fgd_hierarchy.json', '7'), 9),\n",
       " (('fgd_hierarchy.json', '11'), 8)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    GPTNeoXTokenizerFast,\n",
    "    LlamaTokenizer,\n",
    "    GPT2Tokenizer,\n",
    "    GemmaTokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaTokenizerFast(name_or_path='google/gemma-2b', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [\"cat\", \"dog\", \"cow\", \"bird\"]\n",
    "model_name = 'google/gemma-2b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast='pythia' in 'google')\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# batch = tokenizer((\"one word\", \"two two two words\", \"three three three three three words\"), padding=True, return_tensors='pt')\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n",
    "# output = model(**batch)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  505,  1573, 50256, 50256, 50256, 50256],\n",
       "        [11545,   734,   734,  2456, 50256, 50256],\n",
       "        [15542,  1115,  1115,  1115,  1115,  2456]]), 'attention_mask': tensor([[1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  505,  1573, 50256, 50256, 50256, 50256],\n",
       "        [11545,   734,   734,  2456, 50256, 50256],\n",
       "        [15542,  1115,  1115,  1115,  1115,  2456]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.attention_mask.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-17.2175, -27.2265, -33.0799], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "targets = torch.cat((batch[\"input_ids\"][:, 1:], torch.tensor([tokenizer.eos_token_id]).expand(batch[\"input_ids\"].shape[0], 1)), dim=1)\n",
    "out = torch.gather(F.log_softmax(output.logits, dim=-1), -1, targets.unsqueeze(-1))\n",
    "(out * batch.attention_mask.unsqueeze(-1)).view(batch.input_ids.shape[0], -1).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['animate_subject_passive.jsonl',\n",
       " 'principle_A_domain_3.jsonl',\n",
       " 'npi_present_1.jsonl',\n",
       " 'wh_vs_that_no_gap_long_distance.jsonl',\n",
       " 'existential_there_quantifiers_1.jsonl',\n",
       " 'determiner_noun_agreement_irregular_1.jsonl',\n",
       " 'ellipsis_n_bar_1.jsonl',\n",
       " 'causative.jsonl',\n",
       " 'wh_questions_object_gap.jsonl',\n",
       " 'transitive.jsonl',\n",
       " 'superlative_quantifiers_1.jsonl',\n",
       " 'tough_vs_raising_2.jsonl',\n",
       " 'principle_A_case_1.jsonl',\n",
       " 'expletive_it_object_raising.jsonl',\n",
       " 'irregular_plural_subject_verb_agreement_2.jsonl',\n",
       " 'principle_A_domain_2.jsonl',\n",
       " 'sentential_negation_npi_scope.jsonl',\n",
       " 'ellipsis_n_bar_2.jsonl',\n",
       " 'determiner_noun_agreement_irregular_2.jsonl',\n",
       " 'left_branch_island_echo_question.jsonl',\n",
       " 'existential_there_quantifiers_2.jsonl',\n",
       " 'wh_island.jsonl',\n",
       " 'superlative_quantifiers_2.jsonl',\n",
       " 'principle_A_domain_1.jsonl',\n",
       " 'irregular_plural_subject_verb_agreement_1.jsonl',\n",
       " 'distractor_agreement_relative_clause.jsonl',\n",
       " 'existential_there_subject_raising.jsonl',\n",
       " 'tough_vs_raising_1.jsonl',\n",
       " 'principle_A_case_2.jsonl',\n",
       " 'coordinate_structure_constraint_object_extraction.jsonl',\n",
       " 'wh_vs_that_no_gap.jsonl',\n",
       " 'anaphor_gender_agreement.jsonl',\n",
       " 'determiner_noun_agreement_with_adj_2.jsonl',\n",
       " 'only_npi_scope.jsonl',\n",
       " 'inchoative.jsonl',\n",
       " 'wh_vs_that_with_gap_long_distance.jsonl',\n",
       " 'npi_present_2.jsonl',\n",
       " 'coordinate_structure_constraint_complex_left_branch.jsonl',\n",
       " 'left_branch_island_simple_question.jsonl',\n",
       " 'determiner_noun_agreement_with_adj_irregular_2.jsonl',\n",
       " 'determiner_noun_agreement_2.jsonl',\n",
       " 'wh_vs_that_with_gap.jsonl',\n",
       " 'regular_plural_subject_verb_agreement_1.jsonl',\n",
       " 'passive_2.jsonl',\n",
       " 'anaphor_number_agreement.jsonl',\n",
       " 'animate_subject_trans.jsonl',\n",
       " 'intransitive.jsonl',\n",
       " 'irregular_past_participle_verbs.jsonl',\n",
       " 'only_npi_licensor_present.jsonl',\n",
       " 'passive_1.jsonl',\n",
       " 'sentential_subject_island.jsonl',\n",
       " 'regular_plural_subject_verb_agreement_2.jsonl',\n",
       " 'principle_A_c_command.jsonl',\n",
       " 'wh_questions_subject_gap.jsonl',\n",
       " 'sentential_negation_npi_licensor_present.jsonl',\n",
       " 'principle_A_reconstruction.jsonl',\n",
       " 'determiner_noun_agreement_with_adjective_1.jsonl',\n",
       " 'drop_argument.jsonl',\n",
       " 'existential_there_object_raising.jsonl',\n",
       " 'irregular_past_participle_adjectives.jsonl',\n",
       " 'adjunct_island.jsonl',\n",
       " 'matrix_question_npi_licensor_present.jsonl',\n",
       " 'distractor_agreement_relational_noun.jsonl',\n",
       " 'complex_NP_island.jsonl',\n",
       " 'wh_questions_subject_gap_long_distance.jsonl',\n",
       " 'determiner_noun_agreement_1.jsonl',\n",
       " 'determiner_noun_agreement_with_adj_irregular_1.jsonl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../blimp/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.4905, -6.7212, -6.6658, -6.3376], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = torch.cat(\n",
    "    (\n",
    "        batch.input_ids[:, 1:],\n",
    "        torch.tensor([[tokenizer.eos_token_id]]).expand(\n",
    "            batch.input_ids.shape[0], 1\n",
    "        ),\n",
    "    ),\n",
    "    dim=-1,\n",
    ")\n",
    "logprobs = torch.gather(\n",
    "    torch.nn.functional.log_softmax(output.logits, dim=-1),\n",
    "    dim=-1,\n",
    "    index=targets.unsqueeze(1),\n",
    ").reshape(-1)\n",
    "logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(logprobs > logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256],\n",
       "        [50256],\n",
       "        [50256],\n",
       "        [50256]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[tokenizer.eos_token_id]]).expand(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92989cfd2446428999e353de76b84785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaConfig {\n",
       "  \"_name_or_path\": \"google/gemma-2b\",\n",
       "  \"architectures\": [\n",
       "    \"GemmaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"head_dim\": 256,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 16384,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"model_type\": \"gemma\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 18,\n",
       "  \"num_key_value_heads\": 1,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 256000\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModelForCausalLM.from_pretrained('google/gemma-2b').config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.41.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191cfe3dc3db42589749e788d9dfce4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db203ad8506467aa419b0af9a628583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733c0e0d90ec490b97f8cbd93917276e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254e59aead874acc995a2ad10194db98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56dc4b34438644c1b297ed0c6e630f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0271308e33a041a7943394d116f80d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "olmo = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-1B-hf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [12092, 1533], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1533], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\" world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n",
      "Pseudoperplexity: -18.2569522857666\n",
      "torch.Size([1, 6])\n",
      "Pseudoperplexity: -30.995452880859375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_pseudoperplexity(model, tokenizer, text):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids']\n",
    "    print(input_ids.shape)\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    # Get the length of the sequence\n",
    "    seq_length = input_ids.size(1)\n",
    "    \n",
    "    # Create batched inputs with each token masked once\n",
    "    masked_input_ids = input_ids.repeat(seq_length, 1)\n",
    "    \n",
    "    masked_attention_mask = attention_mask.repeat(seq_length, 1)\n",
    "    masked_input_ids.fill_diagonal_(tokenizer.mask_token_id)\n",
    "    \n",
    "    # Get the model's predictions for the batched masked inputs\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(masked_input_ids, attention_mask=masked_attention_mask)\n",
    "    \n",
    "    # Extract the logits and compute the log softmax to get log probabilities\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the actual token IDs\n",
    "    actual_token_ids = input_ids[0]\n",
    "\n",
    "    # Extract the log probabilities of the actual tokens\n",
    "    token_log_probs = log_probs[torch.arange(seq_length), torch.arange(seq_length), actual_token_ids]\n",
    "    \n",
    "    # Compute the sum of log probabilities\n",
    "    return token_log_probs.sum().item()\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'FacebookAI/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Example text\n",
    "text = \"The quick brown fox jumped.\"\n",
    "\n",
    "# Compute pseudoperplexity\n",
    "print(f\"Pseudoperplexity: {compute_pseudoperplexity(model, tokenizer, 'He is cool.')}\")\n",
    "print(f\"Pseudoperplexity: {compute_pseudoperplexity(model, tokenizer, 'He are cool.')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40bfb0012ea4b8b85f3ce3d2f4ba977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b276f07a1be4c7a950452c8cc120820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, DebertaV2ForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained('microsoft/deberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta.embeddings.word_embeddings.weight\n",
      "deberta.embeddings.LayerNorm.weight\n",
      "deberta.embeddings.LayerNorm.bias\n",
      "deberta.encoder.layer.0.attention.self.q_bias\n",
      "deberta.encoder.layer.0.attention.self.v_bias\n",
      "deberta.encoder.layer.0.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.0.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.0.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.0.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.0.attention.output.dense.weight\n",
      "deberta.encoder.layer.0.attention.output.dense.bias\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.0.intermediate.dense.weight\n",
      "deberta.encoder.layer.0.intermediate.dense.bias\n",
      "deberta.encoder.layer.0.output.dense.weight\n",
      "deberta.encoder.layer.0.output.dense.bias\n",
      "deberta.encoder.layer.0.output.LayerNorm.weight\n",
      "deberta.encoder.layer.0.output.LayerNorm.bias\n",
      "deberta.encoder.layer.1.attention.self.q_bias\n",
      "deberta.encoder.layer.1.attention.self.v_bias\n",
      "deberta.encoder.layer.1.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.1.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.1.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.1.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.1.attention.output.dense.weight\n",
      "deberta.encoder.layer.1.attention.output.dense.bias\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.1.intermediate.dense.weight\n",
      "deberta.encoder.layer.1.intermediate.dense.bias\n",
      "deberta.encoder.layer.1.output.dense.weight\n",
      "deberta.encoder.layer.1.output.dense.bias\n",
      "deberta.encoder.layer.1.output.LayerNorm.weight\n",
      "deberta.encoder.layer.1.output.LayerNorm.bias\n",
      "deberta.encoder.layer.2.attention.self.q_bias\n",
      "deberta.encoder.layer.2.attention.self.v_bias\n",
      "deberta.encoder.layer.2.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.2.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.2.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.2.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.2.attention.output.dense.weight\n",
      "deberta.encoder.layer.2.attention.output.dense.bias\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.2.intermediate.dense.weight\n",
      "deberta.encoder.layer.2.intermediate.dense.bias\n",
      "deberta.encoder.layer.2.output.dense.weight\n",
      "deberta.encoder.layer.2.output.dense.bias\n",
      "deberta.encoder.layer.2.output.LayerNorm.weight\n",
      "deberta.encoder.layer.2.output.LayerNorm.bias\n",
      "deberta.encoder.layer.3.attention.self.q_bias\n",
      "deberta.encoder.layer.3.attention.self.v_bias\n",
      "deberta.encoder.layer.3.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.3.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.3.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.3.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.3.attention.output.dense.weight\n",
      "deberta.encoder.layer.3.attention.output.dense.bias\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.3.intermediate.dense.weight\n",
      "deberta.encoder.layer.3.intermediate.dense.bias\n",
      "deberta.encoder.layer.3.output.dense.weight\n",
      "deberta.encoder.layer.3.output.dense.bias\n",
      "deberta.encoder.layer.3.output.LayerNorm.weight\n",
      "deberta.encoder.layer.3.output.LayerNorm.bias\n",
      "deberta.encoder.layer.4.attention.self.q_bias\n",
      "deberta.encoder.layer.4.attention.self.v_bias\n",
      "deberta.encoder.layer.4.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.4.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.4.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.4.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.4.attention.output.dense.weight\n",
      "deberta.encoder.layer.4.attention.output.dense.bias\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.4.intermediate.dense.weight\n",
      "deberta.encoder.layer.4.intermediate.dense.bias\n",
      "deberta.encoder.layer.4.output.dense.weight\n",
      "deberta.encoder.layer.4.output.dense.bias\n",
      "deberta.encoder.layer.4.output.LayerNorm.weight\n",
      "deberta.encoder.layer.4.output.LayerNorm.bias\n",
      "deberta.encoder.layer.5.attention.self.q_bias\n",
      "deberta.encoder.layer.5.attention.self.v_bias\n",
      "deberta.encoder.layer.5.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.5.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.5.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.5.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.5.attention.output.dense.weight\n",
      "deberta.encoder.layer.5.attention.output.dense.bias\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.5.intermediate.dense.weight\n",
      "deberta.encoder.layer.5.intermediate.dense.bias\n",
      "deberta.encoder.layer.5.output.dense.weight\n",
      "deberta.encoder.layer.5.output.dense.bias\n",
      "deberta.encoder.layer.5.output.LayerNorm.weight\n",
      "deberta.encoder.layer.5.output.LayerNorm.bias\n",
      "deberta.encoder.layer.6.attention.self.q_bias\n",
      "deberta.encoder.layer.6.attention.self.v_bias\n",
      "deberta.encoder.layer.6.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.6.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.6.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.6.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.6.attention.output.dense.weight\n",
      "deberta.encoder.layer.6.attention.output.dense.bias\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.6.intermediate.dense.weight\n",
      "deberta.encoder.layer.6.intermediate.dense.bias\n",
      "deberta.encoder.layer.6.output.dense.weight\n",
      "deberta.encoder.layer.6.output.dense.bias\n",
      "deberta.encoder.layer.6.output.LayerNorm.weight\n",
      "deberta.encoder.layer.6.output.LayerNorm.bias\n",
      "deberta.encoder.layer.7.attention.self.q_bias\n",
      "deberta.encoder.layer.7.attention.self.v_bias\n",
      "deberta.encoder.layer.7.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.7.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.7.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.7.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.7.attention.output.dense.weight\n",
      "deberta.encoder.layer.7.attention.output.dense.bias\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.7.intermediate.dense.weight\n",
      "deberta.encoder.layer.7.intermediate.dense.bias\n",
      "deberta.encoder.layer.7.output.dense.weight\n",
      "deberta.encoder.layer.7.output.dense.bias\n",
      "deberta.encoder.layer.7.output.LayerNorm.weight\n",
      "deberta.encoder.layer.7.output.LayerNorm.bias\n",
      "deberta.encoder.layer.8.attention.self.q_bias\n",
      "deberta.encoder.layer.8.attention.self.v_bias\n",
      "deberta.encoder.layer.8.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.8.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.8.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.8.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.8.attention.output.dense.weight\n",
      "deberta.encoder.layer.8.attention.output.dense.bias\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.8.intermediate.dense.weight\n",
      "deberta.encoder.layer.8.intermediate.dense.bias\n",
      "deberta.encoder.layer.8.output.dense.weight\n",
      "deberta.encoder.layer.8.output.dense.bias\n",
      "deberta.encoder.layer.8.output.LayerNorm.weight\n",
      "deberta.encoder.layer.8.output.LayerNorm.bias\n",
      "deberta.encoder.layer.9.attention.self.q_bias\n",
      "deberta.encoder.layer.9.attention.self.v_bias\n",
      "deberta.encoder.layer.9.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.9.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.9.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.9.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.9.attention.output.dense.weight\n",
      "deberta.encoder.layer.9.attention.output.dense.bias\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.9.intermediate.dense.weight\n",
      "deberta.encoder.layer.9.intermediate.dense.bias\n",
      "deberta.encoder.layer.9.output.dense.weight\n",
      "deberta.encoder.layer.9.output.dense.bias\n",
      "deberta.encoder.layer.9.output.LayerNorm.weight\n",
      "deberta.encoder.layer.9.output.LayerNorm.bias\n",
      "deberta.encoder.layer.10.attention.self.q_bias\n",
      "deberta.encoder.layer.10.attention.self.v_bias\n",
      "deberta.encoder.layer.10.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.10.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.10.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.10.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.10.attention.output.dense.weight\n",
      "deberta.encoder.layer.10.attention.output.dense.bias\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.10.intermediate.dense.weight\n",
      "deberta.encoder.layer.10.intermediate.dense.bias\n",
      "deberta.encoder.layer.10.output.dense.weight\n",
      "deberta.encoder.layer.10.output.dense.bias\n",
      "deberta.encoder.layer.10.output.LayerNorm.weight\n",
      "deberta.encoder.layer.10.output.LayerNorm.bias\n",
      "deberta.encoder.layer.11.attention.self.q_bias\n",
      "deberta.encoder.layer.11.attention.self.v_bias\n",
      "deberta.encoder.layer.11.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.11.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.11.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.11.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.11.attention.output.dense.weight\n",
      "deberta.encoder.layer.11.attention.output.dense.bias\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.11.intermediate.dense.weight\n",
      "deberta.encoder.layer.11.intermediate.dense.bias\n",
      "deberta.encoder.layer.11.output.dense.weight\n",
      "deberta.encoder.layer.11.output.dense.bias\n",
      "deberta.encoder.layer.11.output.LayerNorm.weight\n",
      "deberta.encoder.layer.11.output.LayerNorm.bias\n",
      "deberta.encoder.rel_embeddings.weight\n",
      "cls.predictions.bias\n",
      "cls.predictions.transform.dense.weight\n",
      "cls.predictions.transform.dense.bias\n",
      "cls.predictions.transform.LayerNorm.weight\n",
      "cls.predictions.transform.LayerNorm.bias\n",
      "cls.predictions.decoder.weight\n",
      "cls.predictions.decoder.bias\n"
     ]
    }
   ],
   "source": [
    "for k in model.state_dict().keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scr/biggest/ananthag\n"
     ]
    }
   ],
   "source": [
    "!echo $HF_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6d219d74254083b5664f333b4609c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e0815223bb4b69b5931bc1b244c3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoModelForMaskedLM\n",
    "\n",
    "roberta = AutoModelForMaskedLM.from_pretrained('FacebookAI/roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1476, -0.0365,  0.0753,  ..., -0.0023,  0.0172, -0.0016],\n",
       "        [ 0.0156,  0.0076, -0.0118,  ..., -0.0022,  0.0081, -0.0156],\n",
       "        [-0.0347, -0.0873, -0.0180,  ...,  0.1174, -0.0098, -0.0355],\n",
       "        ...,\n",
       "        [ 0.0304,  0.0504, -0.0307,  ...,  0.0377,  0.0096,  0.0084],\n",
       "        [ 0.0623, -0.0596,  0.0307,  ..., -0.0920,  0.1080, -0.0183],\n",
       "        [ 0.1259, -0.0145,  0.0332,  ...,  0.0121,  0.0342,  0.0168]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta.lm_head.decoder.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "state_dict = torch.load('/scr/ananthag/hub/models--microsoft--deberta-v3-xsmall/snapshots/4b419818330868dff6a60ad3e6b1c730f8b8c0c6/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta.embeddings.word_embeddings._weight torch.Size([128100, 384])\n",
      "deberta.embeddings.word_embeddings.weight torch.Size([128100, 384])\n",
      "deberta.embeddings.position_embeddings._weight torch.Size([512, 384])\n",
      "deberta.embeddings.position_embeddings.weight torch.Size([512, 384])\n",
      "deberta.embeddings.LayerNorm.weight torch.Size([384])\n",
      "deberta.embeddings.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.0.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.0.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.0.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.1.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.1.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.1.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.2.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.2.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.2.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.3.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.3.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.3.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.4.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.4.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.4.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.5.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.5.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.5.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.6.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.6.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.6.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.7.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.7.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.7.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.8.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.8.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.8.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.9.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.9.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.9.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.10.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.10.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.10.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.11.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.11.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.11.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.rel_embeddings.weight torch.Size([512, 384])\n",
      "deberta.encoder.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.LayerNorm.bias torch.Size([384])\n",
      "lm_predictions.lm_head.bias torch.Size([128100])\n",
      "lm_predictions.lm_head.dense.weight torch.Size([384, 384])\n",
      "lm_predictions.lm_head.dense.bias torch.Size([384])\n",
      "lm_predictions.lm_head.LayerNorm.weight torch.Size([384])\n",
      "lm_predictions.lm_head.LayerNorm.bias torch.Size([384])\n",
      "mask_predictions.dense.weight torch.Size([384, 384])\n",
      "mask_predictions.dense.bias torch.Size([384])\n",
      "mask_predictions.LayerNorm.weight torch.Size([384])\n",
      "mask_predictions.LayerNorm.bias torch.Size([384])\n",
      "mask_predictions.classifier.weight torch.Size([1, 384])\n",
      "mask_predictions.classifier.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for k, v in state_dict.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta.embeddings.word_embeddings.weight torch.Size([128100, 384])\n",
      "deberta.embeddings.LayerNorm.weight torch.Size([384])\n",
      "deberta.embeddings.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.0.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.0.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.0.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.1.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.1.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.1.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.2.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.2.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.2.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.3.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.3.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.3.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.4.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.4.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.4.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.5.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.5.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.5.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.6.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.6.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.6.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.7.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.7.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.7.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.8.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.8.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.8.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.9.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.9.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.9.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.10.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.10.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.10.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.11.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.11.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.11.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.rel_embeddings.weight torch.Size([512, 384])\n",
      "deberta.encoder.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.LayerNorm.bias torch.Size([384])\n",
      "cls.predictions.bias torch.Size([128100])\n",
      "cls.predictions.transform.dense.weight torch.Size([384, 384])\n",
      "cls.predictions.transform.dense.bias torch.Size([384])\n",
      "cls.predictions.transform.LayerNorm.weight torch.Size([384])\n",
      "cls.predictions.transform.LayerNorm.bias torch.Size([384])\n",
      "cls.predictions.decoder.weight torch.Size([128100, 384])\n",
      "cls.predictions.decoder.bias torch.Size([128100])\n"
     ]
    }
   ],
   "source": [
    "for k, v in deberta.state_dict().items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.bias torch.Size([128100])\n",
      "predictions.transform.dense.weight torch.Size([384, 384])\n",
      "predictions.transform.dense.bias torch.Size([384])\n",
      "predictions.transform.LayerNorm.weight torch.Size([384])\n",
      "predictions.transform.LayerNorm.bias torch.Size([384])\n",
      "predictions.decoder.weight torch.Size([128100, 384])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.cls.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ef8c9282c64fb28b318114dc3248e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef13e7f92cf47ffa8d9d0fb53168ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196587104f2f462389b97177784fd537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello <extra_id_0>\", return_tensors='pt', padding=True)\n",
    "labels = tokenizer(\"this is <extra_id_0> world\", return_tensors='pt', padding=True).input_ids\n",
    "output = model(**inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(32099)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 32128])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=tensor(6.2130, grad_fn=<NllLossBackward0>), logits=tensor([[[-43.1238, -12.7323, -26.8814,  ..., -66.6852, -66.6937, -66.6066],\n",
       "         [-27.5227,  -5.1289,  -6.9508,  ..., -41.1360, -41.1700, -41.1575],\n",
       "         [-28.0383,  -6.3891,  -5.8660,  ..., -40.2829, -40.2964, -40.3289]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[-0.4865, -2.3323, -1.1428,  ..., -2.5693, -1.7539, -0.5693],\n",
       "          [-0.6358, -1.1153, -0.3137,  ..., -0.7512, -0.5316, -0.5639],\n",
       "          [ 1.6587,  3.2717, -2.2787,  ...,  0.3732, -1.0274,  0.8426]],\n",
       "\n",
       "         [[-0.9565, -2.3581, -0.2478,  ...,  1.1193,  0.7770,  0.5377],\n",
       "          [ 2.1201, -1.6266, -0.7735,  ...,  1.0306,  1.5982,  6.4674],\n",
       "          [ 1.4973, -0.2845, -0.7443,  ...,  0.9890,  1.3783, -1.7426]],\n",
       "\n",
       "         [[ 1.0704, -2.5709,  0.6892,  ..., -1.3349,  0.2768,  0.5756],\n",
       "          [ 0.8899, -3.0563,  0.7858,  ...,  0.4327, -0.8198,  1.0157],\n",
       "          [-0.1107, -0.7388,  0.5365,  ...,  1.1146,  0.7549, -1.7762]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.6660,  2.2657,  0.3124,  ...,  1.4158, -2.9123, -1.1275],\n",
       "          [-0.0994,  0.4854,  0.9066,  ...,  1.2753, -0.2312,  0.2930],\n",
       "          [-0.4136, -0.1592, -1.7534,  ...,  4.3566, -0.1560,  0.4038]],\n",
       "\n",
       "         [[-6.0374, -1.1453, -1.5700,  ..., -1.3067, -0.1885,  1.6042],\n",
       "          [-4.4308, -0.8538, -1.4613,  ..., -0.2507, -0.0910,  1.5712],\n",
       "          [-1.0759,  0.1320,  0.3018,  ..., -1.3452,  0.8117,  0.6980]],\n",
       "\n",
       "         [[ 0.0485, -0.0356, -0.0691,  ...,  0.6128, -0.2485, -0.2954],\n",
       "          [ 0.3269,  0.0944,  0.8320,  ...,  1.3678,  0.6110,  0.6745],\n",
       "          [-0.2185,  0.1111, -0.4495,  ...,  0.5399,  0.2215,  1.8020]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 2.4430e-01,  3.2305e-01, -2.2954e-01,  ...,  7.0325e-01,\n",
       "            2.8494e-01,  1.7667e-01],\n",
       "          [ 6.0846e-01,  8.1564e-01, -6.2543e-01,  ..., -1.3925e-03,\n",
       "           -6.2205e-01,  2.1129e-01],\n",
       "          [-3.1442e-01,  1.2729e+00,  9.5270e-01,  ...,  1.2199e+00,\n",
       "            1.6016e-01,  3.5790e-01]],\n",
       "\n",
       "         [[-1.6416e-01,  5.7965e-01, -4.2177e-01,  ..., -5.3154e-02,\n",
       "           -9.0629e-02, -2.6091e-01],\n",
       "          [ 1.2116e+00,  5.1494e-01,  6.0132e-01,  ...,  3.8335e-01,\n",
       "            1.9411e+00, -2.2539e-01],\n",
       "          [-8.6819e-01,  8.5467e-01,  6.7290e-01,  ...,  8.6678e-01,\n",
       "           -3.2779e-01, -8.6272e-01]],\n",
       "\n",
       "         [[ 1.3425e-01,  2.9316e-01, -5.6067e-02,  ..., -8.8016e-02,\n",
       "           -8.8274e-01,  1.0571e-01],\n",
       "          [-2.5734e-01, -3.3990e-01,  4.1947e-01,  ..., -2.5694e-01,\n",
       "           -5.3157e-01,  8.5176e-02],\n",
       "          [ 5.2256e-01,  2.2336e-01, -5.2537e-03,  ...,  1.2392e+00,\n",
       "           -1.2696e+00, -4.4177e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.2518e-01, -2.0497e-01, -2.9561e-02,  ..., -2.6988e-01,\n",
       "            9.7269e-02, -1.0568e-01],\n",
       "          [-1.0319e-01,  1.9930e-01, -6.2021e-02,  ..., -7.7206e-02,\n",
       "            7.8045e-02,  1.3173e-03],\n",
       "          [-1.1283e+00, -1.6303e+00, -1.5670e+00,  ...,  1.1173e+00,\n",
       "            3.1469e-02,  1.2299e+00]],\n",
       "\n",
       "         [[-2.3741e-01, -4.8000e-02,  7.8948e-03,  ...,  5.3948e-02,\n",
       "            2.7783e-02, -1.5002e-01],\n",
       "          [ 9.2761e-01, -4.8887e-01, -2.0265e-01,  ...,  1.5016e-01,\n",
       "            6.7017e-02, -4.1232e-01],\n",
       "          [-9.4093e-01,  2.0756e-01,  1.6496e+00,  ...,  1.6214e-01,\n",
       "            1.0549e+00, -9.8243e-02]],\n",
       "\n",
       "         [[ 5.3252e-02, -7.6834e-01,  2.6697e-01,  ...,  2.2539e-01,\n",
       "           -1.8973e-01,  6.1827e-02],\n",
       "          [ 1.2374e-01, -5.6097e-02, -4.0457e-01,  ...,  1.0049e-01,\n",
       "            7.8244e-02, -9.5537e-02],\n",
       "          [-4.9552e-01, -1.2621e+00, -1.0584e+00,  ..., -2.2819e+00,\n",
       "            8.5126e-01, -2.6877e+00]]]], grad_fn=<TransposeBackward0>), tensor([[[[-5.4957e-02, -1.9532e-01, -4.1637e-01,  ..., -6.5323e-01,\n",
       "            4.6465e-01,  7.2568e-02],\n",
       "          [-9.5916e-01,  7.3140e-02, -9.0114e-01,  ..., -1.5376e+00,\n",
       "           -4.2797e-01, -1.4504e+00],\n",
       "          [-2.5142e+00, -4.9431e-01, -1.2560e+00,  ..., -5.7088e-01,\n",
       "           -1.4761e+00, -1.7123e+00]],\n",
       "\n",
       "         [[-2.0970e-02, -1.2786e-01,  7.5802e-02,  ...,  3.2485e+00,\n",
       "           -1.3244e-02, -1.0813e-01],\n",
       "          [ 7.9134e-01, -1.6444e+00, -1.5099e+00,  ...,  2.0409e-01,\n",
       "           -8.1057e-01, -3.3095e-01],\n",
       "          [ 1.2459e+00,  1.8759e-01, -9.9652e-01,  ...,  2.1257e+00,\n",
       "            6.7472e-01, -1.9616e+00]],\n",
       "\n",
       "         [[-1.5886e-01,  1.5340e-01, -2.7496e-01,  ...,  9.8112e-02,\n",
       "           -5.7781e-01,  3.1198e-01],\n",
       "          [ 7.7359e-01,  7.9095e-01,  1.4275e+00,  ..., -2.5783e-01,\n",
       "            4.2623e-01,  2.3933e+00],\n",
       "          [-3.9836e-01,  3.0739e-01, -2.4425e+00,  ..., -4.1286e-01,\n",
       "            1.0919e+00,  1.6089e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-6.4500e-01, -6.8860e-01,  2.1592e-02,  ..., -2.5114e-01,\n",
       "           -3.1040e-01, -4.0270e-01],\n",
       "          [ 2.8336e+00,  1.9716e-01, -1.0608e+00,  ..., -9.1586e-01,\n",
       "           -5.3145e-01,  2.9272e+00],\n",
       "          [ 1.7381e+00,  2.5508e+00, -1.3216e+00,  ..., -2.4700e-01,\n",
       "           -1.5558e-01,  1.6600e-01]],\n",
       "\n",
       "         [[-4.3472e-01, -8.2575e-01,  1.6269e-01,  ..., -2.6392e+00,\n",
       "            6.7445e-01,  4.1708e-01],\n",
       "          [-3.2367e-01,  1.5919e+00,  2.7772e+00,  ...,  7.7071e-01,\n",
       "           -1.2927e+00,  2.2496e+00],\n",
       "          [ 4.5720e-01,  1.5869e-02,  1.7646e+00,  ..., -6.4511e-01,\n",
       "            4.0243e-01,  2.4044e+00]],\n",
       "\n",
       "         [[ 4.3056e-01, -1.6023e-01,  5.9175e-01,  ...,  2.3020e-01,\n",
       "           -1.1013e+00, -7.4666e-01],\n",
       "          [-2.3295e-02,  1.2176e+00,  1.3967e-02,  ...,  3.3197e-01,\n",
       "            8.3904e-01, -1.1033e+00],\n",
       "          [-7.1171e-01,  2.6333e-03, -9.0893e-01,  ..., -1.7756e-01,\n",
       "            1.6711e+00,  1.2745e+00]]]], grad_fn=<TransposeBackward0>), tensor([[[[-3.1448e-01, -2.7446e-01,  8.7923e-01,  ..., -1.0071e-01,\n",
       "            2.5872e-01,  2.0277e-01],\n",
       "          [-4.6013e-01, -6.8046e-01, -3.3078e-01,  ..., -2.0340e+00,\n",
       "            1.8903e+00,  1.0268e+00],\n",
       "          [ 1.5356e+00,  1.3583e+00,  1.9601e+00,  ...,  1.5303e+00,\n",
       "            1.6931e+00, -4.6779e-01]],\n",
       "\n",
       "         [[-3.3418e-01, -2.0615e-01, -3.7680e-04,  ...,  1.1116e-01,\n",
       "           -5.2495e-02,  5.0538e-01],\n",
       "          [ 6.4999e-01,  1.4522e+00,  1.6719e+00,  ...,  3.0213e-01,\n",
       "           -6.0166e-01,  1.1076e+00],\n",
       "          [-2.1770e-01, -6.5270e-02,  1.2682e+00,  ..., -1.4368e+00,\n",
       "            7.9445e-02, -1.7625e-01]],\n",
       "\n",
       "         [[-1.3854e+00, -2.4378e-01,  1.5769e+00,  ..., -2.8722e-01,\n",
       "            2.5463e-01, -5.1110e-01],\n",
       "          [-5.5331e-02,  4.2202e-01,  6.4037e+00,  ...,  1.1870e+00,\n",
       "           -8.8823e-01, -1.8781e+00],\n",
       "          [-2.5392e+00, -2.1663e+00,  2.9437e+00,  ..., -1.1084e+00,\n",
       "           -5.1072e-01, -8.0566e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.7069e-03, -1.4467e-01, -3.1267e-01,  ...,  8.9439e-01,\n",
       "            2.6241e-01,  5.2564e-01],\n",
       "          [ 2.9971e-01,  5.1311e-01, -1.6133e+00,  ..., -6.9213e-01,\n",
       "           -9.9673e-01,  3.7086e-01],\n",
       "          [ 1.2627e+00,  2.6316e+00,  1.5481e-01,  ...,  1.5547e-01,\n",
       "           -8.2094e-01,  1.8819e+00]],\n",
       "\n",
       "         [[ 1.2122e-01,  1.1425e-01,  1.4162e-01,  ...,  1.7501e-01,\n",
       "            1.2413e-01, -7.4470e-04],\n",
       "          [-5.5335e-01,  1.9755e+00,  2.7954e+00,  ..., -9.8495e-01,\n",
       "           -1.3373e-01,  1.0764e+00],\n",
       "          [-6.6796e-01,  7.5985e-01,  2.0312e+00,  ..., -6.2444e-01,\n",
       "            2.8351e+00,  1.6827e+00]],\n",
       "\n",
       "         [[ 1.8496e-01,  2.1982e-01, -6.8635e-01,  ...,  2.2125e-01,\n",
       "           -2.9628e-01,  2.5956e-01],\n",
       "          [ 9.2930e-01, -7.6618e-02,  2.0500e+00,  ...,  3.4051e+00,\n",
       "            9.6832e-01,  6.2519e-03],\n",
       "          [-2.3485e+00, -9.9908e-01,  4.4089e-01,  ..., -8.0769e-01,\n",
       "            2.5141e+00, -1.9328e+00]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.1211,  1.2557,  1.6903,  ...,  1.3358, -1.6834,  1.6825],\n",
       "          [ 0.7297, -0.4075,  0.8379,  ..., -0.9190, -0.4061, -0.7289],\n",
       "          [ 1.4168,  0.0907,  0.9734,  ..., -0.0040, -0.1220, -0.6502]],\n",
       "\n",
       "         [[-0.5560, -1.2353, -3.9771,  ...,  3.0600,  1.0448, -0.1128],\n",
       "          [-0.9215, -1.2580, -2.9182,  ...,  2.6667,  0.5416,  0.7907],\n",
       "          [ 0.1505,  0.6264,  1.7995,  ...,  0.8189, -0.1826,  0.2284]],\n",
       "\n",
       "         [[-3.0270, -0.5178,  0.3810,  ..., -0.4299,  0.6174,  1.5427],\n",
       "          [-1.4496,  1.0578,  0.9767,  ..., -1.8805, -0.6665, -1.7112],\n",
       "          [-1.1978,  0.4223,  1.3236,  ...,  2.0570,  0.0470, -0.7829]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0288,  0.3719, -0.0196,  ..., -0.4072,  1.2932, -1.4275],\n",
       "          [ 0.9258, -0.2705, -0.7706,  ...,  0.2115,  0.5622, -0.9932],\n",
       "          [ 2.9210, -1.8488, -0.6630,  ...,  1.6621,  1.9262, -0.1975]],\n",
       "\n",
       "         [[-0.2861,  1.3584, -0.4054,  ..., -0.5086,  0.3110,  0.5676],\n",
       "          [ 1.3435, -0.1610, -0.1758,  ...,  0.5433,  1.0370,  1.2331],\n",
       "          [ 1.7408,  1.6164, -0.2125,  ...,  1.5656,  2.1082,  0.3172]],\n",
       "\n",
       "         [[ 1.0251, -0.3532,  0.0983,  ..., -0.4225,  0.3300, -0.1400],\n",
       "          [ 0.8235, -0.9062,  0.1591,  ..., -0.2570,  1.2007, -0.2018],\n",
       "          [ 1.6103, -1.2473,  1.3453,  ..., -0.1458,  1.7749, -0.0097]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 0.1900,  0.0526, -0.1111,  ...,  0.0710, -0.2501,  0.1097],\n",
       "          [ 1.2067, -0.4898,  0.5175,  ...,  0.1859, -0.5663, -0.3104],\n",
       "          [-1.3302,  2.3589, -0.7083,  ..., -1.0556,  0.7995, -4.5847]],\n",
       "\n",
       "         [[-1.2944,  0.0457,  0.9345,  ..., -0.5010,  0.2012, -0.5994],\n",
       "          [-4.6156,  0.6882,  2.5022,  ..., -2.6889, -2.5113, -1.3622],\n",
       "          [-0.3564,  1.0565, -1.5132,  ..., -1.9598, -0.8877, -1.2358]],\n",
       "\n",
       "         [[ 0.2426,  0.0887,  0.0516,  ...,  0.1331,  0.3471,  0.3518],\n",
       "          [-0.2589, -0.0981,  0.6708,  ..., -1.1530, -2.1033, -1.1678],\n",
       "          [ 0.9553, -0.5706,  1.3783,  ..., -0.1914,  1.5599, -0.4731]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1547, -0.1189,  0.0207,  ...,  0.1983, -0.0093,  0.0641],\n",
       "          [-0.8149,  0.3561, -0.3004,  ..., -0.3860,  0.8369,  1.2332],\n",
       "          [ 1.4507, -0.9008, -1.4385,  ..., -3.4328, -1.5145, -2.8441]],\n",
       "\n",
       "         [[-0.1830, -0.0716,  0.0479,  ..., -0.1443, -0.1981, -0.0695],\n",
       "          [ 0.3691,  0.2998, -0.7416,  ..., -1.4394, -0.9884, -1.8549],\n",
       "          [ 1.7590, -1.5582, -0.7722,  ..., -0.8009, -5.0405, -0.8411]],\n",
       "\n",
       "         [[-0.1224,  0.0269, -0.0225,  ..., -0.1736,  0.0276, -0.1703],\n",
       "          [ 0.9218,  1.8901, -0.0689,  ...,  0.0221,  0.7954, -1.3216],\n",
       "          [ 0.2818, -4.9092, -0.4829,  ...,  1.0085,  0.4012,  0.4743]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-0.4315, -0.1020,  0.0294,  ..., -0.1002,  0.1122,  0.8982],\n",
       "          [ 0.5660, -1.3578,  2.3000,  ...,  0.8023,  0.3354,  0.6597],\n",
       "          [ 0.7806, -0.9454,  0.6159,  ...,  1.9351, -0.9452,  2.4333]],\n",
       "\n",
       "         [[ 0.9352, -0.2005, -0.0733,  ...,  0.7355,  0.8333, -0.5958],\n",
       "          [-0.0955, -1.7305,  0.3901,  ..., -0.2580,  0.2779,  0.5430],\n",
       "          [-0.1083, -0.1621, -1.1166,  ...,  0.7042, -0.3042,  1.2606]],\n",
       "\n",
       "         [[ 1.8871, -1.3899,  0.3829,  ..., -0.2243,  0.2305, -1.0589],\n",
       "          [-2.2075, -2.2928,  2.1640,  ...,  1.0762, -2.0162, -0.6737],\n",
       "          [-0.6762, -1.2626,  0.5144,  ...,  1.1125, -0.5476, -0.8286]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5704, -0.6693,  1.2046,  ...,  0.2817,  0.8552,  1.1498],\n",
       "          [ 2.4493, -1.6716,  1.0169,  ..., -0.5151,  1.5769,  0.3203],\n",
       "          [ 1.2731, -0.8122,  2.6970,  ...,  0.0661,  0.4140, -0.5878]],\n",
       "\n",
       "         [[ 0.0982,  0.0859,  2.0299,  ..., -0.5146,  0.5690,  0.6795],\n",
       "          [-1.0114,  0.6413, -0.7130,  ..., -0.1956, -0.0068,  1.7561],\n",
       "          [ 1.5683, -0.5637,  1.9450,  ..., -0.8388,  0.2997,  0.4422]],\n",
       "\n",
       "         [[ 3.6490,  0.2482, -2.9401,  ..., -0.9993, -0.5515, -0.3386],\n",
       "          [ 2.8525, -0.5390, -2.0796,  ..., -1.4136, -1.8033,  0.7849],\n",
       "          [ 3.3548, -0.8683, -0.8269,  ...,  0.3246, -1.4073,  2.5682]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 0.0243, -0.1419, -0.7936,  ..., -0.7902, -0.3375, -0.1530],\n",
       "          [ 0.7423, -0.3670,  0.3123,  ...,  0.3703,  2.2100,  0.0531],\n",
       "          [-0.9094, -1.4515, -1.4021,  ..., -0.8589, -1.4025, -0.4623]],\n",
       "\n",
       "         [[-0.1161, -0.6045, -0.3281,  ..., -0.8253,  0.0414, -0.6346],\n",
       "          [ 3.4119, -2.5107, -2.5643,  ...,  1.8615, -1.3625, -1.3310],\n",
       "          [ 1.3664, -3.4593, -0.0629,  ..., -0.5124, -5.0804,  2.5258]],\n",
       "\n",
       "         [[-0.1382,  0.3439,  0.5311,  ...,  0.3179,  0.0196, -0.6297],\n",
       "          [-2.0833,  0.4565,  0.8059,  ..., -0.9411,  0.1120,  1.6984],\n",
       "          [-3.4358, -2.5550,  2.3949,  ..., -0.9278,  0.4462, -0.8148]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0087,  0.2530,  0.5621,  ...,  0.1837, -0.0307,  0.5475],\n",
       "          [ 0.9879, -0.0191, -0.6887,  ..., -2.1776, -0.8089,  0.4997],\n",
       "          [-0.3017,  0.1184, -0.0710,  ..., -0.3090, -2.2315,  1.1451]],\n",
       "\n",
       "         [[-0.0717,  0.7735, -0.0905,  ..., -0.1648,  0.8884,  0.3614],\n",
       "          [-0.1166, -0.3738, -0.0669,  ...,  1.3000, -0.6405, -1.9273],\n",
       "          [-0.1179,  1.1923, -1.2505,  ...,  2.3309,  1.5657, -1.2873]],\n",
       "\n",
       "         [[-0.5402,  0.2109, -1.3727,  ...,  0.4613, -0.8400, -0.1572],\n",
       "          [ 0.6743,  0.8960,  1.8667,  ...,  0.6169, -1.9153, -0.7095],\n",
       "          [ 3.6341, -1.3857,  0.5000,  ...,  1.4581, -1.0971, -0.3464]]]],\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-1.0799, -0.8696, -0.5513,  ...,  0.1670,  0.2329, -0.7019],\n",
       "          [-0.5763, -2.6603, -0.4913,  ...,  0.7710,  1.3768,  0.1127],\n",
       "          [-1.4015, -1.2968, -2.7757,  ...,  1.2359,  0.8435, -0.8345]],\n",
       "\n",
       "         [[-0.7409,  0.1335,  0.3939,  ...,  0.8545,  0.2489,  0.8745],\n",
       "          [-0.7867,  0.5450,  1.3869,  ...,  1.2275,  0.9990,  0.4256],\n",
       "          [-1.2511,  0.2848,  0.7866,  ...,  0.5124,  1.2157, -0.2830]],\n",
       "\n",
       "         [[-1.3235,  0.0612,  1.9077,  ...,  0.7835,  0.6441, -0.6100],\n",
       "          [-2.2514, -0.1180,  0.5314,  ..., -0.2720,  0.4001,  0.9797],\n",
       "          [-3.0585,  0.5723,  1.6223,  ..., -0.0775,  1.7994,  0.4073]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1218, -1.5370, -0.5138,  ...,  1.8058,  0.3001,  1.2573],\n",
       "          [-0.8095, -0.6318, -1.5894,  ...,  0.9248,  2.6497, -0.2794],\n",
       "          [-0.5794,  1.4548,  0.2069,  ...,  0.7417,  3.5668, -1.3056]],\n",
       "\n",
       "         [[ 0.8790, -0.3096, -0.4305,  ..., -1.1021,  0.8828, -4.7130],\n",
       "          [ 2.3554, -1.1622,  0.1780,  ..., -0.1619,  0.5978,  1.8232],\n",
       "          [ 1.6226,  0.0269, -0.0174,  ...,  1.8741, -0.4904,  3.3628]],\n",
       "\n",
       "         [[-0.3824,  0.5705, -0.0992,  ..., -0.5829, -0.5415,  0.5301],\n",
       "          [-0.8884, -2.5066,  0.8708,  ..., -2.2455, -1.3718,  1.5083],\n",
       "          [-1.3652, -2.8121,  0.5332,  ..., -1.5092, -0.5689,  0.7071]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-5.1217e-02, -1.2939e-01, -4.4601e-03,  ...,  2.8911e-01,\n",
       "            5.9391e-02, -1.1091e-02],\n",
       "          [ 5.3231e-01, -5.6186e-01,  1.4779e-01,  ..., -8.8513e-01,\n",
       "            1.3635e+00,  1.4790e+00],\n",
       "          [ 2.3053e+00,  2.0118e+00, -3.1613e+00,  ..., -2.4613e+00,\n",
       "            2.5662e+00,  4.5792e+00]],\n",
       "\n",
       "         [[ 4.1407e-02,  1.1150e-02, -5.7584e-02,  ...,  3.4734e-02,\n",
       "           -1.0025e-02,  8.3526e-02],\n",
       "          [ 4.6663e-01,  2.1243e+00, -3.4278e-02,  ..., -2.7581e+00,\n",
       "            1.7011e+00, -6.6392e-01],\n",
       "          [ 1.5329e+00,  5.2070e-01, -1.6719e+00,  ...,  1.0632e+00,\n",
       "           -4.4108e-01, -1.9710e+00]],\n",
       "\n",
       "         [[-1.0558e-01,  8.0188e-02, -1.0509e-02,  ...,  7.7476e-02,\n",
       "           -4.0770e-02,  1.9049e-01],\n",
       "          [-2.8732e-01,  1.7811e+00, -1.1113e+00,  ...,  8.1835e-01,\n",
       "           -2.1298e-01, -1.7906e-01],\n",
       "          [ 2.4920e+00, -2.3578e+00,  2.8242e+00,  ..., -1.0119e-01,\n",
       "           -1.3315e+00, -1.2412e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.5533e-01,  1.5650e-01,  4.0229e-03,  ...,  7.1491e-02,\n",
       "           -1.3037e-01,  1.4398e-02],\n",
       "          [-2.5350e-01, -7.4268e-01, -1.1563e+00,  ..., -3.6412e-01,\n",
       "            3.8001e+00,  3.9751e-01],\n",
       "          [ 1.0675e+00, -3.1280e+00, -1.9035e+00,  ..., -1.2572e+00,\n",
       "            1.1577e+00, -6.2391e-02]],\n",
       "\n",
       "         [[-4.8223e-02,  2.4330e-02,  7.4130e-02,  ..., -1.4475e-02,\n",
       "            5.3936e-03,  1.0158e-01],\n",
       "          [ 9.1264e-01, -3.9368e-01,  3.0174e-02,  ...,  3.3033e-01,\n",
       "            1.7868e+00,  1.8588e+00],\n",
       "          [ 2.6050e+00,  1.5382e+00,  3.0465e+00,  ...,  1.6702e+00,\n",
       "            3.7554e-01,  2.4079e+00]],\n",
       "\n",
       "         [[ 6.3615e-02, -3.5715e-01, -6.1391e-02,  ..., -2.9848e-01,\n",
       "           -1.9989e-01,  4.3809e-01],\n",
       "          [-1.4194e-01,  1.1218e+00,  2.2834e-01,  ...,  4.5026e-01,\n",
       "           -4.3673e-01,  2.2528e-01],\n",
       "          [-2.5382e+00,  2.1001e-01,  2.4087e-01,  ...,  5.7646e-01,\n",
       "            5.9802e-01,  4.7411e-01]]]], grad_fn=<TransposeBackward0>), tensor([[[[-0.9335, -0.8396, -0.2527,  ..., -1.9613, -0.7052, -0.0241],\n",
       "          [ 1.5760,  0.4133, -0.4780,  ..., -2.9163,  1.4427, -1.0442],\n",
       "          [-2.2457,  1.6227, -0.1990,  ..., -1.5761,  0.0474,  0.7525]],\n",
       "\n",
       "         [[ 3.5751,  0.0496,  0.2969,  ...,  0.4841,  0.7310,  0.3556],\n",
       "          [ 0.5949, -2.4965,  2.0236,  ..., -2.5393,  2.0502, -1.1371],\n",
       "          [ 4.3262,  0.0806,  3.2126,  ..., -3.7706, -0.1648, -1.0198]],\n",
       "\n",
       "         [[-1.0465, -0.1835,  1.8320,  ..., -2.1520, -0.3824, -0.3106],\n",
       "          [-0.8842, -2.6482,  0.9038,  ...,  1.1427,  2.5725,  1.5341],\n",
       "          [-0.3981,  1.1736,  1.5877,  ...,  0.8431,  0.3376, -1.4076]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.9097,  0.6322, -0.7881,  ..., -0.4857,  1.3169, -2.3982],\n",
       "          [ 3.2917,  1.4247,  0.1713,  ..., -1.8573,  2.1688,  0.1665],\n",
       "          [ 0.2223, -2.1606,  1.2090,  ..., -0.5373,  2.1043,  1.7362]],\n",
       "\n",
       "         [[ 0.5433, -0.0625, -0.6347,  ...,  0.0816,  0.3649, -0.9589],\n",
       "          [-2.4966,  1.0544, -2.5471,  ..., -1.0169, -1.2442, -0.6410],\n",
       "          [-2.3293,  1.3050,  0.5975,  ..., -2.3535, -1.3773,  0.1285]],\n",
       "\n",
       "         [[ 0.2414,  1.7278,  1.1718,  ...,  1.2023, -0.7337,  1.2951],\n",
       "          [-0.8670, -0.5604, -0.2011,  ...,  2.3975,  1.4498,  0.4487],\n",
       "          [-1.1652,  1.0330, -0.4913,  ...,  4.4379,  1.0117,  1.8207]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-0.0644,  0.1431, -0.1006,  ..., -0.3743,  0.2773, -0.2454],\n",
       "          [-3.5048, -3.4838, -0.4158,  ..., -1.6188,  0.0164,  0.1255],\n",
       "          [-0.4459, -1.2073,  3.0497,  ..., -1.4485, -2.7940, -0.1140]],\n",
       "\n",
       "         [[ 0.1633, -1.6199, -1.7003,  ...,  1.7547,  0.4425,  0.9769],\n",
       "          [-2.1906, -2.2267, -0.9334,  ...,  1.4888,  3.6915,  0.3308],\n",
       "          [-2.3732,  4.0673, -3.8805,  ..., -2.4145,  2.9915, -0.2158]],\n",
       "\n",
       "         [[-2.0407, -0.4175,  0.5318,  ..., -0.6667,  0.4876, -0.9708],\n",
       "          [-0.3993, -0.2094,  0.2460,  ...,  6.3637,  2.4267, -5.6165],\n",
       "          [ 4.4699,  3.4948,  4.5498,  ...,  5.5666, -0.2835, -2.2751]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2827, -0.8624,  1.5509,  ...,  2.6465, -1.2077,  0.0257],\n",
       "          [ 0.2172, -4.1605,  0.2431,  ..., -2.1785,  2.3618, -0.8644],\n",
       "          [ 3.6266, -2.3951,  2.6117,  ...,  1.1275,  0.4250,  2.1398]],\n",
       "\n",
       "         [[-0.1447,  0.5588, -0.3500,  ...,  0.4961, -0.1232, -0.0464],\n",
       "          [ 1.9778,  2.4689,  1.4366,  ...,  0.0610, -0.4787,  0.7569],\n",
       "          [ 2.5257,  5.9018, -0.6852,  ..., -2.2817, -1.0754, -0.8383]],\n",
       "\n",
       "         [[-1.1879, -2.3262,  1.6121,  ...,  0.6969,  0.4698, -0.2933],\n",
       "          [ 2.5550, -1.1357,  1.1250,  ..., -1.1353,  1.1709,  0.8278],\n",
       "          [-0.9660, -1.8379,  1.7132,  ..., -1.2892, -2.4411, -0.7718]]]],\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 2.0900,  2.1918,  2.4538,  ...,  2.0690, -0.5154, -0.1946],\n",
       "          [ 1.0773,  0.5579,  2.4395,  ..., -0.6825, -0.9442, -1.4814],\n",
       "          [-1.6505,  0.7209,  3.5186,  ..., -0.7428, -2.3174, -2.5886]],\n",
       "\n",
       "         [[-0.7243, -0.0142, -0.6002,  ...,  0.6687, -1.0710,  0.6133],\n",
       "          [-0.0832, -0.3202, -2.1519,  ...,  1.0593, -2.2134,  0.9643],\n",
       "          [ 0.3018,  0.9524, -2.6983,  ...,  1.2960, -1.1230,  0.3891]],\n",
       "\n",
       "         [[ 0.7630, -3.4165, -0.8202,  ...,  1.4704,  0.4119,  1.0249],\n",
       "          [-1.0686, -1.5526,  1.1413,  ..., -1.3740,  0.7577,  0.0058],\n",
       "          [-0.6469, -2.8653,  0.3024,  ..., -0.3122,  0.4132, -0.8546]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.3097, -0.1628, -0.1801,  ..., -2.0079,  1.3841, -1.3792],\n",
       "          [ 0.4612, -3.4827, -1.4615,  ..., -2.6220,  1.1545, -2.0978],\n",
       "          [-1.0817, -3.8654, -2.1557,  ..., -2.5839,  1.5667, -1.8943]],\n",
       "\n",
       "         [[ 0.5727, -0.3993, -0.2675,  ..., -0.5831, -0.0913, -0.2781],\n",
       "          [ 1.2306,  0.6211, -0.4378,  ..., -0.8744, -0.4788, -0.6984],\n",
       "          [ 0.4249, -0.9467,  0.7000,  ..., -1.8098, -0.6163,  0.0927]],\n",
       "\n",
       "         [[-0.1709,  0.6759,  2.0382,  ...,  0.2671,  0.1851, -0.4972],\n",
       "          [-1.1569,  2.3152,  1.7172,  ...,  0.8988, -0.9139, -1.4173],\n",
       "          [-2.5327,  1.4586,  0.8692,  ...,  2.6736,  0.1736, -2.0791]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-1.8999e-02, -8.4820e-01, -8.1142e-01,  ...,  7.6843e-01,\n",
       "           -7.8794e-01,  2.9732e-01],\n",
       "          [-1.8335e+00, -1.1309e+00,  8.1153e-01,  ..., -3.6070e-02,\n",
       "           -2.9170e+00, -6.9696e-01],\n",
       "          [-3.9900e-01,  4.6119e-01, -4.4591e-02,  ..., -1.3395e+00,\n",
       "           -2.1565e+00, -1.2155e+00]],\n",
       "\n",
       "         [[ 1.7928e-01, -2.9857e-01,  2.1389e-02,  ..., -1.1863e-01,\n",
       "           -2.7593e-01, -4.7293e-01],\n",
       "          [ 1.7800e+00, -2.9948e+00, -3.7342e+00,  ..., -1.6913e+00,\n",
       "           -1.0919e+00,  1.4240e+00],\n",
       "          [ 5.4199e+00,  4.8199e-01, -8.3637e-01,  ..., -5.9453e+00,\n",
       "           -3.2919e+00, -8.1036e-01]],\n",
       "\n",
       "         [[ 2.0289e-02,  2.7303e-01, -1.9323e-01,  ..., -2.6374e-01,\n",
       "            9.0080e-02,  8.0836e-02],\n",
       "          [ 1.4789e-01, -1.1406e+00,  1.9014e+00,  ..., -8.2283e-01,\n",
       "            5.3856e-01,  1.3819e+00],\n",
       "          [ 2.3222e+00,  1.0730e+00,  4.1428e+00,  ..., -1.2001e+00,\n",
       "            3.3408e-01, -4.5838e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.9711e-02, -7.2854e-02,  2.6933e-02,  ..., -8.3554e-02,\n",
       "           -8.2822e-02,  4.7452e-01],\n",
       "          [ 9.4188e-01, -2.7378e+00,  1.8959e+00,  ...,  2.2647e+00,\n",
       "           -7.1506e-01, -6.1741e-01],\n",
       "          [ 2.9245e+00, -4.5899e+00, -1.4498e-02,  ...,  1.8569e+00,\n",
       "            3.3653e+00,  1.4067e+00]],\n",
       "\n",
       "         [[ 1.0918e-01, -1.1845e-02,  6.3754e-02,  ..., -2.6492e-01,\n",
       "            9.9016e-02,  5.4479e-02],\n",
       "          [-2.9271e+00, -1.4971e+00, -1.1398e+00,  ...,  7.0510e-01,\n",
       "            2.5991e+00,  1.3189e+00],\n",
       "          [ 4.2350e+00,  1.7367e+00,  1.7340e+00,  ...,  4.2711e-02,\n",
       "            4.9845e+00,  3.3070e-01]],\n",
       "\n",
       "         [[-5.6599e-02, -1.9072e-01, -3.1837e-02,  ..., -1.7003e-01,\n",
       "           -2.7134e-03, -1.1508e-01],\n",
       "          [-3.3839e+00,  4.5029e-01, -8.4532e-01,  ...,  2.6165e+00,\n",
       "            7.6993e-01,  1.6910e+00],\n",
       "          [-8.2935e-01, -1.0235e+00, -2.3029e+00,  ...,  6.5525e+00,\n",
       "           -3.1939e-01,  3.1842e+00]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 8.8734e-02, -2.6080e-02, -2.4378e+00,  ..., -1.2205e+00,\n",
       "           -5.3483e-01,  1.2460e+00],\n",
       "          [ 2.4853e+00, -3.4388e-01,  4.0074e-01,  ...,  2.7330e+00,\n",
       "            9.0821e-01, -2.3814e+00],\n",
       "          [ 8.5320e-01, -1.8636e+00, -1.8708e+00,  ..., -3.3748e-01,\n",
       "           -1.7218e-01, -2.2607e+00]],\n",
       "\n",
       "         [[ 2.1375e-01, -5.4697e-01, -5.0122e-01,  ...,  4.1312e-01,\n",
       "            1.7947e-01,  6.5221e-01],\n",
       "          [-2.5466e-01, -1.5442e+00,  1.5842e+00,  ..., -1.7287e+00,\n",
       "           -1.2815e+00, -1.0954e+00],\n",
       "          [-9.6301e-01, -3.2624e+00,  2.7518e-01,  ..., -1.3120e-02,\n",
       "            4.4148e-02,  1.5774e+00]],\n",
       "\n",
       "         [[-5.4480e-01, -1.7828e+00, -1.0812e+00,  ...,  9.9414e-01,\n",
       "            5.4922e-01, -5.2005e+00],\n",
       "          [ 4.1434e-01, -1.5297e+00,  1.3175e+00,  ..., -2.7121e+00,\n",
       "           -1.6810e+00,  1.5788e+00],\n",
       "          [-6.0274e-02,  7.6625e-01, -8.5638e-01,  ..., -1.9895e+00,\n",
       "           -1.5752e+00, -2.2734e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.2297e-01, -4.7728e-01, -7.1955e-01,  ...,  1.6768e-01,\n",
       "            1.9667e-01, -4.8655e-01],\n",
       "          [ 1.6284e+00, -8.6698e-01, -2.0980e-01,  ..., -3.8810e-02,\n",
       "            4.9539e-03, -1.0755e+00],\n",
       "          [ 1.2720e+00,  1.3946e+00,  1.9665e+00,  ...,  8.7439e-01,\n",
       "           -3.3752e-01,  1.3927e+00]],\n",
       "\n",
       "         [[-8.8747e-01,  9.0874e-01, -5.8409e-01,  ...,  2.4682e-01,\n",
       "            3.7004e+00,  5.6559e-01],\n",
       "          [ 1.0990e+00, -1.6356e+00, -3.4238e-01,  ...,  1.3111e+00,\n",
       "           -9.1645e-01, -9.7451e-01],\n",
       "          [-3.1694e-01, -1.8144e-01, -6.2831e-01,  ...,  3.1462e+00,\n",
       "            6.2492e-01, -4.8089e-01]],\n",
       "\n",
       "         [[-1.1349e-01,  1.8464e-01,  6.8661e-01,  ..., -6.2199e-01,\n",
       "            4.5980e-01,  5.2529e-01],\n",
       "          [ 2.5163e-01,  3.1905e+00, -4.2891e-01,  ..., -2.0382e+00,\n",
       "            1.9587e+00, -1.1219e+00],\n",
       "          [-1.3193e+00,  2.1324e+00,  1.0377e+00,  ...,  5.0677e-01,\n",
       "           -1.0928e+00, -2.5914e+00]]]], grad_fn=<TransposeBackward0>), tensor([[[[-0.0689,  0.0583, -0.4919,  ...,  0.3878,  0.4490,  0.4292],\n",
       "          [-0.8043, -0.6426, -2.4756,  ...,  2.7933, -0.1207,  0.8541],\n",
       "          [ 2.5261,  3.2588, -0.8434,  ...,  2.6057, -2.2657, -4.0448]],\n",
       "\n",
       "         [[-0.4442,  0.3642,  0.1170,  ..., -0.2261,  0.5705,  0.2153],\n",
       "          [-2.6048,  0.2495,  1.3002,  ..., -0.9953, -4.3983, -2.4920],\n",
       "          [-1.3089,  1.5568, -0.3701,  ...,  1.7433, -3.4542, -8.2947]],\n",
       "\n",
       "         [[ 0.8935, -0.1162,  0.4822,  ..., -0.5233,  0.6208, -0.3752],\n",
       "          [ 0.2148,  0.3779,  0.5203,  ...,  3.2035, -0.9669,  1.6483],\n",
       "          [-2.7195, -7.5741, -0.7410,  ...,  0.4938, -4.7649,  4.1209]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2052,  2.7864,  0.9406,  ..., -0.5755, -0.0424, -0.9935],\n",
       "          [ 0.1338,  3.2641,  1.4913,  ..., -3.3808,  0.3622, -1.1548],\n",
       "          [ 1.2805,  3.3563,  2.5421,  ..., -3.8806,  3.2726, -0.3519]],\n",
       "\n",
       "         [[-0.0459, -0.6943, -0.6537,  ..., -1.3019,  0.5773, -0.7560],\n",
       "          [-0.8283, -0.9860, -3.3830,  ...,  0.7416, -1.9135,  4.4705],\n",
       "          [-0.7548, -1.8878,  0.6307,  ...,  0.4503,  0.2495,  0.0951]],\n",
       "\n",
       "         [[ 0.1343, -0.4096, -0.0106,  ..., -0.5800,  0.8645,  0.1267],\n",
       "          [ 1.7461, -1.6336,  2.0874,  ..., -1.5473,  3.2592, -0.1715],\n",
       "          [-4.9783, -2.0054,  3.9475,  ..., -5.7463, -0.6741, -0.1781]]]],\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-0.0042,  0.8726, -1.7387,  ..., -0.7135, -0.1226,  0.4709],\n",
       "          [-1.4646, -0.8228, -1.4508,  ...,  0.6224, -0.1231,  1.5258],\n",
       "          [-0.7483, -1.1288, -2.7616,  ...,  1.5861, -0.6194,  1.5143]],\n",
       "\n",
       "         [[-2.7134, -0.6992,  1.2512,  ...,  0.1068,  1.0789,  1.5087],\n",
       "          [-3.3980, -0.5498,  1.6374,  ..., -1.3058,  0.2114,  3.9391],\n",
       "          [-3.5246,  0.8530,  2.8521,  ..., -0.6004,  0.7983,  3.0900]],\n",
       "\n",
       "         [[ 0.0541,  1.4676, -1.8800,  ..., -0.0657, -1.4928, -0.4899],\n",
       "          [ 0.7037,  3.4595, -2.2501,  ..., -0.7963, -2.8287, -2.4006],\n",
       "          [ 1.6746,  2.2675, -3.8135,  ..., -1.4617, -3.9726, -0.5546]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3801,  0.8184, -0.4074,  ...,  0.4541, -0.2589, -2.2266],\n",
       "          [ 3.3705, -0.7989,  1.0516,  ...,  1.0535,  1.2661, -1.3879],\n",
       "          [ 3.1892, -1.3857, -0.0095,  ...,  2.2305,  2.0003, -0.2092]],\n",
       "\n",
       "         [[-0.3796,  0.7513,  0.4358,  ..., -0.3829,  0.1577, -1.2898],\n",
       "          [-0.8689,  3.8774,  1.4408,  ..., -1.3076, -1.9101, -3.0388],\n",
       "          [ 0.6513,  2.9407,  1.8734,  ..., -2.5491, -1.5842, -1.5568]],\n",
       "\n",
       "         [[-0.3850,  0.9549, -0.6012,  ...,  0.2960, -0.6577,  0.8976],\n",
       "          [-1.2189,  0.5173,  0.0353,  ..., -0.0654,  0.4308,  1.1716],\n",
       "          [-1.8098,  0.5362,  1.4902,  ..., -0.2731, -0.3028,  0.4745]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 2.4514e-01,  2.0517e-01, -1.9721e-01,  ...,  2.9018e-01,\n",
       "            3.8202e-02,  3.1615e-01],\n",
       "          [ 1.0766e+00, -5.9436e+00,  2.1562e+00,  ..., -8.6449e-01,\n",
       "           -2.1148e-01,  3.9243e+00],\n",
       "          [-1.1701e+00, -4.0404e+00,  2.3470e+00,  ..., -1.0353e+00,\n",
       "            1.4225e+00,  7.0328e-01]],\n",
       "\n",
       "         [[-9.9214e-02, -4.9430e-01, -4.6734e-01,  ..., -2.0976e-01,\n",
       "           -4.2670e-01,  2.6243e-01],\n",
       "          [ 1.3812e+00,  1.1880e+00,  7.4742e-01,  ..., -1.3832e+00,\n",
       "            9.8011e-02,  2.4654e+00],\n",
       "          [-3.2442e+00,  2.9490e+00, -5.3918e+00,  ...,  1.1772e+00,\n",
       "            2.8091e+00, -3.3215e+00]],\n",
       "\n",
       "         [[-1.1220e-01, -9.3784e-02, -1.0007e-01,  ...,  9.1648e-02,\n",
       "           -1.3648e-01,  1.1037e-01],\n",
       "          [-3.2095e+00, -2.0418e-01,  1.1859e+00,  ...,  2.6830e+00,\n",
       "           -1.5143e+00, -2.1535e+00],\n",
       "          [-3.2519e+00,  2.9770e-01, -1.1370e+00,  ...,  9.0963e-01,\n",
       "            2.1290e+00, -2.1679e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.2936e-01, -4.1617e-01, -1.3627e-01,  ..., -2.8406e-01,\n",
       "            7.0658e-02,  3.4637e-01],\n",
       "          [ 3.5586e-01,  3.4420e+00,  4.8035e-01,  ..., -3.2909e+00,\n",
       "            4.0463e+00, -8.4251e-01],\n",
       "          [ 4.3520e-01,  4.3887e+00, -1.6372e+00,  ...,  2.0259e-01,\n",
       "            2.9954e+00, -3.8417e+00]],\n",
       "\n",
       "         [[-1.7691e-01, -1.9848e-01, -3.7998e-02,  ...,  6.5931e-02,\n",
       "            2.5506e-01, -1.2391e-01],\n",
       "          [-9.5595e-02, -1.5776e+00, -1.6198e+00,  ..., -2.0196e-01,\n",
       "            7.4615e-01, -8.3027e-01],\n",
       "          [ 2.9570e+00,  9.8600e-01, -3.2093e+00,  ..., -2.3867e+00,\n",
       "           -1.0991e+00, -3.3022e+00]],\n",
       "\n",
       "         [[-9.0722e-02, -1.0127e-01,  8.6811e-03,  ..., -1.9347e-01,\n",
       "           -2.2971e-03, -1.5630e-02],\n",
       "          [-8.3846e-02, -1.9582e+00, -1.7120e+00,  ...,  1.3005e+00,\n",
       "           -3.4689e+00,  5.3882e-01],\n",
       "          [ 5.3396e-01,  1.7507e+00,  3.2471e-01,  ...,  9.3196e-02,\n",
       "           -2.2428e+00, -4.6865e+00]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 6.0575, -0.3672,  8.0873,  ..., -2.0098, -1.8023,  0.9657],\n",
       "          [ 2.9958,  5.0789,  1.1982,  ...,  1.5993, -1.0090,  1.1165],\n",
       "          [ 6.9864,  3.8615,  5.8116,  ..., -1.6803, -0.5348,  4.8782]],\n",
       "\n",
       "         [[-0.8556,  4.2451, -4.0444,  ...,  1.6182, -0.4795, -0.3867],\n",
       "          [-0.4050, -7.2184,  7.7603,  ...,  1.3087, -5.4288, -3.3457],\n",
       "          [-0.8100,  0.3020,  0.5915,  ..., -0.6154, -1.3429, -1.5179]],\n",
       "\n",
       "         [[ 0.7644,  1.0921, -1.3533,  ..., -0.4148, -4.7640, -1.6071],\n",
       "          [-2.9937,  0.3331,  0.0296,  ...,  3.6686, -1.5278, -1.2619],\n",
       "          [-1.4224,  1.0407,  1.1761,  ..., -0.2798, -0.6683, -0.8804]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.4112,  1.1483, -3.1615,  ..., -2.5393,  0.6715, -3.1705],\n",
       "          [-0.4778,  2.0911, -2.4399,  ..., -4.1122, -4.4129,  5.1171],\n",
       "          [-0.0347, -0.0126, -1.0717,  ..., -4.9479, -0.6070, -1.1042]],\n",
       "\n",
       "         [[ 1.1086,  0.6205, -0.1000,  ...,  1.5646,  0.2178,  1.2468],\n",
       "          [-3.4887,  3.1464, -1.2671,  ..., -1.2952, -4.7813, -2.9554],\n",
       "          [ 2.2076,  2.2794,  0.9363,  ...,  0.2087,  0.1927,  1.8241]],\n",
       "\n",
       "         [[ 0.9858, -0.3945,  0.0096,  ..., -0.3135, -0.5553, -0.0996],\n",
       "          [ 0.0443, -0.9648, -0.0947,  ..., -0.5970,  2.0252,  0.6818],\n",
       "          [ 2.5456, -1.5497, -1.2776,  ..., -0.0795,  2.5376,  2.1992]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-1.1046e-01, -2.9016e+00, -1.8864e-01,  ...,  6.6322e-01,\n",
       "            8.3086e-01, -8.0972e-02],\n",
       "          [-4.7760e+00,  7.4723e+00, -1.9515e+00,  ...,  3.7237e-01,\n",
       "            2.1470e-01, -4.0311e+00],\n",
       "          [ 2.0676e+00,  1.3967e+01,  2.4838e+00,  ..., -2.3263e+00,\n",
       "           -6.5311e-01, -6.1978e+00]],\n",
       "\n",
       "         [[ 7.5874e-01, -7.5531e-01, -4.3326e-01,  ...,  7.4344e-01,\n",
       "            1.6446e+00,  1.0252e+00],\n",
       "          [ 6.6588e+00, -2.2486e+00,  5.0108e-01,  ..., -1.3266e+00,\n",
       "           -2.8464e+00, -3.9558e+00],\n",
       "          [ 5.0420e+00,  5.8263e+00, -4.4991e+00,  ..., -3.6834e+00,\n",
       "           -6.6078e+00,  3.1071e+00]],\n",
       "\n",
       "         [[ 2.4227e-01, -1.2880e+00,  5.1696e-02,  ..., -1.8208e-01,\n",
       "           -6.4040e-01, -5.6198e-01],\n",
       "          [-3.3569e+00,  6.8882e+00,  1.4408e+00,  ..., -4.0848e+00,\n",
       "           -3.4315e-01,  1.8426e+00],\n",
       "          [-4.8049e+00,  4.2831e+00,  3.8110e+00,  ..., -9.0701e+00,\n",
       "           -5.6704e+00,  2.7169e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.2308e-01, -6.7794e-02,  1.2511e+00,  ...,  1.5349e+00,\n",
       "            2.2966e-01, -7.2365e-01],\n",
       "          [ 3.7213e+00,  1.7698e+00,  4.0405e+00,  ...,  3.9911e+00,\n",
       "            9.2516e+00,  4.9049e+00],\n",
       "          [ 5.5665e+00,  4.5939e+00,  5.8379e+00,  ...,  5.1064e-01,\n",
       "            2.5838e+00, -1.5623e+00]],\n",
       "\n",
       "         [[-1.4544e-01,  1.6280e-01, -2.7988e-03,  ...,  1.3079e-01,\n",
       "            2.7628e-01,  5.1406e-02],\n",
       "          [-5.0715e+00,  1.4495e+00, -6.3497e-01,  ..., -2.6976e+00,\n",
       "            2.5427e+00,  1.5700e+00],\n",
       "          [-2.9606e+00,  1.1767e+00,  4.1003e+00,  ...,  2.3559e-01,\n",
       "            5.3146e-01, -3.7014e-01]],\n",
       "\n",
       "         [[ 8.4094e-01, -7.4256e-01, -1.3522e+00,  ..., -2.8393e-01,\n",
       "            1.2843e+00,  5.9655e-01],\n",
       "          [ 7.3919e-02, -4.0686e+00,  1.1473e+00,  ...,  1.9607e+00,\n",
       "            1.9836e+00,  1.9787e+00],\n",
       "          [-3.1096e+00,  1.2499e+00,  1.2559e+00,  ...,  1.2577e+00,\n",
       "            4.3923e+00, -4.5153e+00]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-0.2248, -0.1670,  3.1635,  ...,  1.0193, -0.5814, -0.6587],\n",
       "          [-2.6769, -1.4596,  3.5817,  ...,  1.2214,  1.6829, -2.6003],\n",
       "          [-3.9255, -0.8742,  1.7921,  ...,  0.2814,  2.5257, -3.8580]],\n",
       "\n",
       "         [[ 0.3781,  0.9105,  1.6799,  ..., -2.3702,  2.0299, -1.9861],\n",
       "          [ 2.0837,  1.1289,  3.4722,  ..., -1.8426,  0.8665, -3.4563],\n",
       "          [ 1.4743,  0.5575,  3.0332,  ..., -1.6627, -0.5800, -2.5980]],\n",
       "\n",
       "         [[-1.4167, -0.8533,  0.2365,  ...,  0.8410,  0.4136,  1.4163],\n",
       "          [-1.2638, -1.3069, -1.0191,  ...,  1.3421, -0.7199,  4.4497],\n",
       "          [-0.5109, -0.3531, -0.3395,  ...,  2.0371, -2.3454,  3.3083]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.2886, -0.4313, -0.4936,  ..., -1.9834, -0.8072,  3.1920],\n",
       "          [-2.0007,  1.0439,  0.1850,  ..., -3.8250, -1.4645,  2.6436],\n",
       "          [-0.7421,  1.3521, -0.0137,  ..., -4.1624, -1.8443,  3.1201]],\n",
       "\n",
       "         [[-1.2965,  0.8137, -0.2918,  ..., -0.6592,  1.8026,  0.2612],\n",
       "          [-2.4027,  1.3857, -1.4450,  ..., -1.7734,  1.5969,  0.1863],\n",
       "          [-1.4660,  1.2564, -1.1089,  ..., -2.0914, -0.3276, -0.2942]],\n",
       "\n",
       "         [[ 0.6568,  0.8633,  0.3937,  ...,  0.2519,  4.0546, -1.0628],\n",
       "          [ 0.8607,  1.6443,  1.4059,  ...,  0.2244,  4.9812, -3.1032],\n",
       "          [ 0.8350,  2.4185,  1.6668,  ...,  0.5671,  4.4739, -3.1054]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-1.3667e-01, -2.1000e+00, -5.5650e-01,  ...,  7.3077e-02,\n",
       "           -7.2225e-02, -3.1616e-01],\n",
       "          [ 1.0745e+00,  2.5529e+00, -7.6877e-01,  ...,  8.2481e-01,\n",
       "            1.4790e+00, -3.2309e+00],\n",
       "          [ 5.9074e-01,  1.6935e+00, -2.9180e-01,  ..., -1.5097e+00,\n",
       "            1.4168e+00, -4.1291e+00]],\n",
       "\n",
       "         [[-2.4625e-01,  7.8708e-01,  2.2894e-01,  ...,  1.7922e-01,\n",
       "            2.6452e-01,  6.1333e-01],\n",
       "          [-5.1108e+00, -2.0225e-01, -1.5490e+00,  ...,  9.0793e-01,\n",
       "            3.2086e+00, -3.0288e+00],\n",
       "          [-2.5895e+00, -1.0523e+00, -1.6546e+00,  ..., -3.7015e+00,\n",
       "            5.8756e+00, -1.2644e+00]],\n",
       "\n",
       "         [[-7.4454e-01, -1.1806e-01,  6.4536e-02,  ..., -8.0928e-01,\n",
       "           -4.1893e-02, -2.7920e-01],\n",
       "          [-1.8091e+00,  1.1226e+00,  3.4247e+00,  ..., -2.3300e+00,\n",
       "           -1.1662e+00,  2.9465e+00],\n",
       "          [ 3.3128e-01,  6.0186e-01,  2.7666e+00,  ..., -1.4777e+00,\n",
       "           -1.5706e+00, -6.0891e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.2604e-01, -1.6604e-02,  1.5173e-01,  ..., -3.8591e-01,\n",
       "            7.8586e-01,  1.5010e-01],\n",
       "          [ 4.4846e+00,  1.6950e-02,  1.7973e+00,  ...,  2.4310e+00,\n",
       "            3.3444e+00,  3.4799e+00],\n",
       "          [ 5.1788e+00,  3.3540e-02,  2.1983e+00,  ...,  4.5243e+00,\n",
       "            4.1034e+00,  4.8906e+00]],\n",
       "\n",
       "         [[-2.2391e-01, -3.0609e-01,  1.5731e-02,  ...,  2.9518e-02,\n",
       "           -3.1900e-01, -4.0537e-01],\n",
       "          [-3.9603e+00,  6.1408e-01, -2.9252e-01,  ..., -6.4297e-01,\n",
       "           -2.1949e+00,  3.5192e-01],\n",
       "          [ 3.8890e-01, -5.8603e-03, -2.5507e+00,  ..., -6.8057e-01,\n",
       "           -4.4303e+00,  3.1912e-01]],\n",
       "\n",
       "         [[-3.1076e-02, -1.6035e-01, -2.8787e-01,  ...,  3.9142e-01,\n",
       "            4.2925e-02,  9.3214e-02],\n",
       "          [-3.1034e-01, -3.6725e+00, -2.1110e+00,  ...,  2.7481e+00,\n",
       "            1.0570e+00, -2.3986e+00],\n",
       "          [-1.2044e+00, -6.6476e+00, -1.2837e+00,  ...,  3.2409e+00,\n",
       "            6.9512e+00, -3.8149e+00]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 2.0067, -0.7354,  0.1122,  ..., -0.9194, -1.9448, -0.8684],\n",
       "          [-1.4975,  2.2563,  0.1160,  ..., -2.0038, -0.6673, -2.1203],\n",
       "          [-1.4759, -1.3311,  2.6147,  ..., -0.6332, -1.5641, -2.7554]],\n",
       "\n",
       "         [[-0.0503,  1.1526, -0.5862,  ..., -1.5449,  0.6332, -1.4549],\n",
       "          [-0.6989, -2.1652, -0.2127,  ...,  2.0380, -0.6737, -0.8756],\n",
       "          [-0.3016,  2.1563, -0.9289,  ...,  0.0302, -0.8635, -0.1444]],\n",
       "\n",
       "         [[-1.3363, -2.3527,  0.9939,  ..., -1.4376, -0.5787,  1.4984],\n",
       "          [ 2.2186, -0.0905,  1.2913,  ...,  3.8745,  1.8508, -4.0399],\n",
       "          [ 0.9991, -1.3072,  0.3900,  ...,  0.2662,  1.5839, -0.4677]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2266,  0.4825,  0.1191,  ..., -9.5250,  0.7021, -0.8796],\n",
       "          [-1.6021,  2.1743,  0.7623,  ..., -1.6699, -1.6989, -0.2564],\n",
       "          [-1.5015, -0.7744,  2.2366,  ..., -7.1930, -1.0050, -2.1725]],\n",
       "\n",
       "         [[ 0.5191, -0.3649,  1.5718,  ...,  0.3745,  0.0195,  0.6954],\n",
       "          [ 0.6456, -2.1898,  1.9902,  ..., -2.5093, -1.0209, -1.6619],\n",
       "          [ 1.2073, -0.9784,  0.7770,  ..., -1.0757,  0.7277, -0.0561]],\n",
       "\n",
       "         [[-1.1155, -0.3165, -0.8682,  ..., -0.6803,  3.0457,  0.3404],\n",
       "          [ 4.8546,  0.1351, -2.7742,  ...,  1.2746, -1.4872,  0.5982],\n",
       "          [ 1.1675,  0.4338,  0.1980,  ..., -0.2916,  2.8171, -0.4937]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 1.4181e-02, -3.9055e-01,  5.6747e-01,  ..., -2.6110e-01,\n",
       "            1.6605e-02, -3.8286e-01],\n",
       "          [-2.2157e+00,  2.9512e+00,  5.0819e+00,  ...,  2.2643e+00,\n",
       "           -7.2976e-01, -2.4908e-01],\n",
       "          [ 1.0746e+01,  1.5834e+00,  1.1687e+00,  ...,  6.0122e-01,\n",
       "           -7.7511e+00, -9.9418e-01]],\n",
       "\n",
       "         [[-1.7080e-01, -3.1272e-01,  7.1436e-02,  ..., -6.9522e-01,\n",
       "            3.5863e-01,  7.3134e-01],\n",
       "          [ 1.2437e+00,  1.4817e+00,  1.2264e+00,  ...,  9.4521e-01,\n",
       "            5.0069e+00, -4.1339e+00],\n",
       "          [-4.8365e+00,  1.7811e+00, -2.2010e+00,  ..., -1.5379e+00,\n",
       "           -3.1764e+00, -2.9469e+00]],\n",
       "\n",
       "         [[-5.6227e-01, -1.0678e-01, -2.4468e-01,  ...,  4.0243e-01,\n",
       "           -1.8864e-01, -4.3039e-01],\n",
       "          [ 1.7721e-01, -1.3934e+00,  4.4707e+00,  ..., -1.7596e+00,\n",
       "            4.5130e+00, -1.9502e-01],\n",
       "          [ 9.0553e-01, -1.1504e+01, -1.0471e+01,  ..., -6.9865e-01,\n",
       "            1.3792e+01,  6.1713e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.7474e-01,  2.1895e-01,  3.3770e-01,  ..., -1.8846e-01,\n",
       "            6.0948e-01, -3.8745e-01],\n",
       "          [ 1.7291e+00, -5.3147e+00, -8.1112e+00,  ...,  2.6303e-01,\n",
       "           -3.0328e+00,  3.8051e+00],\n",
       "          [-4.3489e+00, -1.1270e+01, -8.6697e+00,  ...,  7.8201e+00,\n",
       "            5.4317e-01, -1.9959e+00]],\n",
       "\n",
       "         [[-2.1687e-01,  8.1170e-02, -1.1243e-01,  ..., -5.8586e-01,\n",
       "            7.7005e-03, -1.2532e-02],\n",
       "          [ 2.5625e+00,  4.4561e+00, -1.8868e+00,  ...,  4.3303e+00,\n",
       "           -7.4760e+00,  5.8410e+00],\n",
       "          [ 4.2111e+00,  5.5108e+00, -7.8843e+00,  ...,  5.0099e+00,\n",
       "           -4.6006e+00,  2.1584e-01]],\n",
       "\n",
       "         [[-5.7797e-01, -2.0087e-01,  7.2495e-01,  ...,  1.7377e-01,\n",
       "            3.1557e-01,  5.0267e-01],\n",
       "          [-6.9834e+00, -2.3109e+00, -1.6527e+00,  ..., -4.9930e+00,\n",
       "            3.7800e+00, -7.7579e+00],\n",
       "          [-1.7172e+00,  4.7333e+00,  2.7027e+00,  ..., -7.5692e-01,\n",
       "           -1.0009e+00, -2.1212e+00]]]], grad_fn=<TransposeBackward0>))), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.0022,  0.0061, -0.0031,  ..., -0.0014, -0.0044,  0.0064],\n",
       "         [ 0.0719, -0.0596,  0.0446,  ...,  0.0784,  0.0225, -0.2440],\n",
       "         [ 0.0057, -0.1413,  0.0802,  ...,  0.0298,  0.0967,  0.0727]]],\n",
       "       grad_fn=<MulBackward0>), encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The quick brown fox\"\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\", use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32099,    33,  1633,     1],\n",
       "        [  216, 32099,  1633,     1],\n",
       "        [  216,    33, 32099,     1],\n",
       "        [  216,    33,  1633, 32099]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"He are cool\"\n",
    "tokenized = tokenizer(input_text, return_tensors=\"pt\")\n",
    "input_ids = tokenized.input_ids\n",
    "attention_mask = tokenized.attention_mask\n",
    "seq_length = input_ids.shape[1]\n",
    "\n",
    "# Mask out each token in the input sequence\n",
    "masked_input_ids = input_ids.repeat(seq_length, 1)\n",
    "masked_attention_mask = attention_mask.repeat(seq_length, 1)\n",
    "masked_input_ids.fill_diagonal_(mask_token)\n",
    "masked_input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32099"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token = tokenizer.convert_tokens_to_ids(tokenizer.additional_special_tokens[0])\n",
    "mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ids = masked_input_ids.tril()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "decoder_inputs = torch.tensor([[model.config.decoder_start_token_id, mask_token]]).expand(seq_length, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(input_ids=masked_input_ids, attention_mask=masked_attention_mask, decoder_input_ids=decoder_inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-38.9922, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[torch.arange(seq_length), 1, input_ids.squeeze()].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cool guys', '<extra_id_0> is', '<extra_id_0>.', 'cool cool']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(logits.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<extra_id_0> are cool</s>',\n",
       " 'He <extra_id_0> cool</s>',\n",
       " 'He are <extra_id_0> </s>',\n",
       " 'He are cool <extra_id_0>']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(masked_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [216, 33, 1633, 5, 1], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"He are cool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\", use_fast=False)\n",
    "model = RobertaForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Paris'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"The capital of France is <mask>.\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of <mask>\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "\n",
    "# labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# mask labels of non-<mask> tokens\n",
    "# labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "# outputs = model(**inputs, labels=labels)\n",
    "# round(outputs.loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Paris Lyon Nice'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.topk(logits[0, mask_token_index], 3, -1).indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2201)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0, 6].argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2201])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='FacebookAI/roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 31414, 232, 2], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma = AutoTokenizer.from_pretrained(\"google/gemma-2b\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.130485951900482"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "t = torch.rand(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1305)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/23/2024 00:57:09|INFO|logging|00| Loaded pretrained model file /afs/cs.stanford.edu/u/ananthag/.~DeBERTa/assets/latest/deberta-v3-base/pytorch_model.bin\n",
      "05/23/2024 00:57:13|WARNING|logging|00| Missing keys: [], unexpected_keys: ['mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias'], error_msgs: []\n"
     ]
    }
   ],
   "source": [
    "from DeBERTa.DeBERTa.apps.models import masked_language_model\n",
    "from DeBERTa.DeBERTa.deberta import config\n",
    "import pkgutil\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "config_json = hf_hub_download('microsoft/deberta-v3-base', 'config.json')\n",
    "# model = (config.ModelConfig.from_json_file(config_json))\n",
    "model = masked_language_model.MaskedLanguageModel.load_model(\"deberta-v3-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.position_biased_input = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "checkpoint_path = hf_hub_download('microsoft/deberta-v3-base', 'pytorch_model.bin')\n",
    "ckpt = torch.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias'])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(ckpt, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "checkpoint_path = hf_hub_download('microsoft/deberta-v3-xsmall', 'pytorch_model.bin')\n",
    "ckpt = torch.load(checkpoint_path)\n",
    "ckpt['cls.predictions.transform.dense.weight'] = ckpt.pop('lm_predictions.lm_head.dense.weight')\n",
    "ckpt['cls.predictions.transform.dense.bias'] = ckpt.pop('lm_predictions.lm_head.dense.bias')\n",
    "ckpt['cls.predictions.transform.LayerNorm.weight'] = ckpt.pop('lm_predictions.lm_head.LayerNorm.weight')\n",
    "ckpt['cls.predictions.transform.LayerNorm.bias'] = ckpt.pop('lm_predictions.lm_head.LayerNorm.bias')\n",
    "ckpt['cls.predictions.decoder.weight'] = ckpt['deberta.embeddings.word_embeddings.weight']\n",
    "ckpt['cls.predictions.decoder.bias'] = ckpt['lm_predictions.lm_head.bias']\n",
    "ckpt['cls.predictions.bias'] = ckpt.pop('lm_predictions.lm_head.bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\").config\n",
    "config_json = hf_hub_download(\"microsoft/deberta-v3-xsmall\", \"config.json\")\n",
    "from DeBERTa.DeBERTa.deberta import config\n",
    "c = config.ModelConfig.from_json_file(config_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
    "# tokenizer(\"Hi this\")\n",
    "c.position_biased_input = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'capital', 'of', 'France', 'is']"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from DeBERTa.DeBERTa import deberta\n",
    "# from DeBERTa.DeBERTa.apps.tasks import mlm_task\n",
    "vocab_path, vocab_type = deberta.load_vocab(pretrained_id='deberta-v3-base')\n",
    "tokenizer = deberta.tokenizers[vocab_type](vocab_path)\n",
    "# We apply the same schema of special tokens as BERT, e.g. [CLS], [SEP], [MASK]\n",
    "max_seq_len = 512\n",
    "tokens = tokenizer.tokenize('The capital of France is ')\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 279, 1909, 265, 2378, 269, 128000, 2]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = tokenizer.tokenize('A B C D E F')\n",
    "tokens.append(tokenizer.mask())\n",
    "token_ids = ['[CLS]'] + tokens + ['[SEP]']\n",
    "input_ids = tokenizer.convert_tokens_to_ids(token_ids)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(\n",
    "    torch.tensor([input_ids]),\n",
    "    position_ids = torch.arange(len(input_ids)).unsqueeze(0),\n",
    "    input_mask=torch.ones(len(input_ids)).unsqueeze(0),\n",
    "    labels=torch.tensor([[0, 0, 0, 0, 0, 0, 3045, 0]])\n",
    ")['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1336, 269, 262, 128000, 265, 1991, 2]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'akshshenkoarnaArabPINK bitmap fraying Kumasi NAIanthi'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.decode(torch.topk(logits, 10, dim=-1).indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_predictions.lm_head.dense.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'65618'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(['65618'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect\n",
    "import math\n",
    "\n",
    "class NGramMaskGenerator:\n",
    "  \"\"\"\n",
    "  Mask ngram tokens\n",
    "  https://github.com/zihangdai/xlnet/blob/0b642d14dd8aec7f1e1ecbf7d6942d5faa6be1f0/data_utils.py\n",
    "  \"\"\"\n",
    "  def __init__(self, tokenizer, mask_lm_prob=0.15, max_seq_len=512, max_preds_per_seq=None, max_gram = 1, keep_prob = 0.1, mask_prob=0.8, **kwargs):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.mask_lm_prob = mask_lm_prob\n",
    "    self.keep_prob = keep_prob\n",
    "    self.mask_prob = mask_prob\n",
    "    assert self.mask_prob+self.keep_prob<=1, f'The prob of using [MASK]({mask_prob}) and the prob of using original token({keep_prob}) should between [0,1]'\n",
    "    self.max_preds_per_seq = max_preds_per_seq\n",
    "    if max_preds_per_seq is None:\n",
    "      self.max_preds_per_seq = math.ceil(max_seq_len*mask_lm_prob /10)*10\n",
    "\n",
    "    self.max_gram = max(max_gram, 1)\n",
    "    self.mask_window = int(1/mask_lm_prob) # make ngrams per window sized context\n",
    "    self.vocab_words = list(tokenizer.vocab.keys())\n",
    "\n",
    "  def mask_tokens(self, tokens, rng, **kwargs):\n",
    "    special_tokens = ['[MASK]', '[CLS]', '[SEP]', '[PAD]', '[UNK]'] # + self.tokenizer.tokenize(' ')\n",
    "    indices = [i for i in range(len(tokens)) if tokens[i] not in special_tokens]\n",
    "    ngrams = np.arange(1, self.max_gram + 1, dtype=np.int64)\n",
    "    pvals = 1. / np.arange(1, self.max_gram + 1)\n",
    "    pvals /= pvals.sum(keepdims=True)\n",
    "\n",
    "    unigrams = []\n",
    "    for id in indices:\n",
    "      if self.max_gram>1 and len(unigrams)>=1 and self.tokenizer.part_of_whole_word(tokens[id]):\n",
    "        unigrams[-1].append(id)\n",
    "      else:\n",
    "        unigrams.append([id])\n",
    "    \n",
    "    num_to_predict = min(self.max_preds_per_seq, max(1, int(round(len(tokens) * self.mask_lm_prob))))\n",
    "    mask_len = 0\n",
    "    offset = 0\n",
    "    mask_grams = np.array([False]*len(unigrams))\n",
    "    while offset < len(unigrams):\n",
    "      n = self._choice(rng, ngrams, p=pvals)\n",
    "      ctx_size = min(n*self.mask_window, len(unigrams)-offset)\n",
    "      m = rng.randint(0, ctx_size-1)\n",
    "      s = offset + m\n",
    "      e = min(offset+m+n, len(unigrams))\n",
    "      offset = max(offset+ctx_size, e)\n",
    "      mask_grams[s:e] = True\n",
    "\n",
    "    target_labels = [None]*len(tokens)\n",
    "    w_cnt = 0\n",
    "    for m,word in zip(mask_grams, unigrams):\n",
    "      if m:\n",
    "        for idx in word:\n",
    "          label = self._mask_token(idx, tokens, rng, self.mask_prob, self.keep_prob)\n",
    "          target_labels[idx] = label\n",
    "          w_cnt += 1\n",
    "        if w_cnt >= num_to_predict:\n",
    "          break\n",
    "\n",
    "    target_labels = [self.tokenizer.vocab[x] if x else 0 for x in target_labels]\n",
    "    return tokens, target_labels\n",
    "\n",
    "  def _choice(self, rng, data, p):\n",
    "    cul = np.cumsum(p)\n",
    "    x = rng.random()*cul[-1]\n",
    "    id = bisect(cul, x)\n",
    "    return data[id]\n",
    "\n",
    "  def _mask_token(self, idx, tokens, rng, mask_prob, keep_prob):\n",
    "    label = tokens[idx]\n",
    "    mask = '[MASK]'\n",
    "    rand = rng.random()\n",
    "    if rand < mask_prob:\n",
    "      new_label = mask\n",
    "    elif rand < mask_prob+keep_prob:\n",
    "      new_label = label\n",
    "    else:\n",
    "      new_label = rng.choice(self.vocab_words)\n",
    "\n",
    "    tokens[idx] = new_label\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlogits\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "model_mlm.deberta.encoder.layer[0].intermediate.dense.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_gen = NGramMaskGenerator(tokenizer, max_gram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32826, 16, 50264, 646, 32804, 530, 742, 9, 1470]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "tokenizer.convert_tokens_to_ids(mask_gen.mask_tokens(tokens, rng=random)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris FergusonK] of France'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(mask_gen.mask_tokens(tokens, rng=random)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2Tokenizer' object has no attribute 'mask_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_token_id\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2Tokenizer' object has no attribute 'mask_token_id'"
     ]
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['40313', '[MASK]', '262', '685', '31180', '42', '60', '286', '4881']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_gen.mask_tokens(tokens, rng=random)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "from DeBERTa.DeBERTa.apps.models import masked_language_model as deberta_mlm\n",
    "from DeBERTa.DeBERTa.deberta import config\n",
    "\n",
    "\n",
    "def _prepare_deberta(hf_model_name):\n",
    "    config_json = hf_hub_download(hf_model_name, \"config.json\")\n",
    "    d_config = config.ModelConfig.from_json_file(config_json)\n",
    "    d_config.position_biased_input = True\n",
    "    model = deberta_mlm.MaskedLanguageModel(d_config)\n",
    "    checkpoint_path = hf_hub_download(hf_model_name, \"pytorch_model.bin\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "    return model\n",
    "\n",
    "deberta = _prepare_deberta(\"microsoft/deberta-v3-xsmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLanguageModel(\n",
       "  (deberta): DeBERTa(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(128100, 384, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_predictions): EnhancedMaskDecoder(\n",
       "    (lm_head): BertLMPredictionHead(\n",
       "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
    "inputs = hf_tokenizer(\"Hello world\", return_tensors='pt', padding=True)\n",
    "\n",
    "logits = deberta(\n",
    "                input_ids=inputs.input_ids.repeat(4, 1),\n",
    "                labels=torch.diag(inputs.input_ids.squeeze()),\n",
    "                input_mask=inputs.attention_mask,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "            )[\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.0100, -2.3126,  3.0641,  1.0293], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[torch.arange(4), inputs.input_ids.squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def _prepare_deberta(hf_model_name):\n",
    "    checkpoint_path = hf_hub_download(hf_model_name, \"pytorch_model.bin\")\n",
    "    ckpt = torch.load(checkpoint_path)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(hf_model_name)\n",
    "    ckpt[\"cls.predictions.transform.dense.weight\"] = ckpt.pop(\n",
    "        \"lm_predictions.lm_head.dense.weight\"\n",
    "    )\n",
    "    ckpt[\"cls.predictions.transform.dense.bias\"] = ckpt.pop(\n",
    "        \"lm_predictions.lm_head.dense.bias\"\n",
    "    )\n",
    "    ckpt[\"cls.predictions.transform.LayerNorm.weight\"] = ckpt.pop(\n",
    "        \"lm_predictions.lm_head.LayerNorm.weight\"\n",
    "    )\n",
    "    ckpt[\"cls.predictions.transform.LayerNorm.bias\"] = ckpt.pop(\n",
    "        \"lm_predictions.lm_head.LayerNorm.bias\"\n",
    "    )\n",
    "    ckpt[\"cls.predictions.decoder.weight\"] = ckpt[\n",
    "        \"deberta.embeddings.word_embeddings.weight\"\n",
    "    ]\n",
    "    ckpt[\"cls.predictions.decoder.bias\"] = ckpt[\"lm_predictions.lm_head.bias\"]\n",
    "    ckpt[\"cls.predictions.bias\"] = ckpt.pop(\"lm_predictions.lm_head.bias\")\n",
    "    model.load_state_dict(ckpt, strict=False)\n",
    "    return model\n",
    "\n",
    "deberta = _prepare_deberta(\"microsoft/deberta-v3-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForMaskedLM(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 384, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (cls): DebertaV2OnlyMLMHead(\n",
       "    (predictions): DebertaV2LMPredictionHead(\n",
       "      (transform): DebertaV2PredictionHeadTransform(\n",
       "        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=384, out_features=128100, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = \"Who have most people discovered while embarrassing Erin?\"\n",
    "bad = \"Who have most people discovered Erin while embarrassing?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  1876,   286,   370,   355,  2534,   438, 13784, 13736,   302,\n",
      "             2]])\n",
      "tensor([-20.0942, -21.9367, -21.6412, -18.3745, -18.6219, -15.2118, -16.8572,\n",
      "        -15.4021, -17.8441, -23.4265, -17.6024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-207.0126)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def _encoder_log_prob_sum(lm, tokenizer, input_text_batch):\n",
    "    assert len(input_text_batch) == 1\n",
    "    tokenized = tokenizer(input_text_batch, return_tensors=\"pt\")\n",
    "    input_ids = tokenized.input_ids\n",
    "    print(input_ids)\n",
    "    attention_mask = tokenized.attention_mask\n",
    "    seq_length = input_ids.shape[1]\n",
    "\n",
    "    # Mask out each token in the input sequence\n",
    "    masked_input_ids = input_ids.repeat(seq_length, 1)\n",
    "    masked_attention_mask = attention_mask.repeat(seq_length, 1)\n",
    "    masked_input_ids.fill_diagonal_(tokenizer.mask_token_id)\n",
    "\n",
    "    labels = torch.diag(input_ids.squeeze())\n",
    "\n",
    "    # Get the model's predictions for the batched masked inputs\n",
    "    with torch.inference_mode():\n",
    "        if isinstance(lm, deberta_mlm.MaskedLanguageModel):\n",
    "            logits = lm(\n",
    "                input_ids=masked_input_ids,\n",
    "                labels=labels,\n",
    "                input_mask=masked_attention_mask,\n",
    "                attention_mask=masked_attention_mask,\n",
    "            )[\"logits\"]\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            token_log_probs = log_probs[torch.arange(seq_length), input_ids.squeeze()]\n",
    "        else:\n",
    "            logits = lm(masked_input_ids, attention_mask=masked_attention_mask).logits\n",
    "\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            token_log_probs = log_probs[\n",
    "                torch.arange(seq_length), torch.arange(seq_length), input_ids.squeeze()\n",
    "            ]\n",
    "            print(token_log_probs)\n",
    "    return token_log_probs.sum()\n",
    "\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
    "_encoder_log_prob_sum(deberta, tokenizer, [good])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  1876,   286,   370,   355,  2534, 13736,   438, 13784,   302,\n",
      "             2]])\n",
      "tensor([-18.9079, -20.7022, -23.6953, -18.6025, -19.8663, -13.3951, -17.7276,\n",
      "        -18.0960, -12.1095, -23.7737, -18.3432])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-205.2192)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_encoder_log_prob_sum(deberta, AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-xsmall\"), [bad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 11081, 2], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"shocking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForMaskedLM(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 384, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (cls): DebertaV2OnlyMLMHead(\n",
       "    (predictions): DebertaV2LMPredictionHead(\n",
       "      (transform): DebertaV2PredictionHeadTransform(\n",
       "        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=384, out_features=128100, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deberta.dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/Roberta-base\")\n",
    "deberta_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
    "tokens = deberta_tokenizer(\"One Two Three [MASK]\", return_tensors=\"pt\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/Roberta-base\")\n",
    "mask_token_index = torch.where(tokens[\"input_ids\"] == deberta_tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[     1,    593,   1852,   3216, 128000,      2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])},\n",
       " tensor([4]))"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens, mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = deberta(**tokens, position_ids=torch.arange(6).unsqueeze(0)).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BasqueEIS riveting Bayer Maronite kawaii Akhtar Zahid']"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deberta_tokenizer.batch_decode(torch.topk(logits[0][mask_token_index], 10).indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 32826,    16,     5,   646, 32804,   530,   742,     9,  1470,\n",
       "             2]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0972, -0.0294,  0.4988,  ..., -0.0312, -0.0312, -1.0000],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.decoder.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deberta.cls.predictions.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = hf_hub_download(\"microsoft/deberta-v3-xsmall\", \"pytorch_model.bin\")\n",
    "ckpt = torch.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.0730e-02,  3.3051e-02, -4.5532e-02, -7.3730e-02, -1.3245e-01,\n",
       "        -1.0748e-01, -1.1218e-01, -2.1167e-01, -1.2408e-01, -2.1509e-01,\n",
       "        -1.1212e-01, -1.4417e-01, -4.0375e-02, -4.4983e-02, -3.1677e-02,\n",
       "        -3.6774e-02, -7.4829e-02, -2.3041e-02, -2.7393e-01, -1.7432e-01,\n",
       "        -8.3679e-02, -9.1187e-02, -2.8152e-02, -2.7295e-01, -2.2278e-01,\n",
       "        -1.0480e-01, -1.5588e-01, -8.8745e-02, -1.9409e-01, -1.3452e-01,\n",
       "        -1.1774e-01, -3.4912e-02, -9.8511e-02, -1.9104e-01, -9.3933e-02,\n",
       "         6.3538e-02, -4.8920e-02, -6.9427e-03, -1.1115e-01, -1.1072e-01,\n",
       "        -9.4849e-02, -1.1981e-01, -6.7810e-02, -8.1238e-02, -1.3269e-01,\n",
       "        -5.5328e-02, -3.5248e-02, -3.6774e-02, -1.0187e-01, -2.0850e-01,\n",
       "        -1.2311e-01, -1.3208e-01, -7.9895e-02, -2.7417e-01, -1.2520e+00,\n",
       "        -1.6370e-01, -4.3671e-02, -1.2903e-01, -7.5195e-02,  7.1533e-02,\n",
       "        -1.0803e-01, -1.0645e-01, -1.1572e-01, -3.8391e-02, -1.4062e-01,\n",
       "        -1.0583e-01, -1.6858e-01, -9.3506e-02, -1.2585e-01, -1.6077e-01,\n",
       "        -2.6807e-01, -9.3140e-02, -7.7759e-02, -7.7576e-02, -1.8579e-01,\n",
       "        -9.4788e-02, -1.5149e-01, -1.9543e-01, -1.2927e-01, -6.8542e-02,\n",
       "        -1.3416e-01, -2.2131e-01, -8.9233e-02, -1.2756e-01, -2.7008e-03,\n",
       "        -1.4111e-01, -1.3000e-01, -1.6406e-01, -2.1553e-03, -4.0741e-02,\n",
       "        -5.4016e-02, -1.7773e-01, -3.3911e-01, -1.3672e-01, -1.5161e-01,\n",
       "        -7.2205e-02, -1.3708e-01,  7.8659e-03, -5.6946e-02, -1.1737e-01,\n",
       "        -1.3623e-01, -2.9395e-01, -1.3818e-01, -1.4490e-01, -1.3147e-01,\n",
       "        -8.3801e-02, -1.5930e-01, -1.4673e-01, -1.1890e-01, -5.8319e-02,\n",
       "        -1.3306e-01, -8.6731e-02, -1.5039e-01, -1.2659e-01, -1.8152e-01,\n",
       "        -5.6030e-02, -2.0398e-01, -1.6309e-01, -1.5002e-01, -1.4099e-01,\n",
       "        -2.2766e-02, -2.6172e-01, -5.3406e-02, -1.3428e-01, -5.0781e-02,\n",
       "        -5.1270e-02, -1.2402e-01, -2.1240e-01, -2.6245e-02, -6.4514e-02,\n",
       "        -4.9042e-02, -1.1255e-01,  3.2539e+00, -7.2144e-02, -1.0663e-01,\n",
       "        -1.5833e-01, -1.2115e-01, -1.2683e-01, -1.1072e-01, -1.3940e-01,\n",
       "        -1.4880e-01, -1.2408e-01, -1.5125e-01, -9.3933e-02, -2.3914e-01,\n",
       "        -1.1945e-01, -1.4221e-02,  5.1079e-03, -1.9910e-01, -2.8366e-02,\n",
       "        -1.2451e-01, -1.4075e-01, -1.8982e-01, -7.1777e-02, -1.0724e-01,\n",
       "        -1.3794e-01, -2.1423e-01, -6.6162e-02,  4.4739e-02, -2.5562e-01,\n",
       "        -1.1725e-01, -1.6711e-01, -1.2134e-01, -1.0730e-01, -3.7933e-02,\n",
       "        -9.4482e-02, -1.0919e-01, -1.1456e-01, -1.2347e-01, -5.1086e-02,\n",
       "        -8.3389e-03,  1.2463e-01, -1.6357e-01, -1.7297e-01, -7.6904e-02,\n",
       "        -1.4355e-01, -1.2091e-01, -1.5967e-01, -2.0239e-01, -9.3323e-02,\n",
       "        -8.5876e-02, -4.4800e-02, -1.8713e-01, -5.7617e-02, -1.4954e-01,\n",
       "        -1.1353e-01, -6.8909e-02, -1.1249e-01, -9.2529e-02, -1.3184e-01,\n",
       "        -2.6123e-01, -1.7822e-01, -1.0785e-01, -8.8806e-02, -8.3496e-02,\n",
       "        -1.8994e-01, -4.9500e-02, -1.2634e-01, -1.3269e-01, -3.8574e-02,\n",
       "        -6.0852e-02, -1.3733e-01, -5.0934e-02, -1.1566e-01, -1.5356e-01,\n",
       "        -1.9690e-01, -1.3086e-01, -2.7417e-01, -8.3008e-02, -5.2826e-02,\n",
       "        -1.5979e-01, -1.0382e-01, -1.3171e-01, -6.0974e-02, -9.0576e-02,\n",
       "         5.6000e-02, -7.6111e-02, -6.5063e-02, -2.5464e-01, -1.3965e-01,\n",
       "        -1.9275e-01, -1.5027e-01, -1.1792e-01, -6.8970e-02, -3.4912e-02,\n",
       "        -3.6255e-02, -9.5703e-02, -1.6516e-01, -1.4282e-01, -1.1853e-01,\n",
       "        -1.4124e-01, -1.0669e-01, -9.4482e-02, -2.7222e-01, -1.7920e-01,\n",
       "        -1.4062e-01, -1.1279e-01, -3.3905e-02, -3.0200e-01, -5.8411e-02,\n",
       "        -1.5442e-01, -1.1084e-01, -6.7383e-02, -1.3904e-01, -1.5527e-01,\n",
       "         1.3086e-01, -1.1725e-01, -7.8613e-02, -1.7944e-01, -1.0675e-01,\n",
       "        -2.0300e-01, -5.5145e-02, -1.0181e-01, -8.3069e-02, -2.9468e-01,\n",
       "        -6.5674e-02, -6.5430e-02, -1.2280e-01,  2.6749e-02, -1.0706e-01,\n",
       "        -2.8735e-01, -6.9885e-02, -1.2512e-01, -1.4368e-01, -1.6797e-01,\n",
       "        -5.3406e-02, -1.6846e-01, -2.1472e-01, -5.8594e-02, -1.3130e-02,\n",
       "        -1.0730e-01, -1.2695e-01, -1.2219e-01,  6.1035e-03, -5.5176e-02,\n",
       "        -1.5515e-01, -1.8115e-01, -1.5405e-01, -4.7836e-03, -1.3599e-01,\n",
       "        -1.7859e-01, -9.6313e-02, -8.6670e-02, -1.2537e-01, -1.5430e-01,\n",
       "        -8.4912e-01, -4.2065e-01, -1.4368e-01, -1.3330e-01, -3.0396e-01,\n",
       "        -2.7710e-02, -1.2732e-01, -1.7273e-01, -1.8958e-01, -6.1462e-02,\n",
       "        -3.7109e-02,  1.8539e-02, -1.3855e-01, -8.3740e-02, -1.7249e-01,\n",
       "        -2.5732e-01,  7.3547e-02, -7.3669e-02, -2.1082e-01, -1.4185e-01,\n",
       "        -7.6904e-02, -7.0984e-02, -4.9866e-02, -2.5497e-02, -1.1066e-01,\n",
       "        -9.7168e-02, -1.4450e-02, -1.5976e-02, -2.5253e-02, -1.2408e-01,\n",
       "        -1.2610e-01, -2.0312e-01, -7.7637e-02, -1.2378e-01, -2.0349e-01,\n",
       "        -9.6924e-02, -1.8286e-01, -1.9318e-02, -1.7603e-01, -1.0706e-01,\n",
       "        -1.5327e-02, -1.2512e-01, -1.2140e-01, -5.8899e-02, -1.3245e-01,\n",
       "        -1.0162e-01, -1.0736e-01, -1.0669e-01, -4.5074e-02, -1.0443e-01,\n",
       "        -2.3422e-02, -7.1777e-02, -1.4783e-01, -2.5781e-01, -7.2632e-02,\n",
       "        -1.3013e-01, -1.4502e-01, -1.6089e-01, -1.1578e-01, -9.3201e-02,\n",
       "        -9.3628e-02, -1.0938e-01, -1.8787e-01, -1.8921e-01, -5.7312e-02,\n",
       "        -1.9763e-01, -1.1975e-01, -9.0332e-02, -1.4490e-01, -1.3379e-01,\n",
       "        -3.9886e-02, -9.2651e-02, -1.3208e-01, -1.2817e-01, -9.1858e-03,\n",
       "        -1.3196e-01, -1.0492e-01, -7.2449e-02, -9.5276e-02, -1.0742e-01,\n",
       "        -1.0413e-01, -6.9153e-02, -4.3854e-02, -1.6113e-01, -1.2964e-01,\n",
       "        -1.0431e-01, -1.0779e-01, -1.0852e-01,  8.2214e-02, -1.6211e-01,\n",
       "        -6.3660e-02, -1.6650e-01, -3.2715e-02, -1.6821e-01, -1.7505e-01,\n",
       "        -2.6340e-03, -1.0327e-01,  1.6144e-02, -1.3464e-01],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['deberta.encoder.layer.0.attention.output.LayerNorm.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[50264,   133,   812,     9,  1470,    16,  2201,     2],\n",
       "         [    0, 50264,   812,     9,  1470,    16,  2201,     2],\n",
       "         [    0,   133, 50264,     9,  1470,    16,  2201,     2],\n",
       "         [    0,   133,   812, 50264,  1470,    16,  2201,     2],\n",
       "         [    0,   133,   812,     9, 50264,    16,  2201,     2],\n",
       "         [    0,   133,   812,     9,  1470, 50264,  2201,     2],\n",
       "         [    0,   133,   812,     9,  1470,    16, 50264,     2],\n",
       "         [    0,   133,   812,     9,  1470,    16,  2201, 50264]]),\n",
       " tensor([[1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/Roberta-base\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/Roberta-base\")\n",
    "\n",
    "text = \"The capital of France is Paris\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "seq_length = tokens.input_ids.shape[1]\n",
    "input_ids = tokens.input_ids.repeat(seq_length, 1)\n",
    "input_ids.fill_diagonal_(tokenizer.mask_token_id)\n",
    "attention_mask = tokens.attention_mask.repeat(seq_length, 1)\n",
    "attention_mask = torch.tril(attention_mask)\n",
    "input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm_output = model(input_ids, attention_mask=attention_mask, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.8779e-01, 1.0853e-03, 1.0204e-03, 3.4695e-03, 6.5193e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [2.5562e-01, 5.4199e-02, 4.0479e-01, 2.0886e-01, 7.6538e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [2.7441e-01, 2.2751e-02, 1.0773e-01, 1.4917e-01, 4.4604e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [3.3252e-01, 9.1476e-03, 7.3608e-02, 7.5256e-02, 5.0977e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [7.9541e-01, 1.2909e-02, 3.8208e-02, 2.0874e-02, 1.3257e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [9.2041e-01, 3.0655e-02, 2.0493e-02, 1.9007e-03, 2.6398e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [9.7168e-01, 2.0691e-02, 4.5090e-03, 7.6103e-04, 2.3079e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [9.2334e-01, 4.6997e-02, 2.6260e-02, 1.3180e-03, 2.0828e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_lm_output.attentions[1][4][0].half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paris france bordeaux']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, ElectraForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-generator\")\n",
    "model = ElectraForMaskedLM.from_pretrained(\"google/electra-small-generator\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].topk(3, axis=-1)\n",
    "tokenizer.batch_decode(predicted_token_id.indices)\n",
    "\n",
    "# labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# # mask labels of non-[MASK] tokens\n",
    "# labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "# outputs = model(**inputs, labels=labels)\n",
    "# round(outputs.loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd18471ec1c42cd820f8d906b09035e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa002e7203f448559a8f1238b8eff1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3899bc5ac9a74a71996236eca458537a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7a622154d54ffd83664df78007e20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06c834d580c44938463901c9d8e9fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, ElectraForCausalLM, ElectraConfig\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/electra-base-generator\")\n",
    "config = ElectraConfig.from_pretrained(\"google/electra-base-generator\")\n",
    "config.is_decoder = True\n",
    "model = ElectraForCausalLM.from_pretrained(\"google/electra-base-generator\", config=config)\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "prediction_logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. the capital of france is.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(prediction_logits.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syntax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
