{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Head Word Final 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {}\n",
    "for run in api.runs(\n",
    "    path=\"ananthag/Head Word Final 2\"\n",
    "):\n",
    "    runs[run.config['model_name']] = run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37018"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.convert_tokens_to_ids('bite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeGenTokenizerFast(name_or_path='microsoft/phi-2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50257: AddedToken(\"                               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50258: AddedToken(\"                              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50259: AddedToken(\"                             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50260: AddedToken(\"                            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50261: AddedToken(\"                           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50262: AddedToken(\"                          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50263: AddedToken(\"                         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50264: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50265: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50266: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50267: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50268: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50269: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50270: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50271: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50272: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50273: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50274: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50275: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50276: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50277: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50278: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50279: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50280: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50281: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50282: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50283: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50284: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50285: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50286: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50287: AddedToken(\"\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50288: AddedToken(\"\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50289: AddedToken(\"\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50290: AddedToken(\"\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50291: AddedToken(\"\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50292: AddedToken(\"\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50293: AddedToken(\"\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50294: AddedToken(\"\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train_recogs import COGSDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "dataset = COGSDataset(\"dev\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.codegen.tokenization_codegen_fast.CodeGenTokenizerFast"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tok.encode(' adjust', add_prefix_space=False)\n",
    "type(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not tok.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"Hey what's your name\", \"Welcome home\", \"text\" * 20]\n",
    "tok.pad_token_id = tok.eos_token_id\n",
    "inputs_dict = tok.batch_encode_plus(inputs, padding=True, return_tensors='pt')\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "m = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "out = m(**inputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs_dict[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = inputs_dict[\"attention_mask\"]\n",
    "\n",
    "last_non_masked_idx = torch.sum(attn_mask, dim=1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.tensor(\n",
    "            [list(range(inputs.shape[1])) for i in range(inputs.shape[0])]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, position_ids_slice in enumerate(position_ids):\n",
    "    position_ids_slice[last_non_masked_idx[i] :] = position_ids_slice[\n",
    "        last_non_masked_idx[i]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(inputs.shape[1]).repeat(inputs.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[attn_mask] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected index [3, 20] to be smaller than self [3, 20] apart from dimension 0 and to be smaller size than src [3, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_non_masked_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m inputs\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected index [3, 20] to be smaller than self [3, 20] apart from dimension 0 and to be smaller size than src [3, 1]"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs.scatter_(0, 1 - attn_mask, last_non_masked_idx.unsqueeze(1))\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs * attn_mask + (~attn_mask.bool() * last_non_masked_idx.unsqueeze(1).expand(-1, inputs.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10814,   644,   338,   534,  1438,     4,     4,     4,     4,     4,\n",
       "             4,     4,     4,     4,     4,     4,     4,     4,     4,     4],\n",
       "        [14618,  1363,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [ 5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,\n",
       "          5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "from dataset import DepParseDataPickle\n",
    "\n",
    "d = dataset.HeadWordDatasetWithRelns('test', 'gpt2', 12, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2416"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.start_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58223"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.start_indices[2415]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58236, 12, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.hidden_state_cache.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.70710678],\n",
       "       [0.70710678, 1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "X = [[0, 0, 0], [1, 1, 1]]\n",
    "Y = [[1, 0, 0], [1, 1, 0]]\n",
    "cosine_similarity(Y, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ran = np.random.rand(3, 3)\n",
    "ran[np.eye(3, 3, dtype=bool)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "counter = collections.Counter()\n",
    "models = set()\n",
    "with open('surprisals_errata.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        counter[(row['file_name'], row['item_number'])] += 1\n",
    "        models.add(row['model'])\n",
    "\n",
    "len(models)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('reflexive_src_fem.json', '6'), 15),\n",
       " (('reflexive_src_fem.json', '7'), 15),\n",
       " (('reflexive_src_fem.json', '13'), 15),\n",
       " (('reflexive_src_fem.json', '16'), 15),\n",
       " (('reflexive_prep_fem.json', '1'), 15),\n",
       " (('reflexive_prep_fem.json', '2'), 15),\n",
       " (('reflexive_src_fem.json', '1'), 14),\n",
       " (('reflexive_src_fem.json', '5'), 14),\n",
       " (('reflexive_src_fem.json', '15'), 14),\n",
       " (('reflexive_src_fem.json', '19'), 14),\n",
       " (('reflexive_prep_fem.json', '17'), 14),\n",
       " (('fgd-embed3.json', '4'), 14),\n",
       " (('fgd_subject.json', '4'), 14),\n",
       " (('fgd_subject.json', '5'), 14),\n",
       " (('fgd_subject.json', '9'), 14),\n",
       " (('fgd_subject.json', '10'), 14),\n",
       " (('mvrr.json', '5'), 14),\n",
       " (('mvrr_mod.json', '5'), 14),\n",
       " (('number_orc.json', '5'), 14),\n",
       " (('number_orc.json', '17'), 14),\n",
       " (('number_prep.json', '5'), 14),\n",
       " (('number_prep.json', '17'), 14),\n",
       " (('number_src.json', '5'), 14),\n",
       " (('number_src.json', '17'), 14),\n",
       " (('reflexive_orc_fem.json', '1'), 14),\n",
       " (('reflexive_orc_fem.json', '5'), 14),\n",
       " (('reflexive_orc_fem.json', '6'), 14),\n",
       " (('reflexive_orc_fem.json', '7'), 14),\n",
       " (('reflexive_orc_fem.json', '15'), 14),\n",
       " (('reflexive_src_fem.json', '9'), 14),\n",
       " (('fgd_subject.json', '12'), 14),\n",
       " (('reflexive_src_fem.json', '2'), 13),\n",
       " (('reflexive_src_fem.json', '17'), 13),\n",
       " (('reflexive_prep_fem.json', '15'), 13),\n",
       " (('fgd-embed4.json', '15'), 13),\n",
       " (('fgd-embed4.json', '16'), 13),\n",
       " (('number_orc.json', '16'), 13),\n",
       " (('reflexive_orc_fem.json', '11'), 13),\n",
       " (('reflexive_orc_fem.json', '17'), 13),\n",
       " (('fgd_hierarchy.json', '1'), 13),\n",
       " (('reflexive_prep_masc.json', '2'), 13),\n",
       " (('mvrr_mod.json', '16'), 13),\n",
       " (('reflexive_src_masc.json', '9'), 13),\n",
       " (('reflexive_prep_fem.json', '7'), 12),\n",
       " (('reflexive_prep_fem.json', '16'), 12),\n",
       " (('fgd_subject.json', '6'), 12),\n",
       " (('mvrr.json', '14'), 12),\n",
       " (('number_orc.json', '3'), 12),\n",
       " (('reflexive_orc_fem.json', '13'), 12),\n",
       " (('fgd_pp.json', '6'), 12),\n",
       " (('mvrr.json', '20'), 12),\n",
       " (('mvrr.json', '16'), 12),\n",
       " (('fgd-embed3.json', '16'), 11),\n",
       " (('subordination.json', '12'), 11),\n",
       " (('mvrr_mod.json', '19'), 11),\n",
       " (('number_src.json', '3'), 11),\n",
       " (('reflexive_orc_fem.json', '16'), 11),\n",
       " (('reflexive_orc_fem.json', '19'), 11),\n",
       " (('npi_src_any.json', '38'), 11),\n",
       " (('fgd_subject.json', '7'), 11),\n",
       " (('fgd_subject.json', '19'), 11),\n",
       " (('fgd_hierarchy.json', '19'), 10),\n",
       " (('subordination_src-src.json', '9'), 10),\n",
       " (('reflexive_prep_fem.json', '5'), 10),\n",
       " (('fgd-embed3.json', '21'), 10),\n",
       " (('subordination.json', '9'), 10),\n",
       " (('subordination.json', '11'), 10),\n",
       " (('subordination.json', '19'), 10),\n",
       " (('fgd-embed4.json', '3'), 10),\n",
       " (('fgd-embed4.json', '4'), 10),\n",
       " (('fgd-embed4.json', '9'), 10),\n",
       " (('fgd-embed4.json', '21'), 10),\n",
       " (('reflexive_src_masc.json', '19'), 10),\n",
       " (('fgd_object.json', '16'), 10),\n",
       " (('reflexive_src_fem.json', '14'), 10),\n",
       " (('reflexive_prep_fem.json', '9'), 10),\n",
       " (('reflexive_prep_fem.json', '8'), 10),\n",
       " (('fgd_subject.json', '11'), 10),\n",
       " (('fgd_subject.json', '23'), 10),\n",
       " (('reflexive_src_fem.json', '4'), 9),\n",
       " (('reflexive_prep_fem.json', '6'), 9),\n",
       " (('reflexive_prep_fem.json', '13'), 9),\n",
       " (('fgd-embed3.json', '9'), 9),\n",
       " (('subordination.json', '16'), 9),\n",
       " (('fgd_subject.json', '1'), 9),\n",
       " (('mvrr.json', '19'), 9),\n",
       " (('reflexive_src_masc.json', '6'), 9),\n",
       " (('reflexive_src_masc.json', '11'), 9),\n",
       " (('number_src.json', '15'), 9),\n",
       " (('reflexive_orc_fem.json', '8'), 9),\n",
       " (('reflexive_orc_fem.json', '9'), 9),\n",
       " (('fgd_hierarchy.json', '4'), 9),\n",
       " (('fgd_hierarchy.json', '12'), 9),\n",
       " (('fgd-embed3.json', '6'), 9),\n",
       " (('fgd-embed3.json', '8'), 9),\n",
       " (('reflexive_prep_masc.json', '9'), 9),\n",
       " (('fgd-embed4.json', '12'), 9),\n",
       " (('fgd-embed4.json', '8'), 9),\n",
       " (('fgd_hierarchy.json', '7'), 9),\n",
       " (('fgd_hierarchy.json', '11'), 8)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    GPTNeoXTokenizerFast,\n",
    "    LlamaTokenizer,\n",
    "    GPT2Tokenizer,\n",
    "    GemmaTokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaTokenizerFast(name_or_path='google/gemma-2b', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [\"cat\", \"dog\", \"cow\", \"bird\"]\n",
    "model_name = 'google/gemma-2b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast='pythia' in 'google')\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# batch = tokenizer((\"one word\", \"two two two words\", \"three three three three three words\"), padding=True, return_tensors='pt')\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n",
    "# output = model(**batch)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  505,  1573, 50256, 50256, 50256, 50256],\n",
       "        [11545,   734,   734,  2456, 50256, 50256],\n",
       "        [15542,  1115,  1115,  1115,  1115,  2456]]), 'attention_mask': tensor([[1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  505,  1573, 50256, 50256, 50256, 50256],\n",
       "        [11545,   734,   734,  2456, 50256, 50256],\n",
       "        [15542,  1115,  1115,  1115,  1115,  2456]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.attention_mask.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-17.2175, -27.2265, -33.0799], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "targets = torch.cat((batch[\"input_ids\"][:, 1:], torch.tensor([tokenizer.eos_token_id]).expand(batch[\"input_ids\"].shape[0], 1)), dim=1)\n",
    "out = torch.gather(F.log_softmax(output.logits, dim=-1), -1, targets.unsqueeze(-1))\n",
    "(out * batch.attention_mask.unsqueeze(-1)).view(batch.input_ids.shape[0], -1).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['animate_subject_passive.jsonl',\n",
       " 'principle_A_domain_3.jsonl',\n",
       " 'npi_present_1.jsonl',\n",
       " 'wh_vs_that_no_gap_long_distance.jsonl',\n",
       " 'existential_there_quantifiers_1.jsonl',\n",
       " 'determiner_noun_agreement_irregular_1.jsonl',\n",
       " 'ellipsis_n_bar_1.jsonl',\n",
       " 'causative.jsonl',\n",
       " 'wh_questions_object_gap.jsonl',\n",
       " 'transitive.jsonl',\n",
       " 'superlative_quantifiers_1.jsonl',\n",
       " 'tough_vs_raising_2.jsonl',\n",
       " 'principle_A_case_1.jsonl',\n",
       " 'expletive_it_object_raising.jsonl',\n",
       " 'irregular_plural_subject_verb_agreement_2.jsonl',\n",
       " 'principle_A_domain_2.jsonl',\n",
       " 'sentential_negation_npi_scope.jsonl',\n",
       " 'ellipsis_n_bar_2.jsonl',\n",
       " 'determiner_noun_agreement_irregular_2.jsonl',\n",
       " 'left_branch_island_echo_question.jsonl',\n",
       " 'existential_there_quantifiers_2.jsonl',\n",
       " 'wh_island.jsonl',\n",
       " 'superlative_quantifiers_2.jsonl',\n",
       " 'principle_A_domain_1.jsonl',\n",
       " 'irregular_plural_subject_verb_agreement_1.jsonl',\n",
       " 'distractor_agreement_relative_clause.jsonl',\n",
       " 'existential_there_subject_raising.jsonl',\n",
       " 'tough_vs_raising_1.jsonl',\n",
       " 'principle_A_case_2.jsonl',\n",
       " 'coordinate_structure_constraint_object_extraction.jsonl',\n",
       " 'wh_vs_that_no_gap.jsonl',\n",
       " 'anaphor_gender_agreement.jsonl',\n",
       " 'determiner_noun_agreement_with_adj_2.jsonl',\n",
       " 'only_npi_scope.jsonl',\n",
       " 'inchoative.jsonl',\n",
       " 'wh_vs_that_with_gap_long_distance.jsonl',\n",
       " 'npi_present_2.jsonl',\n",
       " 'coordinate_structure_constraint_complex_left_branch.jsonl',\n",
       " 'left_branch_island_simple_question.jsonl',\n",
       " 'determiner_noun_agreement_with_adj_irregular_2.jsonl',\n",
       " 'determiner_noun_agreement_2.jsonl',\n",
       " 'wh_vs_that_with_gap.jsonl',\n",
       " 'regular_plural_subject_verb_agreement_1.jsonl',\n",
       " 'passive_2.jsonl',\n",
       " 'anaphor_number_agreement.jsonl',\n",
       " 'animate_subject_trans.jsonl',\n",
       " 'intransitive.jsonl',\n",
       " 'irregular_past_participle_verbs.jsonl',\n",
       " 'only_npi_licensor_present.jsonl',\n",
       " 'passive_1.jsonl',\n",
       " 'sentential_subject_island.jsonl',\n",
       " 'regular_plural_subject_verb_agreement_2.jsonl',\n",
       " 'principle_A_c_command.jsonl',\n",
       " 'wh_questions_subject_gap.jsonl',\n",
       " 'sentential_negation_npi_licensor_present.jsonl',\n",
       " 'principle_A_reconstruction.jsonl',\n",
       " 'determiner_noun_agreement_with_adjective_1.jsonl',\n",
       " 'drop_argument.jsonl',\n",
       " 'existential_there_object_raising.jsonl',\n",
       " 'irregular_past_participle_adjectives.jsonl',\n",
       " 'adjunct_island.jsonl',\n",
       " 'matrix_question_npi_licensor_present.jsonl',\n",
       " 'distractor_agreement_relational_noun.jsonl',\n",
       " 'complex_NP_island.jsonl',\n",
       " 'wh_questions_subject_gap_long_distance.jsonl',\n",
       " 'determiner_noun_agreement_1.jsonl',\n",
       " 'determiner_noun_agreement_with_adj_irregular_1.jsonl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../blimp/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.4905, -6.7212, -6.6658, -6.3376], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = torch.cat(\n",
    "    (\n",
    "        batch.input_ids[:, 1:],\n",
    "        torch.tensor([[tokenizer.eos_token_id]]).expand(\n",
    "            batch.input_ids.shape[0], 1\n",
    "        ),\n",
    "    ),\n",
    "    dim=-1,\n",
    ")\n",
    "logprobs = torch.gather(\n",
    "    torch.nn.functional.log_softmax(output.logits, dim=-1),\n",
    "    dim=-1,\n",
    "    index=targets.unsqueeze(1),\n",
    ").reshape(-1)\n",
    "logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(logprobs > logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256],\n",
       "        [50256],\n",
       "        [50256],\n",
       "        [50256]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[tokenizer.eos_token_id]]).expand(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92989cfd2446428999e353de76b84785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaConfig {\n",
       "  \"_name_or_path\": \"google/gemma-2b\",\n",
       "  \"architectures\": [\n",
       "    \"GemmaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"head_dim\": 256,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 16384,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"model_type\": \"gemma\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 18,\n",
       "  \"num_key_value_heads\": 1,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 256000\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModelForCausalLM.from_pretrained('google/gemma-2b').config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.41.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191cfe3dc3db42589749e788d9dfce4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db203ad8506467aa419b0af9a628583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733c0e0d90ec490b97f8cbd93917276e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254e59aead874acc995a2ad10194db98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56dc4b34438644c1b297ed0c6e630f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0271308e33a041a7943394d116f80d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "olmo = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-1B-hf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [12092, 1533], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1533], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\" world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n",
      "Pseudoperplexity: -18.2569522857666\n",
      "torch.Size([1, 6])\n",
      "Pseudoperplexity: -30.995452880859375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_pseudoperplexity(model, tokenizer, text):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids']\n",
    "    print(input_ids.shape)\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    # Get the length of the sequence\n",
    "    seq_length = input_ids.size(1)\n",
    "    \n",
    "    # Create batched inputs with each token masked once\n",
    "    masked_input_ids = input_ids.repeat(seq_length, 1)\n",
    "    \n",
    "    masked_attention_mask = attention_mask.repeat(seq_length, 1)\n",
    "    masked_input_ids.fill_diagonal_(tokenizer.mask_token_id)\n",
    "    \n",
    "    # Get the model's predictions for the batched masked inputs\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(masked_input_ids, attention_mask=masked_attention_mask)\n",
    "    \n",
    "    # Extract the logits and compute the log softmax to get log probabilities\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the actual token IDs\n",
    "    actual_token_ids = input_ids[0]\n",
    "\n",
    "    # Extract the log probabilities of the actual tokens\n",
    "    token_log_probs = log_probs[torch.arange(seq_length), torch.arange(seq_length), actual_token_ids]\n",
    "    \n",
    "    # Compute the sum of log probabilities\n",
    "    return token_log_probs.sum().item()\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'FacebookAI/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Example text\n",
    "text = \"The quick brown fox jumped.\"\n",
    "\n",
    "# Compute pseudoperplexity\n",
    "print(f\"Pseudoperplexity: {compute_pseudoperplexity(model, tokenizer, 'He is cool.')}\")\n",
    "print(f\"Pseudoperplexity: {compute_pseudoperplexity(model, tokenizer, 'He are cool.')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40bfb0012ea4b8b85f3ce3d2f4ba977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b276f07a1be4c7a950452c8cc120820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, DebertaV2ForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained('microsoft/deberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta.embeddings.word_embeddings.weight\n",
      "deberta.embeddings.LayerNorm.weight\n",
      "deberta.embeddings.LayerNorm.bias\n",
      "deberta.encoder.layer.0.attention.self.q_bias\n",
      "deberta.encoder.layer.0.attention.self.v_bias\n",
      "deberta.encoder.layer.0.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.0.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.0.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.0.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.0.attention.output.dense.weight\n",
      "deberta.encoder.layer.0.attention.output.dense.bias\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.0.intermediate.dense.weight\n",
      "deberta.encoder.layer.0.intermediate.dense.bias\n",
      "deberta.encoder.layer.0.output.dense.weight\n",
      "deberta.encoder.layer.0.output.dense.bias\n",
      "deberta.encoder.layer.0.output.LayerNorm.weight\n",
      "deberta.encoder.layer.0.output.LayerNorm.bias\n",
      "deberta.encoder.layer.1.attention.self.q_bias\n",
      "deberta.encoder.layer.1.attention.self.v_bias\n",
      "deberta.encoder.layer.1.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.1.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.1.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.1.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.1.attention.output.dense.weight\n",
      "deberta.encoder.layer.1.attention.output.dense.bias\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.1.intermediate.dense.weight\n",
      "deberta.encoder.layer.1.intermediate.dense.bias\n",
      "deberta.encoder.layer.1.output.dense.weight\n",
      "deberta.encoder.layer.1.output.dense.bias\n",
      "deberta.encoder.layer.1.output.LayerNorm.weight\n",
      "deberta.encoder.layer.1.output.LayerNorm.bias\n",
      "deberta.encoder.layer.2.attention.self.q_bias\n",
      "deberta.encoder.layer.2.attention.self.v_bias\n",
      "deberta.encoder.layer.2.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.2.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.2.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.2.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.2.attention.output.dense.weight\n",
      "deberta.encoder.layer.2.attention.output.dense.bias\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.2.intermediate.dense.weight\n",
      "deberta.encoder.layer.2.intermediate.dense.bias\n",
      "deberta.encoder.layer.2.output.dense.weight\n",
      "deberta.encoder.layer.2.output.dense.bias\n",
      "deberta.encoder.layer.2.output.LayerNorm.weight\n",
      "deberta.encoder.layer.2.output.LayerNorm.bias\n",
      "deberta.encoder.layer.3.attention.self.q_bias\n",
      "deberta.encoder.layer.3.attention.self.v_bias\n",
      "deberta.encoder.layer.3.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.3.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.3.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.3.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.3.attention.output.dense.weight\n",
      "deberta.encoder.layer.3.attention.output.dense.bias\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.3.intermediate.dense.weight\n",
      "deberta.encoder.layer.3.intermediate.dense.bias\n",
      "deberta.encoder.layer.3.output.dense.weight\n",
      "deberta.encoder.layer.3.output.dense.bias\n",
      "deberta.encoder.layer.3.output.LayerNorm.weight\n",
      "deberta.encoder.layer.3.output.LayerNorm.bias\n",
      "deberta.encoder.layer.4.attention.self.q_bias\n",
      "deberta.encoder.layer.4.attention.self.v_bias\n",
      "deberta.encoder.layer.4.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.4.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.4.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.4.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.4.attention.output.dense.weight\n",
      "deberta.encoder.layer.4.attention.output.dense.bias\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.4.intermediate.dense.weight\n",
      "deberta.encoder.layer.4.intermediate.dense.bias\n",
      "deberta.encoder.layer.4.output.dense.weight\n",
      "deberta.encoder.layer.4.output.dense.bias\n",
      "deberta.encoder.layer.4.output.LayerNorm.weight\n",
      "deberta.encoder.layer.4.output.LayerNorm.bias\n",
      "deberta.encoder.layer.5.attention.self.q_bias\n",
      "deberta.encoder.layer.5.attention.self.v_bias\n",
      "deberta.encoder.layer.5.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.5.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.5.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.5.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.5.attention.output.dense.weight\n",
      "deberta.encoder.layer.5.attention.output.dense.bias\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.5.intermediate.dense.weight\n",
      "deberta.encoder.layer.5.intermediate.dense.bias\n",
      "deberta.encoder.layer.5.output.dense.weight\n",
      "deberta.encoder.layer.5.output.dense.bias\n",
      "deberta.encoder.layer.5.output.LayerNorm.weight\n",
      "deberta.encoder.layer.5.output.LayerNorm.bias\n",
      "deberta.encoder.layer.6.attention.self.q_bias\n",
      "deberta.encoder.layer.6.attention.self.v_bias\n",
      "deberta.encoder.layer.6.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.6.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.6.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.6.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.6.attention.output.dense.weight\n",
      "deberta.encoder.layer.6.attention.output.dense.bias\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.6.intermediate.dense.weight\n",
      "deberta.encoder.layer.6.intermediate.dense.bias\n",
      "deberta.encoder.layer.6.output.dense.weight\n",
      "deberta.encoder.layer.6.output.dense.bias\n",
      "deberta.encoder.layer.6.output.LayerNorm.weight\n",
      "deberta.encoder.layer.6.output.LayerNorm.bias\n",
      "deberta.encoder.layer.7.attention.self.q_bias\n",
      "deberta.encoder.layer.7.attention.self.v_bias\n",
      "deberta.encoder.layer.7.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.7.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.7.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.7.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.7.attention.output.dense.weight\n",
      "deberta.encoder.layer.7.attention.output.dense.bias\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.7.intermediate.dense.weight\n",
      "deberta.encoder.layer.7.intermediate.dense.bias\n",
      "deberta.encoder.layer.7.output.dense.weight\n",
      "deberta.encoder.layer.7.output.dense.bias\n",
      "deberta.encoder.layer.7.output.LayerNorm.weight\n",
      "deberta.encoder.layer.7.output.LayerNorm.bias\n",
      "deberta.encoder.layer.8.attention.self.q_bias\n",
      "deberta.encoder.layer.8.attention.self.v_bias\n",
      "deberta.encoder.layer.8.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.8.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.8.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.8.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.8.attention.output.dense.weight\n",
      "deberta.encoder.layer.8.attention.output.dense.bias\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.8.intermediate.dense.weight\n",
      "deberta.encoder.layer.8.intermediate.dense.bias\n",
      "deberta.encoder.layer.8.output.dense.weight\n",
      "deberta.encoder.layer.8.output.dense.bias\n",
      "deberta.encoder.layer.8.output.LayerNorm.weight\n",
      "deberta.encoder.layer.8.output.LayerNorm.bias\n",
      "deberta.encoder.layer.9.attention.self.q_bias\n",
      "deberta.encoder.layer.9.attention.self.v_bias\n",
      "deberta.encoder.layer.9.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.9.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.9.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.9.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.9.attention.output.dense.weight\n",
      "deberta.encoder.layer.9.attention.output.dense.bias\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.9.intermediate.dense.weight\n",
      "deberta.encoder.layer.9.intermediate.dense.bias\n",
      "deberta.encoder.layer.9.output.dense.weight\n",
      "deberta.encoder.layer.9.output.dense.bias\n",
      "deberta.encoder.layer.9.output.LayerNorm.weight\n",
      "deberta.encoder.layer.9.output.LayerNorm.bias\n",
      "deberta.encoder.layer.10.attention.self.q_bias\n",
      "deberta.encoder.layer.10.attention.self.v_bias\n",
      "deberta.encoder.layer.10.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.10.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.10.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.10.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.10.attention.output.dense.weight\n",
      "deberta.encoder.layer.10.attention.output.dense.bias\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.10.intermediate.dense.weight\n",
      "deberta.encoder.layer.10.intermediate.dense.bias\n",
      "deberta.encoder.layer.10.output.dense.weight\n",
      "deberta.encoder.layer.10.output.dense.bias\n",
      "deberta.encoder.layer.10.output.LayerNorm.weight\n",
      "deberta.encoder.layer.10.output.LayerNorm.bias\n",
      "deberta.encoder.layer.11.attention.self.q_bias\n",
      "deberta.encoder.layer.11.attention.self.v_bias\n",
      "deberta.encoder.layer.11.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.11.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.11.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.11.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.11.attention.output.dense.weight\n",
      "deberta.encoder.layer.11.attention.output.dense.bias\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.11.intermediate.dense.weight\n",
      "deberta.encoder.layer.11.intermediate.dense.bias\n",
      "deberta.encoder.layer.11.output.dense.weight\n",
      "deberta.encoder.layer.11.output.dense.bias\n",
      "deberta.encoder.layer.11.output.LayerNorm.weight\n",
      "deberta.encoder.layer.11.output.LayerNorm.bias\n",
      "deberta.encoder.rel_embeddings.weight\n",
      "cls.predictions.bias\n",
      "cls.predictions.transform.dense.weight\n",
      "cls.predictions.transform.dense.bias\n",
      "cls.predictions.transform.LayerNorm.weight\n",
      "cls.predictions.transform.LayerNorm.bias\n",
      "cls.predictions.decoder.weight\n",
      "cls.predictions.decoder.bias\n"
     ]
    }
   ],
   "source": [
    "for k in model.state_dict().keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scr/biggest/ananthag\n"
     ]
    }
   ],
   "source": [
    "!echo $HF_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6d219d74254083b5664f333b4609c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e0815223bb4b69b5931bc1b244c3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoModelForMaskedLM\n",
    "\n",
    "roberta = AutoModelForMaskedLM.from_pretrained('FacebookAI/roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1476, -0.0365,  0.0753,  ..., -0.0023,  0.0172, -0.0016],\n",
       "        [ 0.0156,  0.0076, -0.0118,  ..., -0.0022,  0.0081, -0.0156],\n",
       "        [-0.0347, -0.0873, -0.0180,  ...,  0.1174, -0.0098, -0.0355],\n",
       "        ...,\n",
       "        [ 0.0304,  0.0504, -0.0307,  ...,  0.0377,  0.0096,  0.0084],\n",
       "        [ 0.0623, -0.0596,  0.0307,  ..., -0.0920,  0.1080, -0.0183],\n",
       "        [ 0.1259, -0.0145,  0.0332,  ...,  0.0121,  0.0342,  0.0168]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta.lm_head.decoder.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "state_dict = torch.load('/scr/ananthag/hub/models--microsoft--deberta-v3-xsmall/snapshots/4b419818330868dff6a60ad3e6b1c730f8b8c0c6/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta.embeddings.word_embeddings._weight torch.Size([128100, 384])\n",
      "deberta.embeddings.word_embeddings.weight torch.Size([128100, 384])\n",
      "deberta.embeddings.position_embeddings._weight torch.Size([512, 384])\n",
      "deberta.embeddings.position_embeddings.weight torch.Size([512, 384])\n",
      "deberta.embeddings.LayerNorm.weight torch.Size([384])\n",
      "deberta.embeddings.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.0.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.0.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.0.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.1.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.1.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.1.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.2.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.2.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.2.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.3.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.3.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.3.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.4.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.4.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.4.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.5.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.5.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.5.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.6.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.6.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.6.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.7.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.7.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.7.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.8.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.8.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.8.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.9.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.9.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.9.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.10.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.10.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.10.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.11.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.11.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.11.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.rel_embeddings.weight torch.Size([512, 384])\n",
      "deberta.encoder.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.LayerNorm.bias torch.Size([384])\n",
      "lm_predictions.lm_head.bias torch.Size([128100])\n",
      "lm_predictions.lm_head.dense.weight torch.Size([384, 384])\n",
      "lm_predictions.lm_head.dense.bias torch.Size([384])\n",
      "lm_predictions.lm_head.LayerNorm.weight torch.Size([384])\n",
      "lm_predictions.lm_head.LayerNorm.bias torch.Size([384])\n",
      "mask_predictions.dense.weight torch.Size([384, 384])\n",
      "mask_predictions.dense.bias torch.Size([384])\n",
      "mask_predictions.LayerNorm.weight torch.Size([384])\n",
      "mask_predictions.LayerNorm.bias torch.Size([384])\n",
      "mask_predictions.classifier.weight torch.Size([1, 384])\n",
      "mask_predictions.classifier.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for k, v in state_dict.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta.embeddings.word_embeddings.weight torch.Size([128100, 384])\n",
      "deberta.embeddings.LayerNorm.weight torch.Size([384])\n",
      "deberta.embeddings.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.0.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.0.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.0.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.1.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.1.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.1.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.2.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.2.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.2.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.3.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.3.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.3.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.4.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.4.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.4.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.5.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.5.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.5.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.6.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.6.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.6.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.7.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.7.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.7.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.8.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.8.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.8.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.9.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.9.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.9.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.10.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.10.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.10.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.11.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.11.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.11.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.rel_embeddings.weight torch.Size([512, 384])\n",
      "deberta.encoder.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.LayerNorm.bias torch.Size([384])\n",
      "cls.predictions.bias torch.Size([128100])\n",
      "cls.predictions.transform.dense.weight torch.Size([384, 384])\n",
      "cls.predictions.transform.dense.bias torch.Size([384])\n",
      "cls.predictions.transform.LayerNorm.weight torch.Size([384])\n",
      "cls.predictions.transform.LayerNorm.bias torch.Size([384])\n",
      "cls.predictions.decoder.weight torch.Size([128100, 384])\n",
      "cls.predictions.decoder.bias torch.Size([128100])\n"
     ]
    }
   ],
   "source": [
    "for k, v in deberta.state_dict().items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.bias torch.Size([128100])\n",
      "predictions.transform.dense.weight torch.Size([384, 384])\n",
      "predictions.transform.dense.bias torch.Size([384])\n",
      "predictions.transform.LayerNorm.weight torch.Size([384])\n",
      "predictions.transform.LayerNorm.bias torch.Size([384])\n",
      "predictions.decoder.weight torch.Size([128100, 384])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.cls.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ef8c9282c64fb28b318114dc3248e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef13e7f92cf47ffa8d9d0fb53168ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196587104f2f462389b97177784fd537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello <extra_id_0>\", return_tensors='pt', padding=True)\n",
    "labels = tokenizer(\"this is <extra_id_0> world\", return_tensors='pt', padding=True).input_ids\n",
    "output = model(**inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(32099)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 32128])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=tensor(6.2130, grad_fn=<NllLossBackward0>), logits=tensor([[[-43.1238, -12.7323, -26.8814,  ..., -66.6852, -66.6937, -66.6066],\n",
       "         [-27.5227,  -5.1289,  -6.9508,  ..., -41.1360, -41.1700, -41.1575],\n",
       "         [-28.0383,  -6.3891,  -5.8660,  ..., -40.2829, -40.2964, -40.3289]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[-0.4865, -2.3323, -1.1428,  ..., -2.5693, -1.7539, -0.5693],\n",
       "          [-0.6358, -1.1153, -0.3137,  ..., -0.7512, -0.5316, -0.5639],\n",
       "          [ 1.6587,  3.2717, -2.2787,  ...,  0.3732, -1.0274,  0.8426]],\n",
       "\n",
       "         [[-0.9565, -2.3581, -0.2478,  ...,  1.1193,  0.7770,  0.5377],\n",
       "          [ 2.1201, -1.6266, -0.7735,  ...,  1.0306,  1.5982,  6.4674],\n",
       "          [ 1.4973, -0.2845, -0.7443,  ...,  0.9890,  1.3783, -1.7426]],\n",
       "\n",
       "         [[ 1.0704, -2.5709,  0.6892,  ..., -1.3349,  0.2768,  0.5756],\n",
       "          [ 0.8899, -3.0563,  0.7858,  ...,  0.4327, -0.8198,  1.0157],\n",
       "          [-0.1107, -0.7388,  0.5365,  ...,  1.1146,  0.7549, -1.7762]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.6660,  2.2657,  0.3124,  ...,  1.4158, -2.9123, -1.1275],\n",
       "          [-0.0994,  0.4854,  0.9066,  ...,  1.2753, -0.2312,  0.2930],\n",
       "          [-0.4136, -0.1592, -1.7534,  ...,  4.3566, -0.1560,  0.4038]],\n",
       "\n",
       "         [[-6.0374, -1.1453, -1.5700,  ..., -1.3067, -0.1885,  1.6042],\n",
       "          [-4.4308, -0.8538, -1.4613,  ..., -0.2507, -0.0910,  1.5712],\n",
       "          [-1.0759,  0.1320,  0.3018,  ..., -1.3452,  0.8117,  0.6980]],\n",
       "\n",
       "         [[ 0.0485, -0.0356, -0.0691,  ...,  0.6128, -0.2485, -0.2954],\n",
       "          [ 0.3269,  0.0944,  0.8320,  ...,  1.3678,  0.6110,  0.6745],\n",
       "          [-0.2185,  0.1111, -0.4495,  ...,  0.5399,  0.2215,  1.8020]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 2.4430e-01,  3.2305e-01, -2.2954e-01,  ...,  7.0325e-01,\n",
       "            2.8494e-01,  1.7667e-01],\n",
       "          [ 6.0846e-01,  8.1564e-01, -6.2543e-01,  ..., -1.3925e-03,\n",
       "           -6.2205e-01,  2.1129e-01],\n",
       "          [-3.1442e-01,  1.2729e+00,  9.5270e-01,  ...,  1.2199e+00,\n",
       "            1.6016e-01,  3.5790e-01]],\n",
       "\n",
       "         [[-1.6416e-01,  5.7965e-01, -4.2177e-01,  ..., -5.3154e-02,\n",
       "           -9.0629e-02, -2.6091e-01],\n",
       "          [ 1.2116e+00,  5.1494e-01,  6.0132e-01,  ...,  3.8335e-01,\n",
       "            1.9411e+00, -2.2539e-01],\n",
       "          [-8.6819e-01,  8.5467e-01,  6.7290e-01,  ...,  8.6678e-01,\n",
       "           -3.2779e-01, -8.6272e-01]],\n",
       "\n",
       "         [[ 1.3425e-01,  2.9316e-01, -5.6067e-02,  ..., -8.8016e-02,\n",
       "           -8.8274e-01,  1.0571e-01],\n",
       "          [-2.5734e-01, -3.3990e-01,  4.1947e-01,  ..., -2.5694e-01,\n",
       "           -5.3157e-01,  8.5176e-02],\n",
       "          [ 5.2256e-01,  2.2336e-01, -5.2537e-03,  ...,  1.2392e+00,\n",
       "           -1.2696e+00, -4.4177e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.2518e-01, -2.0497e-01, -2.9561e-02,  ..., -2.6988e-01,\n",
       "            9.7269e-02, -1.0568e-01],\n",
       "          [-1.0319e-01,  1.9930e-01, -6.2021e-02,  ..., -7.7206e-02,\n",
       "            7.8045e-02,  1.3173e-03],\n",
       "          [-1.1283e+00, -1.6303e+00, -1.5670e+00,  ...,  1.1173e+00,\n",
       "            3.1469e-02,  1.2299e+00]],\n",
       "\n",
       "         [[-2.3741e-01, -4.8000e-02,  7.8948e-03,  ...,  5.3948e-02,\n",
       "            2.7783e-02, -1.5002e-01],\n",
       "          [ 9.2761e-01, -4.8887e-01, -2.0265e-01,  ...,  1.5016e-01,\n",
       "            6.7017e-02, -4.1232e-01],\n",
       "          [-9.4093e-01,  2.0756e-01,  1.6496e+00,  ...,  1.6214e-01,\n",
       "            1.0549e+00, -9.8243e-02]],\n",
       "\n",
       "         [[ 5.3252e-02, -7.6834e-01,  2.6697e-01,  ...,  2.2539e-01,\n",
       "           -1.8973e-01,  6.1827e-02],\n",
       "          [ 1.2374e-01, -5.6097e-02, -4.0457e-01,  ...,  1.0049e-01,\n",
       "            7.8244e-02, -9.5537e-02],\n",
       "          [-4.9552e-01, -1.2621e+00, -1.0584e+00,  ..., -2.2819e+00,\n",
       "            8.5126e-01, -2.6877e+00]]]], grad_fn=<TransposeBackward0>), tensor([[[[-5.4957e-02, -1.9532e-01, -4.1637e-01,  ..., -6.5323e-01,\n",
       "            4.6465e-01,  7.2568e-02],\n",
       "          [-9.5916e-01,  7.3140e-02, -9.0114e-01,  ..., -1.5376e+00,\n",
       "           -4.2797e-01, -1.4504e+00],\n",
       "          [-2.5142e+00, -4.9431e-01, -1.2560e+00,  ..., -5.7088e-01,\n",
       "           -1.4761e+00, -1.7123e+00]],\n",
       "\n",
       "         [[-2.0970e-02, -1.2786e-01,  7.5802e-02,  ...,  3.2485e+00,\n",
       "           -1.3244e-02, -1.0813e-01],\n",
       "          [ 7.9134e-01, -1.6444e+00, -1.5099e+00,  ...,  2.0409e-01,\n",
       "           -8.1057e-01, -3.3095e-01],\n",
       "          [ 1.2459e+00,  1.8759e-01, -9.9652e-01,  ...,  2.1257e+00,\n",
       "            6.7472e-01, -1.9616e+00]],\n",
       "\n",
       "         [[-1.5886e-01,  1.5340e-01, -2.7496e-01,  ...,  9.8112e-02,\n",
       "           -5.7781e-01,  3.1198e-01],\n",
       "          [ 7.7359e-01,  7.9095e-01,  1.4275e+00,  ..., -2.5783e-01,\n",
       "            4.2623e-01,  2.3933e+00],\n",
       "          [-3.9836e-01,  3.0739e-01, -2.4425e+00,  ..., -4.1286e-01,\n",
       "            1.0919e+00,  1.6089e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-6.4500e-01, -6.8860e-01,  2.1592e-02,  ..., -2.5114e-01,\n",
       "           -3.1040e-01, -4.0270e-01],\n",
       "          [ 2.8336e+00,  1.9716e-01, -1.0608e+00,  ..., -9.1586e-01,\n",
       "           -5.3145e-01,  2.9272e+00],\n",
       "          [ 1.7381e+00,  2.5508e+00, -1.3216e+00,  ..., -2.4700e-01,\n",
       "           -1.5558e-01,  1.6600e-01]],\n",
       "\n",
       "         [[-4.3472e-01, -8.2575e-01,  1.6269e-01,  ..., -2.6392e+00,\n",
       "            6.7445e-01,  4.1708e-01],\n",
       "          [-3.2367e-01,  1.5919e+00,  2.7772e+00,  ...,  7.7071e-01,\n",
       "           -1.2927e+00,  2.2496e+00],\n",
       "          [ 4.5720e-01,  1.5869e-02,  1.7646e+00,  ..., -6.4511e-01,\n",
       "            4.0243e-01,  2.4044e+00]],\n",
       "\n",
       "         [[ 4.3056e-01, -1.6023e-01,  5.9175e-01,  ...,  2.3020e-01,\n",
       "           -1.1013e+00, -7.4666e-01],\n",
       "          [-2.3295e-02,  1.2176e+00,  1.3967e-02,  ...,  3.3197e-01,\n",
       "            8.3904e-01, -1.1033e+00],\n",
       "          [-7.1171e-01,  2.6333e-03, -9.0893e-01,  ..., -1.7756e-01,\n",
       "            1.6711e+00,  1.2745e+00]]]], grad_fn=<TransposeBackward0>), tensor([[[[-3.1448e-01, -2.7446e-01,  8.7923e-01,  ..., -1.0071e-01,\n",
       "            2.5872e-01,  2.0277e-01],\n",
       "          [-4.6013e-01, -6.8046e-01, -3.3078e-01,  ..., -2.0340e+00,\n",
       "            1.8903e+00,  1.0268e+00],\n",
       "          [ 1.5356e+00,  1.3583e+00,  1.9601e+00,  ...,  1.5303e+00,\n",
       "            1.6931e+00, -4.6779e-01]],\n",
       "\n",
       "         [[-3.3418e-01, -2.0615e-01, -3.7680e-04,  ...,  1.1116e-01,\n",
       "           -5.2495e-02,  5.0538e-01],\n",
       "          [ 6.4999e-01,  1.4522e+00,  1.6719e+00,  ...,  3.0213e-01,\n",
       "           -6.0166e-01,  1.1076e+00],\n",
       "          [-2.1770e-01, -6.5270e-02,  1.2682e+00,  ..., -1.4368e+00,\n",
       "            7.9445e-02, -1.7625e-01]],\n",
       "\n",
       "         [[-1.3854e+00, -2.4378e-01,  1.5769e+00,  ..., -2.8722e-01,\n",
       "            2.5463e-01, -5.1110e-01],\n",
       "          [-5.5331e-02,  4.2202e-01,  6.4037e+00,  ...,  1.1870e+00,\n",
       "           -8.8823e-01, -1.8781e+00],\n",
       "          [-2.5392e+00, -2.1663e+00,  2.9437e+00,  ..., -1.1084e+00,\n",
       "           -5.1072e-01, -8.0566e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.7069e-03, -1.4467e-01, -3.1267e-01,  ...,  8.9439e-01,\n",
       "            2.6241e-01,  5.2564e-01],\n",
       "          [ 2.9971e-01,  5.1311e-01, -1.6133e+00,  ..., -6.9213e-01,\n",
       "           -9.9673e-01,  3.7086e-01],\n",
       "          [ 1.2627e+00,  2.6316e+00,  1.5481e-01,  ...,  1.5547e-01,\n",
       "           -8.2094e-01,  1.8819e+00]],\n",
       "\n",
       "         [[ 1.2122e-01,  1.1425e-01,  1.4162e-01,  ...,  1.7501e-01,\n",
       "            1.2413e-01, -7.4470e-04],\n",
       "          [-5.5335e-01,  1.9755e+00,  2.7954e+00,  ..., -9.8495e-01,\n",
       "           -1.3373e-01,  1.0764e+00],\n",
       "          [-6.6796e-01,  7.5985e-01,  2.0312e+00,  ..., -6.2444e-01,\n",
       "            2.8351e+00,  1.6827e+00]],\n",
       "\n",
       "         [[ 1.8496e-01,  2.1982e-01, -6.8635e-01,  ...,  2.2125e-01,\n",
       "           -2.9628e-01,  2.5956e-01],\n",
       "          [ 9.2930e-01, -7.6618e-02,  2.0500e+00,  ...,  3.4051e+00,\n",
       "            9.6832e-01,  6.2519e-03],\n",
       "          [-2.3485e+00, -9.9908e-01,  4.4089e-01,  ..., -8.0769e-01,\n",
       "            2.5141e+00, -1.9328e+00]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.1211,  1.2557,  1.6903,  ...,  1.3358, -1.6834,  1.6825],\n",
       "          [ 0.7297, -0.4075,  0.8379,  ..., -0.9190, -0.4061, -0.7289],\n",
       "          [ 1.4168,  0.0907,  0.9734,  ..., -0.0040, -0.1220, -0.6502]],\n",
       "\n",
       "         [[-0.5560, -1.2353, -3.9771,  ...,  3.0600,  1.0448, -0.1128],\n",
       "          [-0.9215, -1.2580, -2.9182,  ...,  2.6667,  0.5416,  0.7907],\n",
       "          [ 0.1505,  0.6264,  1.7995,  ...,  0.8189, -0.1826,  0.2284]],\n",
       "\n",
       "         [[-3.0270, -0.5178,  0.3810,  ..., -0.4299,  0.6174,  1.5427],\n",
       "          [-1.4496,  1.0578,  0.9767,  ..., -1.8805, -0.6665, -1.7112],\n",
       "          [-1.1978,  0.4223,  1.3236,  ...,  2.0570,  0.0470, -0.7829]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0288,  0.3719, -0.0196,  ..., -0.4072,  1.2932, -1.4275],\n",
       "          [ 0.9258, -0.2705, -0.7706,  ...,  0.2115,  0.5622, -0.9932],\n",
       "          [ 2.9210, -1.8488, -0.6630,  ...,  1.6621,  1.9262, -0.1975]],\n",
       "\n",
       "         [[-0.2861,  1.3584, -0.4054,  ..., -0.5086,  0.3110,  0.5676],\n",
       "          [ 1.3435, -0.1610, -0.1758,  ...,  0.5433,  1.0370,  1.2331],\n",
       "          [ 1.7408,  1.6164, -0.2125,  ...,  1.5656,  2.1082,  0.3172]],\n",
       "\n",
       "         [[ 1.0251, -0.3532,  0.0983,  ..., -0.4225,  0.3300, -0.1400],\n",
       "          [ 0.8235, -0.9062,  0.1591,  ..., -0.2570,  1.2007, -0.2018],\n",
       "          [ 1.6103, -1.2473,  1.3453,  ..., -0.1458,  1.7749, -0.0097]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 0.1900,  0.0526, -0.1111,  ...,  0.0710, -0.2501,  0.1097],\n",
       "          [ 1.2067, -0.4898,  0.5175,  ...,  0.1859, -0.5663, -0.3104],\n",
       "          [-1.3302,  2.3589, -0.7083,  ..., -1.0556,  0.7995, -4.5847]],\n",
       "\n",
       "         [[-1.2944,  0.0457,  0.9345,  ..., -0.5010,  0.2012, -0.5994],\n",
       "          [-4.6156,  0.6882,  2.5022,  ..., -2.6889, -2.5113, -1.3622],\n",
       "          [-0.3564,  1.0565, -1.5132,  ..., -1.9598, -0.8877, -1.2358]],\n",
       "\n",
       "         [[ 0.2426,  0.0887,  0.0516,  ...,  0.1331,  0.3471,  0.3518],\n",
       "          [-0.2589, -0.0981,  0.6708,  ..., -1.1530, -2.1033, -1.1678],\n",
       "          [ 0.9553, -0.5706,  1.3783,  ..., -0.1914,  1.5599, -0.4731]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1547, -0.1189,  0.0207,  ...,  0.1983, -0.0093,  0.0641],\n",
       "          [-0.8149,  0.3561, -0.3004,  ..., -0.3860,  0.8369,  1.2332],\n",
       "          [ 1.4507, -0.9008, -1.4385,  ..., -3.4328, -1.5145, -2.8441]],\n",
       "\n",
       "         [[-0.1830, -0.0716,  0.0479,  ..., -0.1443, -0.1981, -0.0695],\n",
       "          [ 0.3691,  0.2998, -0.7416,  ..., -1.4394, -0.9884, -1.8549],\n",
       "          [ 1.7590, -1.5582, -0.7722,  ..., -0.8009, -5.0405, -0.8411]],\n",
       "\n",
       "         [[-0.1224,  0.0269, -0.0225,  ..., -0.1736,  0.0276, -0.1703],\n",
       "          [ 0.9218,  1.8901, -0.0689,  ...,  0.0221,  0.7954, -1.3216],\n",
       "          [ 0.2818, -4.9092, -0.4829,  ...,  1.0085,  0.4012,  0.4743]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-0.4315, -0.1020,  0.0294,  ..., -0.1002,  0.1122,  0.8982],\n",
       "          [ 0.5660, -1.3578,  2.3000,  ...,  0.8023,  0.3354,  0.6597],\n",
       "          [ 0.7806, -0.9454,  0.6159,  ...,  1.9351, -0.9452,  2.4333]],\n",
       "\n",
       "         [[ 0.9352, -0.2005, -0.0733,  ...,  0.7355,  0.8333, -0.5958],\n",
       "          [-0.0955, -1.7305,  0.3901,  ..., -0.2580,  0.2779,  0.5430],\n",
       "          [-0.1083, -0.1621, -1.1166,  ...,  0.7042, -0.3042,  1.2606]],\n",
       "\n",
       "         [[ 1.8871, -1.3899,  0.3829,  ..., -0.2243,  0.2305, -1.0589],\n",
       "          [-2.2075, -2.2928,  2.1640,  ...,  1.0762, -2.0162, -0.6737],\n",
       "          [-0.6762, -1.2626,  0.5144,  ...,  1.1125, -0.5476, -0.8286]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5704, -0.6693,  1.2046,  ...,  0.2817,  0.8552,  1.1498],\n",
       "          [ 2.4493, -1.6716,  1.0169,  ..., -0.5151,  1.5769,  0.3203],\n",
       "          [ 1.2731, -0.8122,  2.6970,  ...,  0.0661,  0.4140, -0.5878]],\n",
       "\n",
       "         [[ 0.0982,  0.0859,  2.0299,  ..., -0.5146,  0.5690,  0.6795],\n",
       "          [-1.0114,  0.6413, -0.7130,  ..., -0.1956, -0.0068,  1.7561],\n",
       "          [ 1.5683, -0.5637,  1.9450,  ..., -0.8388,  0.2997,  0.4422]],\n",
       "\n",
       "         [[ 3.6490,  0.2482, -2.9401,  ..., -0.9993, -0.5515, -0.3386],\n",
       "          [ 2.8525, -0.5390, -2.0796,  ..., -1.4136, -1.8033,  0.7849],\n",
       "          [ 3.3548, -0.8683, -0.8269,  ...,  0.3246, -1.4073,  2.5682]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 0.0243, -0.1419, -0.7936,  ..., -0.7902, -0.3375, -0.1530],\n",
       "          [ 0.7423, -0.3670,  0.3123,  ...,  0.3703,  2.2100,  0.0531],\n",
       "          [-0.9094, -1.4515, -1.4021,  ..., -0.8589, -1.4025, -0.4623]],\n",
       "\n",
       "         [[-0.1161, -0.6045, -0.3281,  ..., -0.8253,  0.0414, -0.6346],\n",
       "          [ 3.4119, -2.5107, -2.5643,  ...,  1.8615, -1.3625, -1.3310],\n",
       "          [ 1.3664, -3.4593, -0.0629,  ..., -0.5124, -5.0804,  2.5258]],\n",
       "\n",
       "         [[-0.1382,  0.3439,  0.5311,  ...,  0.3179,  0.0196, -0.6297],\n",
       "          [-2.0833,  0.4565,  0.8059,  ..., -0.9411,  0.1120,  1.6984],\n",
       "          [-3.4358, -2.5550,  2.3949,  ..., -0.9278,  0.4462, -0.8148]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0087,  0.2530,  0.5621,  ...,  0.1837, -0.0307,  0.5475],\n",
       "          [ 0.9879, -0.0191, -0.6887,  ..., -2.1776, -0.8089,  0.4997],\n",
       "          [-0.3017,  0.1184, -0.0710,  ..., -0.3090, -2.2315,  1.1451]],\n",
       "\n",
       "         [[-0.0717,  0.7735, -0.0905,  ..., -0.1648,  0.8884,  0.3614],\n",
       "          [-0.1166, -0.3738, -0.0669,  ...,  1.3000, -0.6405, -1.9273],\n",
       "          [-0.1179,  1.1923, -1.2505,  ...,  2.3309,  1.5657, -1.2873]],\n",
       "\n",
       "         [[-0.5402,  0.2109, -1.3727,  ...,  0.4613, -0.8400, -0.1572],\n",
       "          [ 0.6743,  0.8960,  1.8667,  ...,  0.6169, -1.9153, -0.7095],\n",
       "          [ 3.6341, -1.3857,  0.5000,  ...,  1.4581, -1.0971, -0.3464]]]],\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-1.0799, -0.8696, -0.5513,  ...,  0.1670,  0.2329, -0.7019],\n",
       "          [-0.5763, -2.6603, -0.4913,  ...,  0.7710,  1.3768,  0.1127],\n",
       "          [-1.4015, -1.2968, -2.7757,  ...,  1.2359,  0.8435, -0.8345]],\n",
       "\n",
       "         [[-0.7409,  0.1335,  0.3939,  ...,  0.8545,  0.2489,  0.8745],\n",
       "          [-0.7867,  0.5450,  1.3869,  ...,  1.2275,  0.9990,  0.4256],\n",
       "          [-1.2511,  0.2848,  0.7866,  ...,  0.5124,  1.2157, -0.2830]],\n",
       "\n",
       "         [[-1.3235,  0.0612,  1.9077,  ...,  0.7835,  0.6441, -0.6100],\n",
       "          [-2.2514, -0.1180,  0.5314,  ..., -0.2720,  0.4001,  0.9797],\n",
       "          [-3.0585,  0.5723,  1.6223,  ..., -0.0775,  1.7994,  0.4073]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1218, -1.5370, -0.5138,  ...,  1.8058,  0.3001,  1.2573],\n",
       "          [-0.8095, -0.6318, -1.5894,  ...,  0.9248,  2.6497, -0.2794],\n",
       "          [-0.5794,  1.4548,  0.2069,  ...,  0.7417,  3.5668, -1.3056]],\n",
       "\n",
       "         [[ 0.8790, -0.3096, -0.4305,  ..., -1.1021,  0.8828, -4.7130],\n",
       "          [ 2.3554, -1.1622,  0.1780,  ..., -0.1619,  0.5978,  1.8232],\n",
       "          [ 1.6226,  0.0269, -0.0174,  ...,  1.8741, -0.4904,  3.3628]],\n",
       "\n",
       "         [[-0.3824,  0.5705, -0.0992,  ..., -0.5829, -0.5415,  0.5301],\n",
       "          [-0.8884, -2.5066,  0.8708,  ..., -2.2455, -1.3718,  1.5083],\n",
       "          [-1.3652, -2.8121,  0.5332,  ..., -1.5092, -0.5689,  0.7071]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-5.1217e-02, -1.2939e-01, -4.4601e-03,  ...,  2.8911e-01,\n",
       "            5.9391e-02, -1.1091e-02],\n",
       "          [ 5.3231e-01, -5.6186e-01,  1.4779e-01,  ..., -8.8513e-01,\n",
       "            1.3635e+00,  1.4790e+00],\n",
       "          [ 2.3053e+00,  2.0118e+00, -3.1613e+00,  ..., -2.4613e+00,\n",
       "            2.5662e+00,  4.5792e+00]],\n",
       "\n",
       "         [[ 4.1407e-02,  1.1150e-02, -5.7584e-02,  ...,  3.4734e-02,\n",
       "           -1.0025e-02,  8.3526e-02],\n",
       "          [ 4.6663e-01,  2.1243e+00, -3.4278e-02,  ..., -2.7581e+00,\n",
       "            1.7011e+00, -6.6392e-01],\n",
       "          [ 1.5329e+00,  5.2070e-01, -1.6719e+00,  ...,  1.0632e+00,\n",
       "           -4.4108e-01, -1.9710e+00]],\n",
       "\n",
       "         [[-1.0558e-01,  8.0188e-02, -1.0509e-02,  ...,  7.7476e-02,\n",
       "           -4.0770e-02,  1.9049e-01],\n",
       "          [-2.8732e-01,  1.7811e+00, -1.1113e+00,  ...,  8.1835e-01,\n",
       "           -2.1298e-01, -1.7906e-01],\n",
       "          [ 2.4920e+00, -2.3578e+00,  2.8242e+00,  ..., -1.0119e-01,\n",
       "           -1.3315e+00, -1.2412e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.5533e-01,  1.5650e-01,  4.0229e-03,  ...,  7.1491e-02,\n",
       "           -1.3037e-01,  1.4398e-02],\n",
       "          [-2.5350e-01, -7.4268e-01, -1.1563e+00,  ..., -3.6412e-01,\n",
       "            3.8001e+00,  3.9751e-01],\n",
       "          [ 1.0675e+00, -3.1280e+00, -1.9035e+00,  ..., -1.2572e+00,\n",
       "            1.1577e+00, -6.2391e-02]],\n",
       "\n",
       "         [[-4.8223e-02,  2.4330e-02,  7.4130e-02,  ..., -1.4475e-02,\n",
       "            5.3936e-03,  1.0158e-01],\n",
       "          [ 9.1264e-01, -3.9368e-01,  3.0174e-02,  ...,  3.3033e-01,\n",
       "            1.7868e+00,  1.8588e+00],\n",
       "          [ 2.6050e+00,  1.5382e+00,  3.0465e+00,  ...,  1.6702e+00,\n",
       "            3.7554e-01,  2.4079e+00]],\n",
       "\n",
       "         [[ 6.3615e-02, -3.5715e-01, -6.1391e-02,  ..., -2.9848e-01,\n",
       "           -1.9989e-01,  4.3809e-01],\n",
       "          [-1.4194e-01,  1.1218e+00,  2.2834e-01,  ...,  4.5026e-01,\n",
       "           -4.3673e-01,  2.2528e-01],\n",
       "          [-2.5382e+00,  2.1001e-01,  2.4087e-01,  ...,  5.7646e-01,\n",
       "            5.9802e-01,  4.7411e-01]]]], grad_fn=<TransposeBackward0>), tensor([[[[-0.9335, -0.8396, -0.2527,  ..., -1.9613, -0.7052, -0.0241],\n",
       "          [ 1.5760,  0.4133, -0.4780,  ..., -2.9163,  1.4427, -1.0442],\n",
       "          [-2.2457,  1.6227, -0.1990,  ..., -1.5761,  0.0474,  0.7525]],\n",
       "\n",
       "         [[ 3.5751,  0.0496,  0.2969,  ...,  0.4841,  0.7310,  0.3556],\n",
       "          [ 0.5949, -2.4965,  2.0236,  ..., -2.5393,  2.0502, -1.1371],\n",
       "          [ 4.3262,  0.0806,  3.2126,  ..., -3.7706, -0.1648, -1.0198]],\n",
       "\n",
       "         [[-1.0465, -0.1835,  1.8320,  ..., -2.1520, -0.3824, -0.3106],\n",
       "          [-0.8842, -2.6482,  0.9038,  ...,  1.1427,  2.5725,  1.5341],\n",
       "          [-0.3981,  1.1736,  1.5877,  ...,  0.8431,  0.3376, -1.4076]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.9097,  0.6322, -0.7881,  ..., -0.4857,  1.3169, -2.3982],\n",
       "          [ 3.2917,  1.4247,  0.1713,  ..., -1.8573,  2.1688,  0.1665],\n",
       "          [ 0.2223, -2.1606,  1.2090,  ..., -0.5373,  2.1043,  1.7362]],\n",
       "\n",
       "         [[ 0.5433, -0.0625, -0.6347,  ...,  0.0816,  0.3649, -0.9589],\n",
       "          [-2.4966,  1.0544, -2.5471,  ..., -1.0169, -1.2442, -0.6410],\n",
       "          [-2.3293,  1.3050,  0.5975,  ..., -2.3535, -1.3773,  0.1285]],\n",
       "\n",
       "         [[ 0.2414,  1.7278,  1.1718,  ...,  1.2023, -0.7337,  1.2951],\n",
       "          [-0.8670, -0.5604, -0.2011,  ...,  2.3975,  1.4498,  0.4487],\n",
       "          [-1.1652,  1.0330, -0.4913,  ...,  4.4379,  1.0117,  1.8207]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-0.0644,  0.1431, -0.1006,  ..., -0.3743,  0.2773, -0.2454],\n",
       "          [-3.5048, -3.4838, -0.4158,  ..., -1.6188,  0.0164,  0.1255],\n",
       "          [-0.4459, -1.2073,  3.0497,  ..., -1.4485, -2.7940, -0.1140]],\n",
       "\n",
       "         [[ 0.1633, -1.6199, -1.7003,  ...,  1.7547,  0.4425,  0.9769],\n",
       "          [-2.1906, -2.2267, -0.9334,  ...,  1.4888,  3.6915,  0.3308],\n",
       "          [-2.3732,  4.0673, -3.8805,  ..., -2.4145,  2.9915, -0.2158]],\n",
       "\n",
       "         [[-2.0407, -0.4175,  0.5318,  ..., -0.6667,  0.4876, -0.9708],\n",
       "          [-0.3993, -0.2094,  0.2460,  ...,  6.3637,  2.4267, -5.6165],\n",
       "          [ 4.4699,  3.4948,  4.5498,  ...,  5.5666, -0.2835, -2.2751]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2827, -0.8624,  1.5509,  ...,  2.6465, -1.2077,  0.0257],\n",
       "          [ 0.2172, -4.1605,  0.2431,  ..., -2.1785,  2.3618, -0.8644],\n",
       "          [ 3.6266, -2.3951,  2.6117,  ...,  1.1275,  0.4250,  2.1398]],\n",
       "\n",
       "         [[-0.1447,  0.5588, -0.3500,  ...,  0.4961, -0.1232, -0.0464],\n",
       "          [ 1.9778,  2.4689,  1.4366,  ...,  0.0610, -0.4787,  0.7569],\n",
       "          [ 2.5257,  5.9018, -0.6852,  ..., -2.2817, -1.0754, -0.8383]],\n",
       "\n",
       "         [[-1.1879, -2.3262,  1.6121,  ...,  0.6969,  0.4698, -0.2933],\n",
       "          [ 2.5550, -1.1357,  1.1250,  ..., -1.1353,  1.1709,  0.8278],\n",
       "          [-0.9660, -1.8379,  1.7132,  ..., -1.2892, -2.4411, -0.7718]]]],\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 2.0900,  2.1918,  2.4538,  ...,  2.0690, -0.5154, -0.1946],\n",
       "          [ 1.0773,  0.5579,  2.4395,  ..., -0.6825, -0.9442, -1.4814],\n",
       "          [-1.6505,  0.7209,  3.5186,  ..., -0.7428, -2.3174, -2.5886]],\n",
       "\n",
       "         [[-0.7243, -0.0142, -0.6002,  ...,  0.6687, -1.0710,  0.6133],\n",
       "          [-0.0832, -0.3202, -2.1519,  ...,  1.0593, -2.2134,  0.9643],\n",
       "          [ 0.3018,  0.9524, -2.6983,  ...,  1.2960, -1.1230,  0.3891]],\n",
       "\n",
       "         [[ 0.7630, -3.4165, -0.8202,  ...,  1.4704,  0.4119,  1.0249],\n",
       "          [-1.0686, -1.5526,  1.1413,  ..., -1.3740,  0.7577,  0.0058],\n",
       "          [-0.6469, -2.8653,  0.3024,  ..., -0.3122,  0.4132, -0.8546]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.3097, -0.1628, -0.1801,  ..., -2.0079,  1.3841, -1.3792],\n",
       "          [ 0.4612, -3.4827, -1.4615,  ..., -2.6220,  1.1545, -2.0978],\n",
       "          [-1.0817, -3.8654, -2.1557,  ..., -2.5839,  1.5667, -1.8943]],\n",
       "\n",
       "         [[ 0.5727, -0.3993, -0.2675,  ..., -0.5831, -0.0913, -0.2781],\n",
       "          [ 1.2306,  0.6211, -0.4378,  ..., -0.8744, -0.4788, -0.6984],\n",
       "          [ 0.4249, -0.9467,  0.7000,  ..., -1.8098, -0.6163,  0.0927]],\n",
       "\n",
       "         [[-0.1709,  0.6759,  2.0382,  ...,  0.2671,  0.1851, -0.4972],\n",
       "          [-1.1569,  2.3152,  1.7172,  ...,  0.8988, -0.9139, -1.4173],\n",
       "          [-2.5327,  1.4586,  0.8692,  ...,  2.6736,  0.1736, -2.0791]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-1.8999e-02, -8.4820e-01, -8.1142e-01,  ...,  7.6843e-01,\n",
       "           -7.8794e-01,  2.9732e-01],\n",
       "          [-1.8335e+00, -1.1309e+00,  8.1153e-01,  ..., -3.6070e-02,\n",
       "           -2.9170e+00, -6.9696e-01],\n",
       "          [-3.9900e-01,  4.6119e-01, -4.4591e-02,  ..., -1.3395e+00,\n",
       "           -2.1565e+00, -1.2155e+00]],\n",
       "\n",
       "         [[ 1.7928e-01, -2.9857e-01,  2.1389e-02,  ..., -1.1863e-01,\n",
       "           -2.7593e-01, -4.7293e-01],\n",
       "          [ 1.7800e+00, -2.9948e+00, -3.7342e+00,  ..., -1.6913e+00,\n",
       "           -1.0919e+00,  1.4240e+00],\n",
       "          [ 5.4199e+00,  4.8199e-01, -8.3637e-01,  ..., -5.9453e+00,\n",
       "           -3.2919e+00, -8.1036e-01]],\n",
       "\n",
       "         [[ 2.0289e-02,  2.7303e-01, -1.9323e-01,  ..., -2.6374e-01,\n",
       "            9.0080e-02,  8.0836e-02],\n",
       "          [ 1.4789e-01, -1.1406e+00,  1.9014e+00,  ..., -8.2283e-01,\n",
       "            5.3856e-01,  1.3819e+00],\n",
       "          [ 2.3222e+00,  1.0730e+00,  4.1428e+00,  ..., -1.2001e+00,\n",
       "            3.3408e-01, -4.5838e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.9711e-02, -7.2854e-02,  2.6933e-02,  ..., -8.3554e-02,\n",
       "           -8.2822e-02,  4.7452e-01],\n",
       "          [ 9.4188e-01, -2.7378e+00,  1.8959e+00,  ...,  2.2647e+00,\n",
       "           -7.1506e-01, -6.1741e-01],\n",
       "          [ 2.9245e+00, -4.5899e+00, -1.4498e-02,  ...,  1.8569e+00,\n",
       "            3.3653e+00,  1.4067e+00]],\n",
       "\n",
       "         [[ 1.0918e-01, -1.1845e-02,  6.3754e-02,  ..., -2.6492e-01,\n",
       "            9.9016e-02,  5.4479e-02],\n",
       "          [-2.9271e+00, -1.4971e+00, -1.1398e+00,  ...,  7.0510e-01,\n",
       "            2.5991e+00,  1.3189e+00],\n",
       "          [ 4.2350e+00,  1.7367e+00,  1.7340e+00,  ...,  4.2711e-02,\n",
       "            4.9845e+00,  3.3070e-01]],\n",
       "\n",
       "         [[-5.6599e-02, -1.9072e-01, -3.1837e-02,  ..., -1.7003e-01,\n",
       "           -2.7134e-03, -1.1508e-01],\n",
       "          [-3.3839e+00,  4.5029e-01, -8.4532e-01,  ...,  2.6165e+00,\n",
       "            7.6993e-01,  1.6910e+00],\n",
       "          [-8.2935e-01, -1.0235e+00, -2.3029e+00,  ...,  6.5525e+00,\n",
       "           -3.1939e-01,  3.1842e+00]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 8.8734e-02, -2.6080e-02, -2.4378e+00,  ..., -1.2205e+00,\n",
       "           -5.3483e-01,  1.2460e+00],\n",
       "          [ 2.4853e+00, -3.4388e-01,  4.0074e-01,  ...,  2.7330e+00,\n",
       "            9.0821e-01, -2.3814e+00],\n",
       "          [ 8.5320e-01, -1.8636e+00, -1.8708e+00,  ..., -3.3748e-01,\n",
       "           -1.7218e-01, -2.2607e+00]],\n",
       "\n",
       "         [[ 2.1375e-01, -5.4697e-01, -5.0122e-01,  ...,  4.1312e-01,\n",
       "            1.7947e-01,  6.5221e-01],\n",
       "          [-2.5466e-01, -1.5442e+00,  1.5842e+00,  ..., -1.7287e+00,\n",
       "           -1.2815e+00, -1.0954e+00],\n",
       "          [-9.6301e-01, -3.2624e+00,  2.7518e-01,  ..., -1.3120e-02,\n",
       "            4.4148e-02,  1.5774e+00]],\n",
       "\n",
       "         [[-5.4480e-01, -1.7828e+00, -1.0812e+00,  ...,  9.9414e-01,\n",
       "            5.4922e-01, -5.2005e+00],\n",
       "          [ 4.1434e-01, -1.5297e+00,  1.3175e+00,  ..., -2.7121e+00,\n",
       "           -1.6810e+00,  1.5788e+00],\n",
       "          [-6.0274e-02,  7.6625e-01, -8.5638e-01,  ..., -1.9895e+00,\n",
       "           -1.5752e+00, -2.2734e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.2297e-01, -4.7728e-01, -7.1955e-01,  ...,  1.6768e-01,\n",
       "            1.9667e-01, -4.8655e-01],\n",
       "          [ 1.6284e+00, -8.6698e-01, -2.0980e-01,  ..., -3.8810e-02,\n",
       "            4.9539e-03, -1.0755e+00],\n",
       "          [ 1.2720e+00,  1.3946e+00,  1.9665e+00,  ...,  8.7439e-01,\n",
       "           -3.3752e-01,  1.3927e+00]],\n",
       "\n",
       "         [[-8.8747e-01,  9.0874e-01, -5.8409e-01,  ...,  2.4682e-01,\n",
       "            3.7004e+00,  5.6559e-01],\n",
       "          [ 1.0990e+00, -1.6356e+00, -3.4238e-01,  ...,  1.3111e+00,\n",
       "           -9.1645e-01, -9.7451e-01],\n",
       "          [-3.1694e-01, -1.8144e-01, -6.2831e-01,  ...,  3.1462e+00,\n",
       "            6.2492e-01, -4.8089e-01]],\n",
       "\n",
       "         [[-1.1349e-01,  1.8464e-01,  6.8661e-01,  ..., -6.2199e-01,\n",
       "            4.5980e-01,  5.2529e-01],\n",
       "          [ 2.5163e-01,  3.1905e+00, -4.2891e-01,  ..., -2.0382e+00,\n",
       "            1.9587e+00, -1.1219e+00],\n",
       "          [-1.3193e+00,  2.1324e+00,  1.0377e+00,  ...,  5.0677e-01,\n",
       "           -1.0928e+00, -2.5914e+00]]]], grad_fn=<TransposeBackward0>), tensor([[[[-0.0689,  0.0583, -0.4919,  ...,  0.3878,  0.4490,  0.4292],\n",
       "          [-0.8043, -0.6426, -2.4756,  ...,  2.7933, -0.1207,  0.8541],\n",
       "          [ 2.5261,  3.2588, -0.8434,  ...,  2.6057, -2.2657, -4.0448]],\n",
       "\n",
       "         [[-0.4442,  0.3642,  0.1170,  ..., -0.2261,  0.5705,  0.2153],\n",
       "          [-2.6048,  0.2495,  1.3002,  ..., -0.9953, -4.3983, -2.4920],\n",
       "          [-1.3089,  1.5568, -0.3701,  ...,  1.7433, -3.4542, -8.2947]],\n",
       "\n",
       "         [[ 0.8935, -0.1162,  0.4822,  ..., -0.5233,  0.6208, -0.3752],\n",
       "          [ 0.2148,  0.3779,  0.5203,  ...,  3.2035, -0.9669,  1.6483],\n",
       "          [-2.7195, -7.5741, -0.7410,  ...,  0.4938, -4.7649,  4.1209]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2052,  2.7864,  0.9406,  ..., -0.5755, -0.0424, -0.9935],\n",
       "          [ 0.1338,  3.2641,  1.4913,  ..., -3.3808,  0.3622, -1.1548],\n",
       "          [ 1.2805,  3.3563,  2.5421,  ..., -3.8806,  3.2726, -0.3519]],\n",
       "\n",
       "         [[-0.0459, -0.6943, -0.6537,  ..., -1.3019,  0.5773, -0.7560],\n",
       "          [-0.8283, -0.9860, -3.3830,  ...,  0.7416, -1.9135,  4.4705],\n",
       "          [-0.7548, -1.8878,  0.6307,  ...,  0.4503,  0.2495,  0.0951]],\n",
       "\n",
       "         [[ 0.1343, -0.4096, -0.0106,  ..., -0.5800,  0.8645,  0.1267],\n",
       "          [ 1.7461, -1.6336,  2.0874,  ..., -1.5473,  3.2592, -0.1715],\n",
       "          [-4.9783, -2.0054,  3.9475,  ..., -5.7463, -0.6741, -0.1781]]]],\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-0.0042,  0.8726, -1.7387,  ..., -0.7135, -0.1226,  0.4709],\n",
       "          [-1.4646, -0.8228, -1.4508,  ...,  0.6224, -0.1231,  1.5258],\n",
       "          [-0.7483, -1.1288, -2.7616,  ...,  1.5861, -0.6194,  1.5143]],\n",
       "\n",
       "         [[-2.7134, -0.6992,  1.2512,  ...,  0.1068,  1.0789,  1.5087],\n",
       "          [-3.3980, -0.5498,  1.6374,  ..., -1.3058,  0.2114,  3.9391],\n",
       "          [-3.5246,  0.8530,  2.8521,  ..., -0.6004,  0.7983,  3.0900]],\n",
       "\n",
       "         [[ 0.0541,  1.4676, -1.8800,  ..., -0.0657, -1.4928, -0.4899],\n",
       "          [ 0.7037,  3.4595, -2.2501,  ..., -0.7963, -2.8287, -2.4006],\n",
       "          [ 1.6746,  2.2675, -3.8135,  ..., -1.4617, -3.9726, -0.5546]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3801,  0.8184, -0.4074,  ...,  0.4541, -0.2589, -2.2266],\n",
       "          [ 3.3705, -0.7989,  1.0516,  ...,  1.0535,  1.2661, -1.3879],\n",
       "          [ 3.1892, -1.3857, -0.0095,  ...,  2.2305,  2.0003, -0.2092]],\n",
       "\n",
       "         [[-0.3796,  0.7513,  0.4358,  ..., -0.3829,  0.1577, -1.2898],\n",
       "          [-0.8689,  3.8774,  1.4408,  ..., -1.3076, -1.9101, -3.0388],\n",
       "          [ 0.6513,  2.9407,  1.8734,  ..., -2.5491, -1.5842, -1.5568]],\n",
       "\n",
       "         [[-0.3850,  0.9549, -0.6012,  ...,  0.2960, -0.6577,  0.8976],\n",
       "          [-1.2189,  0.5173,  0.0353,  ..., -0.0654,  0.4308,  1.1716],\n",
       "          [-1.8098,  0.5362,  1.4902,  ..., -0.2731, -0.3028,  0.4745]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 2.4514e-01,  2.0517e-01, -1.9721e-01,  ...,  2.9018e-01,\n",
       "            3.8202e-02,  3.1615e-01],\n",
       "          [ 1.0766e+00, -5.9436e+00,  2.1562e+00,  ..., -8.6449e-01,\n",
       "           -2.1148e-01,  3.9243e+00],\n",
       "          [-1.1701e+00, -4.0404e+00,  2.3470e+00,  ..., -1.0353e+00,\n",
       "            1.4225e+00,  7.0328e-01]],\n",
       "\n",
       "         [[-9.9214e-02, -4.9430e-01, -4.6734e-01,  ..., -2.0976e-01,\n",
       "           -4.2670e-01,  2.6243e-01],\n",
       "          [ 1.3812e+00,  1.1880e+00,  7.4742e-01,  ..., -1.3832e+00,\n",
       "            9.8011e-02,  2.4654e+00],\n",
       "          [-3.2442e+00,  2.9490e+00, -5.3918e+00,  ...,  1.1772e+00,\n",
       "            2.8091e+00, -3.3215e+00]],\n",
       "\n",
       "         [[-1.1220e-01, -9.3784e-02, -1.0007e-01,  ...,  9.1648e-02,\n",
       "           -1.3648e-01,  1.1037e-01],\n",
       "          [-3.2095e+00, -2.0418e-01,  1.1859e+00,  ...,  2.6830e+00,\n",
       "           -1.5143e+00, -2.1535e+00],\n",
       "          [-3.2519e+00,  2.9770e-01, -1.1370e+00,  ...,  9.0963e-01,\n",
       "            2.1290e+00, -2.1679e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.2936e-01, -4.1617e-01, -1.3627e-01,  ..., -2.8406e-01,\n",
       "            7.0658e-02,  3.4637e-01],\n",
       "          [ 3.5586e-01,  3.4420e+00,  4.8035e-01,  ..., -3.2909e+00,\n",
       "            4.0463e+00, -8.4251e-01],\n",
       "          [ 4.3520e-01,  4.3887e+00, -1.6372e+00,  ...,  2.0259e-01,\n",
       "            2.9954e+00, -3.8417e+00]],\n",
       "\n",
       "         [[-1.7691e-01, -1.9848e-01, -3.7998e-02,  ...,  6.5931e-02,\n",
       "            2.5506e-01, -1.2391e-01],\n",
       "          [-9.5595e-02, -1.5776e+00, -1.6198e+00,  ..., -2.0196e-01,\n",
       "            7.4615e-01, -8.3027e-01],\n",
       "          [ 2.9570e+00,  9.8600e-01, -3.2093e+00,  ..., -2.3867e+00,\n",
       "           -1.0991e+00, -3.3022e+00]],\n",
       "\n",
       "         [[-9.0722e-02, -1.0127e-01,  8.6811e-03,  ..., -1.9347e-01,\n",
       "           -2.2971e-03, -1.5630e-02],\n",
       "          [-8.3846e-02, -1.9582e+00, -1.7120e+00,  ...,  1.3005e+00,\n",
       "           -3.4689e+00,  5.3882e-01],\n",
       "          [ 5.3396e-01,  1.7507e+00,  3.2471e-01,  ...,  9.3196e-02,\n",
       "           -2.2428e+00, -4.6865e+00]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 6.0575, -0.3672,  8.0873,  ..., -2.0098, -1.8023,  0.9657],\n",
       "          [ 2.9958,  5.0789,  1.1982,  ...,  1.5993, -1.0090,  1.1165],\n",
       "          [ 6.9864,  3.8615,  5.8116,  ..., -1.6803, -0.5348,  4.8782]],\n",
       "\n",
       "         [[-0.8556,  4.2451, -4.0444,  ...,  1.6182, -0.4795, -0.3867],\n",
       "          [-0.4050, -7.2184,  7.7603,  ...,  1.3087, -5.4288, -3.3457],\n",
       "          [-0.8100,  0.3020,  0.5915,  ..., -0.6154, -1.3429, -1.5179]],\n",
       "\n",
       "         [[ 0.7644,  1.0921, -1.3533,  ..., -0.4148, -4.7640, -1.6071],\n",
       "          [-2.9937,  0.3331,  0.0296,  ...,  3.6686, -1.5278, -1.2619],\n",
       "          [-1.4224,  1.0407,  1.1761,  ..., -0.2798, -0.6683, -0.8804]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.4112,  1.1483, -3.1615,  ..., -2.5393,  0.6715, -3.1705],\n",
       "          [-0.4778,  2.0911, -2.4399,  ..., -4.1122, -4.4129,  5.1171],\n",
       "          [-0.0347, -0.0126, -1.0717,  ..., -4.9479, -0.6070, -1.1042]],\n",
       "\n",
       "         [[ 1.1086,  0.6205, -0.1000,  ...,  1.5646,  0.2178,  1.2468],\n",
       "          [-3.4887,  3.1464, -1.2671,  ..., -1.2952, -4.7813, -2.9554],\n",
       "          [ 2.2076,  2.2794,  0.9363,  ...,  0.2087,  0.1927,  1.8241]],\n",
       "\n",
       "         [[ 0.9858, -0.3945,  0.0096,  ..., -0.3135, -0.5553, -0.0996],\n",
       "          [ 0.0443, -0.9648, -0.0947,  ..., -0.5970,  2.0252,  0.6818],\n",
       "          [ 2.5456, -1.5497, -1.2776,  ..., -0.0795,  2.5376,  2.1992]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-1.1046e-01, -2.9016e+00, -1.8864e-01,  ...,  6.6322e-01,\n",
       "            8.3086e-01, -8.0972e-02],\n",
       "          [-4.7760e+00,  7.4723e+00, -1.9515e+00,  ...,  3.7237e-01,\n",
       "            2.1470e-01, -4.0311e+00],\n",
       "          [ 2.0676e+00,  1.3967e+01,  2.4838e+00,  ..., -2.3263e+00,\n",
       "           -6.5311e-01, -6.1978e+00]],\n",
       "\n",
       "         [[ 7.5874e-01, -7.5531e-01, -4.3326e-01,  ...,  7.4344e-01,\n",
       "            1.6446e+00,  1.0252e+00],\n",
       "          [ 6.6588e+00, -2.2486e+00,  5.0108e-01,  ..., -1.3266e+00,\n",
       "           -2.8464e+00, -3.9558e+00],\n",
       "          [ 5.0420e+00,  5.8263e+00, -4.4991e+00,  ..., -3.6834e+00,\n",
       "           -6.6078e+00,  3.1071e+00]],\n",
       "\n",
       "         [[ 2.4227e-01, -1.2880e+00,  5.1696e-02,  ..., -1.8208e-01,\n",
       "           -6.4040e-01, -5.6198e-01],\n",
       "          [-3.3569e+00,  6.8882e+00,  1.4408e+00,  ..., -4.0848e+00,\n",
       "           -3.4315e-01,  1.8426e+00],\n",
       "          [-4.8049e+00,  4.2831e+00,  3.8110e+00,  ..., -9.0701e+00,\n",
       "           -5.6704e+00,  2.7169e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.2308e-01, -6.7794e-02,  1.2511e+00,  ...,  1.5349e+00,\n",
       "            2.2966e-01, -7.2365e-01],\n",
       "          [ 3.7213e+00,  1.7698e+00,  4.0405e+00,  ...,  3.9911e+00,\n",
       "            9.2516e+00,  4.9049e+00],\n",
       "          [ 5.5665e+00,  4.5939e+00,  5.8379e+00,  ...,  5.1064e-01,\n",
       "            2.5838e+00, -1.5623e+00]],\n",
       "\n",
       "         [[-1.4544e-01,  1.6280e-01, -2.7988e-03,  ...,  1.3079e-01,\n",
       "            2.7628e-01,  5.1406e-02],\n",
       "          [-5.0715e+00,  1.4495e+00, -6.3497e-01,  ..., -2.6976e+00,\n",
       "            2.5427e+00,  1.5700e+00],\n",
       "          [-2.9606e+00,  1.1767e+00,  4.1003e+00,  ...,  2.3559e-01,\n",
       "            5.3146e-01, -3.7014e-01]],\n",
       "\n",
       "         [[ 8.4094e-01, -7.4256e-01, -1.3522e+00,  ..., -2.8393e-01,\n",
       "            1.2843e+00,  5.9655e-01],\n",
       "          [ 7.3919e-02, -4.0686e+00,  1.1473e+00,  ...,  1.9607e+00,\n",
       "            1.9836e+00,  1.9787e+00],\n",
       "          [-3.1096e+00,  1.2499e+00,  1.2559e+00,  ...,  1.2577e+00,\n",
       "            4.3923e+00, -4.5153e+00]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-0.2248, -0.1670,  3.1635,  ...,  1.0193, -0.5814, -0.6587],\n",
       "          [-2.6769, -1.4596,  3.5817,  ...,  1.2214,  1.6829, -2.6003],\n",
       "          [-3.9255, -0.8742,  1.7921,  ...,  0.2814,  2.5257, -3.8580]],\n",
       "\n",
       "         [[ 0.3781,  0.9105,  1.6799,  ..., -2.3702,  2.0299, -1.9861],\n",
       "          [ 2.0837,  1.1289,  3.4722,  ..., -1.8426,  0.8665, -3.4563],\n",
       "          [ 1.4743,  0.5575,  3.0332,  ..., -1.6627, -0.5800, -2.5980]],\n",
       "\n",
       "         [[-1.4167, -0.8533,  0.2365,  ...,  0.8410,  0.4136,  1.4163],\n",
       "          [-1.2638, -1.3069, -1.0191,  ...,  1.3421, -0.7199,  4.4497],\n",
       "          [-0.5109, -0.3531, -0.3395,  ...,  2.0371, -2.3454,  3.3083]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.2886, -0.4313, -0.4936,  ..., -1.9834, -0.8072,  3.1920],\n",
       "          [-2.0007,  1.0439,  0.1850,  ..., -3.8250, -1.4645,  2.6436],\n",
       "          [-0.7421,  1.3521, -0.0137,  ..., -4.1624, -1.8443,  3.1201]],\n",
       "\n",
       "         [[-1.2965,  0.8137, -0.2918,  ..., -0.6592,  1.8026,  0.2612],\n",
       "          [-2.4027,  1.3857, -1.4450,  ..., -1.7734,  1.5969,  0.1863],\n",
       "          [-1.4660,  1.2564, -1.1089,  ..., -2.0914, -0.3276, -0.2942]],\n",
       "\n",
       "         [[ 0.6568,  0.8633,  0.3937,  ...,  0.2519,  4.0546, -1.0628],\n",
       "          [ 0.8607,  1.6443,  1.4059,  ...,  0.2244,  4.9812, -3.1032],\n",
       "          [ 0.8350,  2.4185,  1.6668,  ...,  0.5671,  4.4739, -3.1054]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-1.3667e-01, -2.1000e+00, -5.5650e-01,  ...,  7.3077e-02,\n",
       "           -7.2225e-02, -3.1616e-01],\n",
       "          [ 1.0745e+00,  2.5529e+00, -7.6877e-01,  ...,  8.2481e-01,\n",
       "            1.4790e+00, -3.2309e+00],\n",
       "          [ 5.9074e-01,  1.6935e+00, -2.9180e-01,  ..., -1.5097e+00,\n",
       "            1.4168e+00, -4.1291e+00]],\n",
       "\n",
       "         [[-2.4625e-01,  7.8708e-01,  2.2894e-01,  ...,  1.7922e-01,\n",
       "            2.6452e-01,  6.1333e-01],\n",
       "          [-5.1108e+00, -2.0225e-01, -1.5490e+00,  ...,  9.0793e-01,\n",
       "            3.2086e+00, -3.0288e+00],\n",
       "          [-2.5895e+00, -1.0523e+00, -1.6546e+00,  ..., -3.7015e+00,\n",
       "            5.8756e+00, -1.2644e+00]],\n",
       "\n",
       "         [[-7.4454e-01, -1.1806e-01,  6.4536e-02,  ..., -8.0928e-01,\n",
       "           -4.1893e-02, -2.7920e-01],\n",
       "          [-1.8091e+00,  1.1226e+00,  3.4247e+00,  ..., -2.3300e+00,\n",
       "           -1.1662e+00,  2.9465e+00],\n",
       "          [ 3.3128e-01,  6.0186e-01,  2.7666e+00,  ..., -1.4777e+00,\n",
       "           -1.5706e+00, -6.0891e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.2604e-01, -1.6604e-02,  1.5173e-01,  ..., -3.8591e-01,\n",
       "            7.8586e-01,  1.5010e-01],\n",
       "          [ 4.4846e+00,  1.6950e-02,  1.7973e+00,  ...,  2.4310e+00,\n",
       "            3.3444e+00,  3.4799e+00],\n",
       "          [ 5.1788e+00,  3.3540e-02,  2.1983e+00,  ...,  4.5243e+00,\n",
       "            4.1034e+00,  4.8906e+00]],\n",
       "\n",
       "         [[-2.2391e-01, -3.0609e-01,  1.5731e-02,  ...,  2.9518e-02,\n",
       "           -3.1900e-01, -4.0537e-01],\n",
       "          [-3.9603e+00,  6.1408e-01, -2.9252e-01,  ..., -6.4297e-01,\n",
       "           -2.1949e+00,  3.5192e-01],\n",
       "          [ 3.8890e-01, -5.8603e-03, -2.5507e+00,  ..., -6.8057e-01,\n",
       "           -4.4303e+00,  3.1912e-01]],\n",
       "\n",
       "         [[-3.1076e-02, -1.6035e-01, -2.8787e-01,  ...,  3.9142e-01,\n",
       "            4.2925e-02,  9.3214e-02],\n",
       "          [-3.1034e-01, -3.6725e+00, -2.1110e+00,  ...,  2.7481e+00,\n",
       "            1.0570e+00, -2.3986e+00],\n",
       "          [-1.2044e+00, -6.6476e+00, -1.2837e+00,  ...,  3.2409e+00,\n",
       "            6.9512e+00, -3.8149e+00]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 2.0067, -0.7354,  0.1122,  ..., -0.9194, -1.9448, -0.8684],\n",
       "          [-1.4975,  2.2563,  0.1160,  ..., -2.0038, -0.6673, -2.1203],\n",
       "          [-1.4759, -1.3311,  2.6147,  ..., -0.6332, -1.5641, -2.7554]],\n",
       "\n",
       "         [[-0.0503,  1.1526, -0.5862,  ..., -1.5449,  0.6332, -1.4549],\n",
       "          [-0.6989, -2.1652, -0.2127,  ...,  2.0380, -0.6737, -0.8756],\n",
       "          [-0.3016,  2.1563, -0.9289,  ...,  0.0302, -0.8635, -0.1444]],\n",
       "\n",
       "         [[-1.3363, -2.3527,  0.9939,  ..., -1.4376, -0.5787,  1.4984],\n",
       "          [ 2.2186, -0.0905,  1.2913,  ...,  3.8745,  1.8508, -4.0399],\n",
       "          [ 0.9991, -1.3072,  0.3900,  ...,  0.2662,  1.5839, -0.4677]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2266,  0.4825,  0.1191,  ..., -9.5250,  0.7021, -0.8796],\n",
       "          [-1.6021,  2.1743,  0.7623,  ..., -1.6699, -1.6989, -0.2564],\n",
       "          [-1.5015, -0.7744,  2.2366,  ..., -7.1930, -1.0050, -2.1725]],\n",
       "\n",
       "         [[ 0.5191, -0.3649,  1.5718,  ...,  0.3745,  0.0195,  0.6954],\n",
       "          [ 0.6456, -2.1898,  1.9902,  ..., -2.5093, -1.0209, -1.6619],\n",
       "          [ 1.2073, -0.9784,  0.7770,  ..., -1.0757,  0.7277, -0.0561]],\n",
       "\n",
       "         [[-1.1155, -0.3165, -0.8682,  ..., -0.6803,  3.0457,  0.3404],\n",
       "          [ 4.8546,  0.1351, -2.7742,  ...,  1.2746, -1.4872,  0.5982],\n",
       "          [ 1.1675,  0.4338,  0.1980,  ..., -0.2916,  2.8171, -0.4937]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 1.4181e-02, -3.9055e-01,  5.6747e-01,  ..., -2.6110e-01,\n",
       "            1.6605e-02, -3.8286e-01],\n",
       "          [-2.2157e+00,  2.9512e+00,  5.0819e+00,  ...,  2.2643e+00,\n",
       "           -7.2976e-01, -2.4908e-01],\n",
       "          [ 1.0746e+01,  1.5834e+00,  1.1687e+00,  ...,  6.0122e-01,\n",
       "           -7.7511e+00, -9.9418e-01]],\n",
       "\n",
       "         [[-1.7080e-01, -3.1272e-01,  7.1436e-02,  ..., -6.9522e-01,\n",
       "            3.5863e-01,  7.3134e-01],\n",
       "          [ 1.2437e+00,  1.4817e+00,  1.2264e+00,  ...,  9.4521e-01,\n",
       "            5.0069e+00, -4.1339e+00],\n",
       "          [-4.8365e+00,  1.7811e+00, -2.2010e+00,  ..., -1.5379e+00,\n",
       "           -3.1764e+00, -2.9469e+00]],\n",
       "\n",
       "         [[-5.6227e-01, -1.0678e-01, -2.4468e-01,  ...,  4.0243e-01,\n",
       "           -1.8864e-01, -4.3039e-01],\n",
       "          [ 1.7721e-01, -1.3934e+00,  4.4707e+00,  ..., -1.7596e+00,\n",
       "            4.5130e+00, -1.9502e-01],\n",
       "          [ 9.0553e-01, -1.1504e+01, -1.0471e+01,  ..., -6.9865e-01,\n",
       "            1.3792e+01,  6.1713e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.7474e-01,  2.1895e-01,  3.3770e-01,  ..., -1.8846e-01,\n",
       "            6.0948e-01, -3.8745e-01],\n",
       "          [ 1.7291e+00, -5.3147e+00, -8.1112e+00,  ...,  2.6303e-01,\n",
       "           -3.0328e+00,  3.8051e+00],\n",
       "          [-4.3489e+00, -1.1270e+01, -8.6697e+00,  ...,  7.8201e+00,\n",
       "            5.4317e-01, -1.9959e+00]],\n",
       "\n",
       "         [[-2.1687e-01,  8.1170e-02, -1.1243e-01,  ..., -5.8586e-01,\n",
       "            7.7005e-03, -1.2532e-02],\n",
       "          [ 2.5625e+00,  4.4561e+00, -1.8868e+00,  ...,  4.3303e+00,\n",
       "           -7.4760e+00,  5.8410e+00],\n",
       "          [ 4.2111e+00,  5.5108e+00, -7.8843e+00,  ...,  5.0099e+00,\n",
       "           -4.6006e+00,  2.1584e-01]],\n",
       "\n",
       "         [[-5.7797e-01, -2.0087e-01,  7.2495e-01,  ...,  1.7377e-01,\n",
       "            3.1557e-01,  5.0267e-01],\n",
       "          [-6.9834e+00, -2.3109e+00, -1.6527e+00,  ..., -4.9930e+00,\n",
       "            3.7800e+00, -7.7579e+00],\n",
       "          [-1.7172e+00,  4.7333e+00,  2.7027e+00,  ..., -7.5692e-01,\n",
       "           -1.0009e+00, -2.1212e+00]]]], grad_fn=<TransposeBackward0>))), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.0022,  0.0061, -0.0031,  ..., -0.0014, -0.0044,  0.0064],\n",
       "         [ 0.0719, -0.0596,  0.0446,  ...,  0.0784,  0.0225, -0.2440],\n",
       "         [ 0.0057, -0.1413,  0.0802,  ...,  0.0298,  0.0967,  0.0727]]],\n",
       "       grad_fn=<MulBackward0>), encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The quick brown fox\"\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\", use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32099,    33,  1633,     1],\n",
       "        [  216, 32099,  1633,     1],\n",
       "        [  216,    33, 32099,     1],\n",
       "        [  216,    33,  1633, 32099]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"He are cool\"\n",
    "tokenized = tokenizer(input_text, return_tensors=\"pt\")\n",
    "input_ids = tokenized.input_ids\n",
    "attention_mask = tokenized.attention_mask\n",
    "seq_length = input_ids.shape[1]\n",
    "\n",
    "# Mask out each token in the input sequence\n",
    "masked_input_ids = input_ids.repeat(seq_length, 1)\n",
    "masked_attention_mask = attention_mask.repeat(seq_length, 1)\n",
    "masked_input_ids.fill_diagonal_(mask_token)\n",
    "masked_input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32099"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token = tokenizer.convert_tokens_to_ids(tokenizer.additional_special_tokens[0])\n",
    "mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ids = masked_input_ids.tril()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "decoder_inputs = torch.tensor([[model.config.decoder_start_token_id, mask_token]]).expand(seq_length, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(input_ids=masked_input_ids, attention_mask=masked_attention_mask, decoder_input_ids=decoder_inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-38.9922, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[torch.arange(seq_length), 1, input_ids.squeeze()].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cool guys', '<extra_id_0> is', '<extra_id_0>.', 'cool cool']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(logits.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<extra_id_0> are cool</s>',\n",
       " 'He <extra_id_0> cool</s>',\n",
       " 'He are <extra_id_0> </s>',\n",
       " 'He are cool <extra_id_0>']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(masked_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [216, 33, 1633, 5, 1], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"He are cool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\", use_fast=False)\n",
    "model = RobertaForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Paris'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"The capital of France is <mask>.\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of <mask>\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "\n",
    "# labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# mask labels of non-<mask> tokens\n",
    "# labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "# outputs = model(**inputs, labels=labels)\n",
    "# round(outputs.loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Paris Lyon Nice'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.topk(logits[0, mask_token_index], 3, -1).indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2201)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0, 6].argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2201])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='FacebookAI/roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 31414, 232, 2], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma = AutoTokenizer.from_pretrained(\"google/gemma-2b\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.130485951900482"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "t = torch.rand(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1305)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/23/2024 00:57:09|INFO|logging|00| Loaded pretrained model file /afs/cs.stanford.edu/u/ananthag/.~DeBERTa/assets/latest/deberta-v3-base/pytorch_model.bin\n",
      "05/23/2024 00:57:13|WARNING|logging|00| Missing keys: [], unexpected_keys: ['mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias'], error_msgs: []\n"
     ]
    }
   ],
   "source": [
    "from DeBERTa.DeBERTa.apps.models import masked_language_model\n",
    "from DeBERTa.DeBERTa.deberta import config\n",
    "import pkgutil\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "config_json = hf_hub_download('microsoft/deberta-v3-base', 'config.json')\n",
    "# model = (config.ModelConfig.from_json_file(config_json))\n",
    "model = masked_language_model.MaskedLanguageModel.load_model(\"deberta-v3-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.position_biased_input = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "checkpoint_path = hf_hub_download('microsoft/deberta-v3-base', 'pytorch_model.bin')\n",
    "ckpt = torch.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias'])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(ckpt, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "checkpoint_path = hf_hub_download('microsoft/deberta-v3-xsmall', 'pytorch_model.bin')\n",
    "ckpt = torch.load(checkpoint_path)\n",
    "ckpt['cls.predictions.transform.dense.weight'] = ckpt.pop('lm_predictions.lm_head.dense.weight')\n",
    "ckpt['cls.predictions.transform.dense.bias'] = ckpt.pop('lm_predictions.lm_head.dense.bias')\n",
    "ckpt['cls.predictions.transform.LayerNorm.weight'] = ckpt.pop('lm_predictions.lm_head.LayerNorm.weight')\n",
    "ckpt['cls.predictions.transform.LayerNorm.bias'] = ckpt.pop('lm_predictions.lm_head.LayerNorm.bias')\n",
    "ckpt['cls.predictions.decoder.weight'] = ckpt['deberta.embeddings.word_embeddings.weight']\n",
    "ckpt['cls.predictions.decoder.bias'] = ckpt['lm_predictions.lm_head.bias']\n",
    "ckpt['cls.predictions.bias'] = ckpt.pop('lm_predictions.lm_head.bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\").config\n",
    "config_json = hf_hub_download(\"microsoft/deberta-v3-xsmall\", \"config.json\")\n",
    "from DeBERTa.DeBERTa.deberta import config\n",
    "c = config.ModelConfig.from_json_file(config_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
    "# tokenizer(\"Hi this\")\n",
    "c.position_biased_input = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'capital', 'of', 'France', 'is']"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from DeBERTa.DeBERTa import deberta\n",
    "# from DeBERTa.DeBERTa.apps.tasks import mlm_task\n",
    "vocab_path, vocab_type = deberta.load_vocab(pretrained_id='deberta-v3-base')\n",
    "tokenizer = deberta.tokenizers[vocab_type](vocab_path)\n",
    "# We apply the same schema of special tokens as BERT, e.g. [CLS], [SEP], [MASK]\n",
    "max_seq_len = 512\n",
    "tokens = tokenizer.tokenize('The capital of France is ')\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 279, 1909, 265, 2378, 269, 128000, 2]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = tokenizer.tokenize('A B C D E F')\n",
    "tokens.append(tokenizer.mask())\n",
    "token_ids = ['[CLS]'] + tokens + ['[SEP]']\n",
    "input_ids = tokenizer.convert_tokens_to_ids(token_ids)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(\n",
    "    torch.tensor([input_ids]),\n",
    "    position_ids = torch.arange(len(input_ids)).unsqueeze(0),\n",
    "    input_mask=torch.ones(len(input_ids)).unsqueeze(0),\n",
    "    labels=torch.tensor([[0, 0, 0, 0, 0, 0, 3045, 0]])\n",
    ")['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1336, 269, 262, 128000, 265, 1991, 2]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'akshshenkoarnaArabPINK bitmap fraying Kumasi NAIanthi'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.decode(torch.topk(logits, 10, dim=-1).indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_predictions.lm_head.dense.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'65618'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(['65618'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect\n",
    "import math\n",
    "\n",
    "class NGramMaskGenerator:\n",
    "  \"\"\"\n",
    "  Mask ngram tokens\n",
    "  https://github.com/zihangdai/xlnet/blob/0b642d14dd8aec7f1e1ecbf7d6942d5faa6be1f0/data_utils.py\n",
    "  \"\"\"\n",
    "  def __init__(self, tokenizer, mask_lm_prob=0.15, max_seq_len=512, max_preds_per_seq=None, max_gram = 1, keep_prob = 0.1, mask_prob=0.8, **kwargs):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.mask_lm_prob = mask_lm_prob\n",
    "    self.keep_prob = keep_prob\n",
    "    self.mask_prob = mask_prob\n",
    "    assert self.mask_prob+self.keep_prob<=1, f'The prob of using [MASK]({mask_prob}) and the prob of using original token({keep_prob}) should between [0,1]'\n",
    "    self.max_preds_per_seq = max_preds_per_seq\n",
    "    if max_preds_per_seq is None:\n",
    "      self.max_preds_per_seq = math.ceil(max_seq_len*mask_lm_prob /10)*10\n",
    "\n",
    "    self.max_gram = max(max_gram, 1)\n",
    "    self.mask_window = int(1/mask_lm_prob) # make ngrams per window sized context\n",
    "    self.vocab_words = list(tokenizer.vocab.keys())\n",
    "\n",
    "  def mask_tokens(self, tokens, rng, **kwargs):\n",
    "    special_tokens = ['[MASK]', '[CLS]', '[SEP]', '[PAD]', '[UNK]'] # + self.tokenizer.tokenize(' ')\n",
    "    indices = [i for i in range(len(tokens)) if tokens[i] not in special_tokens]\n",
    "    ngrams = np.arange(1, self.max_gram + 1, dtype=np.int64)\n",
    "    pvals = 1. / np.arange(1, self.max_gram + 1)\n",
    "    pvals /= pvals.sum(keepdims=True)\n",
    "\n",
    "    unigrams = []\n",
    "    for id in indices:\n",
    "      if self.max_gram>1 and len(unigrams)>=1 and self.tokenizer.part_of_whole_word(tokens[id]):\n",
    "        unigrams[-1].append(id)\n",
    "      else:\n",
    "        unigrams.append([id])\n",
    "    \n",
    "    num_to_predict = min(self.max_preds_per_seq, max(1, int(round(len(tokens) * self.mask_lm_prob))))\n",
    "    mask_len = 0\n",
    "    offset = 0\n",
    "    mask_grams = np.array([False]*len(unigrams))\n",
    "    while offset < len(unigrams):\n",
    "      n = self._choice(rng, ngrams, p=pvals)\n",
    "      ctx_size = min(n*self.mask_window, len(unigrams)-offset)\n",
    "      m = rng.randint(0, ctx_size-1)\n",
    "      s = offset + m\n",
    "      e = min(offset+m+n, len(unigrams))\n",
    "      offset = max(offset+ctx_size, e)\n",
    "      mask_grams[s:e] = True\n",
    "\n",
    "    target_labels = [None]*len(tokens)\n",
    "    w_cnt = 0\n",
    "    for m,word in zip(mask_grams, unigrams):\n",
    "      if m:\n",
    "        for idx in word:\n",
    "          label = self._mask_token(idx, tokens, rng, self.mask_prob, self.keep_prob)\n",
    "          target_labels[idx] = label\n",
    "          w_cnt += 1\n",
    "        if w_cnt >= num_to_predict:\n",
    "          break\n",
    "\n",
    "    target_labels = [self.tokenizer.vocab[x] if x else 0 for x in target_labels]\n",
    "    return tokens, target_labels\n",
    "\n",
    "  def _choice(self, rng, data, p):\n",
    "    cul = np.cumsum(p)\n",
    "    x = rng.random()*cul[-1]\n",
    "    id = bisect(cul, x)\n",
    "    return data[id]\n",
    "\n",
    "  def _mask_token(self, idx, tokens, rng, mask_prob, keep_prob):\n",
    "    label = tokens[idx]\n",
    "    mask = '[MASK]'\n",
    "    rand = rng.random()\n",
    "    if rand < mask_prob:\n",
    "      new_label = mask\n",
    "    elif rand < mask_prob+keep_prob:\n",
    "      new_label = label\n",
    "    else:\n",
    "      new_label = rng.choice(self.vocab_words)\n",
    "\n",
    "    tokens[idx] = new_label\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlogits\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "model_mlm.deberta.encoder.layer[0].intermediate.dense.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_gen = NGramMaskGenerator(tokenizer, max_gram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32826, 16, 50264, 646, 32804, 530, 742, 9, 1470]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "tokenizer.convert_tokens_to_ids(mask_gen.mask_tokens(tokens, rng=random)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris FergusonK] of France'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(mask_gen.mask_tokens(tokens, rng=random)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2Tokenizer' object has no attribute 'mask_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_token_id\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2Tokenizer' object has no attribute 'mask_token_id'"
     ]
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['40313', '[MASK]', '262', '685', '31180', '42', '60', '286', '4881']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_gen.mask_tokens(tokens, rng=random)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "from DeBERTa.DeBERTa.apps.models import masked_language_model as deberta_mlm\n",
    "from DeBERTa.DeBERTa.deberta import config\n",
    "\n",
    "\n",
    "def _prepare_deberta(hf_model_name):\n",
    "    config_json = hf_hub_download(hf_model_name, \"config.json\")\n",
    "    d_config = config.ModelConfig.from_json_file(config_json)\n",
    "    d_config.position_biased_input = True\n",
    "    model = deberta_mlm.MaskedLanguageModel(d_config)\n",
    "    checkpoint_path = hf_hub_download(hf_model_name, \"pytorch_model.bin\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "    return model\n",
    "\n",
    "deberta = _prepare_deberta(\"microsoft/deberta-v3-xsmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLanguageModel(\n",
       "  (deberta): DeBERTa(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(128100, 384, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_predictions): EnhancedMaskDecoder(\n",
       "    (lm_head): BertLMPredictionHead(\n",
       "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
    "inputs = hf_tokenizer(\"Hello world\", return_tensors='pt', padding=True)\n",
    "\n",
    "logits = deberta(\n",
    "                input_ids=inputs.input_ids.repeat(4, 1),\n",
    "                labels=torch.diag(inputs.input_ids.squeeze()),\n",
    "                input_mask=inputs.attention_mask,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "            )[\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.0100, -2.3126,  3.0641,  1.0293], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[torch.arange(4), inputs.input_ids.squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def _prepare_deberta(hf_model_name):\n",
    "    checkpoint_path = hf_hub_download(hf_model_name, \"pytorch_model.bin\")\n",
    "    ckpt = torch.load(checkpoint_path)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(hf_model_name)\n",
    "    ckpt[\"cls.predictions.transform.dense.weight\"] = ckpt.pop(\n",
    "        \"lm_predictions.lm_head.dense.weight\"\n",
    "    )\n",
    "    ckpt[\"cls.predictions.transform.dense.bias\"] = ckpt.pop(\n",
    "        \"lm_predictions.lm_head.dense.bias\"\n",
    "    )\n",
    "    ckpt[\"cls.predictions.transform.LayerNorm.weight\"] = ckpt.pop(\n",
    "        \"lm_predictions.lm_head.LayerNorm.weight\"\n",
    "    )\n",
    "    ckpt[\"cls.predictions.transform.LayerNorm.bias\"] = ckpt.pop(\n",
    "        \"lm_predictions.lm_head.LayerNorm.bias\"\n",
    "    )\n",
    "    ckpt[\"cls.predictions.decoder.weight\"] = ckpt[\n",
    "        \"deberta.embeddings.word_embeddings.weight\"\n",
    "    ]\n",
    "    ckpt[\"cls.predictions.decoder.bias\"] = ckpt[\"lm_predictions.lm_head.bias\"]\n",
    "    ckpt[\"cls.predictions.bias\"] = ckpt.pop(\"lm_predictions.lm_head.bias\")\n",
    "    model.load_state_dict(ckpt, strict=False)\n",
    "    return model\n",
    "\n",
    "deberta = _prepare_deberta(\"microsoft/deberta-v3-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForMaskedLM(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 384, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (cls): DebertaV2OnlyMLMHead(\n",
       "    (predictions): DebertaV2LMPredictionHead(\n",
       "      (transform): DebertaV2PredictionHeadTransform(\n",
       "        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=384, out_features=128100, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = \"Who have most people discovered while embarrassing Erin?\"\n",
    "bad = \"Who have most people discovered Erin while embarrassing?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  1876,   286,   370,   355,  2534,   438, 13784, 13736,   302,\n",
      "             2]])\n",
      "tensor([-20.0942, -21.9367, -21.6412, -18.3745, -18.6219, -15.2118, -16.8572,\n",
      "        -15.4021, -17.8441, -23.4265, -17.6024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-207.0126)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def _encoder_log_prob_sum(lm, tokenizer, input_text_batch):\n",
    "    assert len(input_text_batch) == 1\n",
    "    tokenized = tokenizer(input_text_batch, return_tensors=\"pt\")\n",
    "    input_ids = tokenized.input_ids\n",
    "    print(input_ids)\n",
    "    attention_mask = tokenized.attention_mask\n",
    "    seq_length = input_ids.shape[1]\n",
    "\n",
    "    # Mask out each token in the input sequence\n",
    "    masked_input_ids = input_ids.repeat(seq_length, 1)\n",
    "    masked_attention_mask = attention_mask.repeat(seq_length, 1)\n",
    "    masked_input_ids.fill_diagonal_(tokenizer.mask_token_id)\n",
    "\n",
    "    labels = torch.diag(input_ids.squeeze())\n",
    "\n",
    "    # Get the model's predictions for the batched masked inputs\n",
    "    with torch.inference_mode():\n",
    "        if isinstance(lm, deberta_mlm.MaskedLanguageModel):\n",
    "            logits = lm(\n",
    "                input_ids=masked_input_ids,\n",
    "                labels=labels,\n",
    "                input_mask=masked_attention_mask,\n",
    "                attention_mask=masked_attention_mask,\n",
    "            )[\"logits\"]\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            token_log_probs = log_probs[torch.arange(seq_length), input_ids.squeeze()]\n",
    "        else:\n",
    "            logits = lm(masked_input_ids, attention_mask=masked_attention_mask).logits\n",
    "\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            token_log_probs = log_probs[\n",
    "                torch.arange(seq_length), torch.arange(seq_length), input_ids.squeeze()\n",
    "            ]\n",
    "            print(token_log_probs)\n",
    "    return token_log_probs.sum()\n",
    "\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
    "_encoder_log_prob_sum(deberta, tokenizer, [good])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  1876,   286,   370,   355,  2534, 13736,   438, 13784,   302,\n",
      "             2]])\n",
      "tensor([-18.9079, -20.7022, -23.6953, -18.6025, -19.8663, -13.3951, -17.7276,\n",
      "        -18.0960, -12.1095, -23.7737, -18.3432])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-205.2192)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_encoder_log_prob_sum(deberta, AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-xsmall\"), [bad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 11081, 2], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"shocking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForMaskedLM(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 384, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (cls): DebertaV2OnlyMLMHead(\n",
       "    (predictions): DebertaV2LMPredictionHead(\n",
       "      (transform): DebertaV2PredictionHeadTransform(\n",
       "        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=384, out_features=128100, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deberta.dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/Roberta-base\")\n",
    "deberta_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
    "tokens = deberta_tokenizer(\"One Two Three [MASK]\", return_tensors=\"pt\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/Roberta-base\")\n",
    "mask_token_index = torch.where(tokens[\"input_ids\"] == deberta_tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[     1,    593,   1852,   3216, 128000,      2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])},\n",
       " tensor([4]))"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens, mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = deberta(**tokens, position_ids=torch.arange(6).unsqueeze(0)).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BasqueEIS riveting Bayer Maronite kawaii Akhtar Zahid']"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deberta_tokenizer.batch_decode(torch.topk(logits[0][mask_token_index], 10).indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 32826,    16,     5,   646, 32804,   530,   742,     9,  1470,\n",
       "             2]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0972, -0.0294,  0.4988,  ..., -0.0312, -0.0312, -1.0000],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.decoder.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deberta.cls.predictions.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = hf_hub_download(\"microsoft/deberta-v3-xsmall\", \"pytorch_model.bin\")\n",
    "ckpt = torch.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.0730e-02,  3.3051e-02, -4.5532e-02, -7.3730e-02, -1.3245e-01,\n",
       "        -1.0748e-01, -1.1218e-01, -2.1167e-01, -1.2408e-01, -2.1509e-01,\n",
       "        -1.1212e-01, -1.4417e-01, -4.0375e-02, -4.4983e-02, -3.1677e-02,\n",
       "        -3.6774e-02, -7.4829e-02, -2.3041e-02, -2.7393e-01, -1.7432e-01,\n",
       "        -8.3679e-02, -9.1187e-02, -2.8152e-02, -2.7295e-01, -2.2278e-01,\n",
       "        -1.0480e-01, -1.5588e-01, -8.8745e-02, -1.9409e-01, -1.3452e-01,\n",
       "        -1.1774e-01, -3.4912e-02, -9.8511e-02, -1.9104e-01, -9.3933e-02,\n",
       "         6.3538e-02, -4.8920e-02, -6.9427e-03, -1.1115e-01, -1.1072e-01,\n",
       "        -9.4849e-02, -1.1981e-01, -6.7810e-02, -8.1238e-02, -1.3269e-01,\n",
       "        -5.5328e-02, -3.5248e-02, -3.6774e-02, -1.0187e-01, -2.0850e-01,\n",
       "        -1.2311e-01, -1.3208e-01, -7.9895e-02, -2.7417e-01, -1.2520e+00,\n",
       "        -1.6370e-01, -4.3671e-02, -1.2903e-01, -7.5195e-02,  7.1533e-02,\n",
       "        -1.0803e-01, -1.0645e-01, -1.1572e-01, -3.8391e-02, -1.4062e-01,\n",
       "        -1.0583e-01, -1.6858e-01, -9.3506e-02, -1.2585e-01, -1.6077e-01,\n",
       "        -2.6807e-01, -9.3140e-02, -7.7759e-02, -7.7576e-02, -1.8579e-01,\n",
       "        -9.4788e-02, -1.5149e-01, -1.9543e-01, -1.2927e-01, -6.8542e-02,\n",
       "        -1.3416e-01, -2.2131e-01, -8.9233e-02, -1.2756e-01, -2.7008e-03,\n",
       "        -1.4111e-01, -1.3000e-01, -1.6406e-01, -2.1553e-03, -4.0741e-02,\n",
       "        -5.4016e-02, -1.7773e-01, -3.3911e-01, -1.3672e-01, -1.5161e-01,\n",
       "        -7.2205e-02, -1.3708e-01,  7.8659e-03, -5.6946e-02, -1.1737e-01,\n",
       "        -1.3623e-01, -2.9395e-01, -1.3818e-01, -1.4490e-01, -1.3147e-01,\n",
       "        -8.3801e-02, -1.5930e-01, -1.4673e-01, -1.1890e-01, -5.8319e-02,\n",
       "        -1.3306e-01, -8.6731e-02, -1.5039e-01, -1.2659e-01, -1.8152e-01,\n",
       "        -5.6030e-02, -2.0398e-01, -1.6309e-01, -1.5002e-01, -1.4099e-01,\n",
       "        -2.2766e-02, -2.6172e-01, -5.3406e-02, -1.3428e-01, -5.0781e-02,\n",
       "        -5.1270e-02, -1.2402e-01, -2.1240e-01, -2.6245e-02, -6.4514e-02,\n",
       "        -4.9042e-02, -1.1255e-01,  3.2539e+00, -7.2144e-02, -1.0663e-01,\n",
       "        -1.5833e-01, -1.2115e-01, -1.2683e-01, -1.1072e-01, -1.3940e-01,\n",
       "        -1.4880e-01, -1.2408e-01, -1.5125e-01, -9.3933e-02, -2.3914e-01,\n",
       "        -1.1945e-01, -1.4221e-02,  5.1079e-03, -1.9910e-01, -2.8366e-02,\n",
       "        -1.2451e-01, -1.4075e-01, -1.8982e-01, -7.1777e-02, -1.0724e-01,\n",
       "        -1.3794e-01, -2.1423e-01, -6.6162e-02,  4.4739e-02, -2.5562e-01,\n",
       "        -1.1725e-01, -1.6711e-01, -1.2134e-01, -1.0730e-01, -3.7933e-02,\n",
       "        -9.4482e-02, -1.0919e-01, -1.1456e-01, -1.2347e-01, -5.1086e-02,\n",
       "        -8.3389e-03,  1.2463e-01, -1.6357e-01, -1.7297e-01, -7.6904e-02,\n",
       "        -1.4355e-01, -1.2091e-01, -1.5967e-01, -2.0239e-01, -9.3323e-02,\n",
       "        -8.5876e-02, -4.4800e-02, -1.8713e-01, -5.7617e-02, -1.4954e-01,\n",
       "        -1.1353e-01, -6.8909e-02, -1.1249e-01, -9.2529e-02, -1.3184e-01,\n",
       "        -2.6123e-01, -1.7822e-01, -1.0785e-01, -8.8806e-02, -8.3496e-02,\n",
       "        -1.8994e-01, -4.9500e-02, -1.2634e-01, -1.3269e-01, -3.8574e-02,\n",
       "        -6.0852e-02, -1.3733e-01, -5.0934e-02, -1.1566e-01, -1.5356e-01,\n",
       "        -1.9690e-01, -1.3086e-01, -2.7417e-01, -8.3008e-02, -5.2826e-02,\n",
       "        -1.5979e-01, -1.0382e-01, -1.3171e-01, -6.0974e-02, -9.0576e-02,\n",
       "         5.6000e-02, -7.6111e-02, -6.5063e-02, -2.5464e-01, -1.3965e-01,\n",
       "        -1.9275e-01, -1.5027e-01, -1.1792e-01, -6.8970e-02, -3.4912e-02,\n",
       "        -3.6255e-02, -9.5703e-02, -1.6516e-01, -1.4282e-01, -1.1853e-01,\n",
       "        -1.4124e-01, -1.0669e-01, -9.4482e-02, -2.7222e-01, -1.7920e-01,\n",
       "        -1.4062e-01, -1.1279e-01, -3.3905e-02, -3.0200e-01, -5.8411e-02,\n",
       "        -1.5442e-01, -1.1084e-01, -6.7383e-02, -1.3904e-01, -1.5527e-01,\n",
       "         1.3086e-01, -1.1725e-01, -7.8613e-02, -1.7944e-01, -1.0675e-01,\n",
       "        -2.0300e-01, -5.5145e-02, -1.0181e-01, -8.3069e-02, -2.9468e-01,\n",
       "        -6.5674e-02, -6.5430e-02, -1.2280e-01,  2.6749e-02, -1.0706e-01,\n",
       "        -2.8735e-01, -6.9885e-02, -1.2512e-01, -1.4368e-01, -1.6797e-01,\n",
       "        -5.3406e-02, -1.6846e-01, -2.1472e-01, -5.8594e-02, -1.3130e-02,\n",
       "        -1.0730e-01, -1.2695e-01, -1.2219e-01,  6.1035e-03, -5.5176e-02,\n",
       "        -1.5515e-01, -1.8115e-01, -1.5405e-01, -4.7836e-03, -1.3599e-01,\n",
       "        -1.7859e-01, -9.6313e-02, -8.6670e-02, -1.2537e-01, -1.5430e-01,\n",
       "        -8.4912e-01, -4.2065e-01, -1.4368e-01, -1.3330e-01, -3.0396e-01,\n",
       "        -2.7710e-02, -1.2732e-01, -1.7273e-01, -1.8958e-01, -6.1462e-02,\n",
       "        -3.7109e-02,  1.8539e-02, -1.3855e-01, -8.3740e-02, -1.7249e-01,\n",
       "        -2.5732e-01,  7.3547e-02, -7.3669e-02, -2.1082e-01, -1.4185e-01,\n",
       "        -7.6904e-02, -7.0984e-02, -4.9866e-02, -2.5497e-02, -1.1066e-01,\n",
       "        -9.7168e-02, -1.4450e-02, -1.5976e-02, -2.5253e-02, -1.2408e-01,\n",
       "        -1.2610e-01, -2.0312e-01, -7.7637e-02, -1.2378e-01, -2.0349e-01,\n",
       "        -9.6924e-02, -1.8286e-01, -1.9318e-02, -1.7603e-01, -1.0706e-01,\n",
       "        -1.5327e-02, -1.2512e-01, -1.2140e-01, -5.8899e-02, -1.3245e-01,\n",
       "        -1.0162e-01, -1.0736e-01, -1.0669e-01, -4.5074e-02, -1.0443e-01,\n",
       "        -2.3422e-02, -7.1777e-02, -1.4783e-01, -2.5781e-01, -7.2632e-02,\n",
       "        -1.3013e-01, -1.4502e-01, -1.6089e-01, -1.1578e-01, -9.3201e-02,\n",
       "        -9.3628e-02, -1.0938e-01, -1.8787e-01, -1.8921e-01, -5.7312e-02,\n",
       "        -1.9763e-01, -1.1975e-01, -9.0332e-02, -1.4490e-01, -1.3379e-01,\n",
       "        -3.9886e-02, -9.2651e-02, -1.3208e-01, -1.2817e-01, -9.1858e-03,\n",
       "        -1.3196e-01, -1.0492e-01, -7.2449e-02, -9.5276e-02, -1.0742e-01,\n",
       "        -1.0413e-01, -6.9153e-02, -4.3854e-02, -1.6113e-01, -1.2964e-01,\n",
       "        -1.0431e-01, -1.0779e-01, -1.0852e-01,  8.2214e-02, -1.6211e-01,\n",
       "        -6.3660e-02, -1.6650e-01, -3.2715e-02, -1.6821e-01, -1.7505e-01,\n",
       "        -2.6340e-03, -1.0327e-01,  1.6144e-02, -1.3464e-01],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['deberta.encoder.layer.0.attention.output.LayerNorm.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Paris'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(2201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[50264,   133,   812,     9,  1470,    16,  2201,     2],\n",
       "         [    0, 50264,   812,     9,  1470,    16,  2201,     2],\n",
       "         [    0,   133, 50264,     9,  1470,    16,  2201,     2],\n",
       "         [    0,   133,   812, 50264,  1470,    16,  2201,     2],\n",
       "         [    0,   133,   812,     9, 50264,    16,  2201,     2],\n",
       "         [    0,   133,   812,     9,  1470, 50264,  2201,     2],\n",
       "         [    0,   133,   812,     9,  1470,    16, 50264,     2],\n",
       "         [    0,   133,   812,     9,  1470,    16,  2201, 50264]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/Roberta-base\", use_fast=False)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/Roberta-base\")\n",
    "\n",
    "text = \"The capital of France is Paris\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "seq_length = tokens.input_ids.shape[1]\n",
    "input_ids = tokens.input_ids.repeat(seq_length, 1)\n",
    "input_ids.fill_diagonal_(tokenizer.mask_token_id)\n",
    "attention_mask = tokens.attention_mask.repeat(seq_length, 1)\n",
    "# input_ids = torch.tril(input_ids)\n",
    "# attention_mask = torch.tril(attention_mask)\n",
    "input_ids, attention_mask\n",
    "# mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[50264,     2,   812,     9,  1470,    16,  2201,     2],\n",
       "         [    0, 50264,     2,     9,  1470,    16,  2201,     2],\n",
       "         [    0,   133, 50264,     2,  1470,    16,  2201,     2],\n",
       "         [    0,   133,   812, 50264,     2,    16,  2201,     2],\n",
       "         [    0,   133,   812,     9, 50264,     2,  2201,     2],\n",
       "         [    0,   133,   812,     9,  1470, 50264,     2,     2],\n",
       "         [    0,   133,   812,     9,  1470,    16, 50264,     2],\n",
       "         [    0,   133,   812,     9,  1470,    16,  2201, 50264]]),\n",
       " tensor([[1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[torch.arange(seq_length - 1), torch.arange(1, seq_length)] = tokenizer.sep_token_id\n",
    "\n",
    "input_ids, torch.tril(attention_mask, diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 3), (3, 4), (4, 4), (4, 4), (4, 5), (5, 6), (6, 7)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions = [\"The\", \"capital\", \"of\", \"\", \"\", \"France\", \"is\", \"Paris\"]\n",
    "all_target_idxs = []\n",
    "st = 0\n",
    "sent_tokens = tokens.input_ids[0].tolist()\n",
    "import utils\n",
    "for idx, region in enumerate(regions):\n",
    "    region = region.lstrip().rstrip()\n",
    "    if not region:\n",
    "        all_target_idxs.append((st, st))\n",
    "        continue\n",
    "    word_tokenized = utils.get_tokenized_word(tokenizer, region, idx)\n",
    "    st_curr, en_curr = utils.get_idxs(word_tokenized, sent_tokens, st)\n",
    "    all_target_idxs.append((st_curr, en_curr))\n",
    "    st = en_curr\n",
    "\n",
    "all_target_idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([r.lstrip().rstrip() for r in regions if len(r) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_mask_output = model(input_ids, attention_mask=attention_mask, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril_mask_output = model(input_ids, attention_mask=attention_mask, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3316, 0.0815, 0.0556, 0.0245, 0.0512, 0.1052, 0.0150, 0.3354],\n",
       "        [0.4165, 0.0360, 0.0394, 0.0173, 0.0095, 0.1151, 0.0171, 0.3492],\n",
       "        [0.3465, 0.0559, 0.0327, 0.0227, 0.0062, 0.2276, 0.0163, 0.2921],\n",
       "        [0.4869, 0.0079, 0.0051, 0.0112, 0.0051, 0.0504, 0.0083, 0.4249],\n",
       "        [0.3987, 0.0399, 0.0197, 0.0451, 0.0185, 0.0971, 0.0089, 0.3721],\n",
       "        [0.4806, 0.0462, 0.0121, 0.0103, 0.0092, 0.0524, 0.0133, 0.3759],\n",
       "        [0.3960, 0.0876, 0.0155, 0.0098, 0.0321, 0.0871, 0.0112, 0.3607],\n",
       "        [0.2780, 0.1147, 0.0720, 0.0335, 0.0709, 0.1239, 0.0218, 0.2852]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_mask_output.attentions[-1][-2][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4801, 0.1124, 0.0791, 0.0352, 0.0611, 0.1859, 0.0462, 0.0000],\n",
       "        [0.6885, 0.0390, 0.0260, 0.0189, 0.0059, 0.2114, 0.0103, 0.0000],\n",
       "        [0.5157, 0.0313, 0.0119, 0.0133, 0.0022, 0.4199, 0.0056, 0.0000],\n",
       "        [0.8695, 0.0091, 0.0034, 0.0138, 0.0025, 0.0994, 0.0023, 0.0000],\n",
       "        [0.6081, 0.0445, 0.0150, 0.0379, 0.0099, 0.2755, 0.0091, 0.0000],\n",
       "        [0.7060, 0.0741, 0.0100, 0.0143, 0.0058, 0.1510, 0.0388, 0.0000],\n",
       "        [0.6602, 0.0672, 0.0165, 0.0223, 0.0233, 0.1199, 0.0906, 0.0000],\n",
       "        [0.4281, 0.1287, 0.0887, 0.0388, 0.0697, 0.1926, 0.0534, 0.0000]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril_mask_output.attentions[-1][-2][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21.9630, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_soft = torch.nn.functional.log_softmax(masked_lm_output.logits, dim=-1)\n",
    "regular = log_soft[6, 6, 2201]\n",
    "regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', ' \"', ',', ' (', ' [', '.', '...', ' and', ' ', ' -']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits = masked_lm_output.logits[torch.arange(seq_length), torch.arange(seq_length)]\n",
    "# tokenizer.batch_decode(torch.topk(logits[6], 10).indices)\n",
    "indices = torch.topk(torch.nn.functional.log_softmax(masked_lm_output.logits, dim=-1)[6, 6], 10, dim=-1).indices\n",
    "tokenizer.batch_decode(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[35.4360, -4.0060, 23.2542,  ...,  2.8371,  4.8490, 12.7460],\n",
       "        [ 7.6120, -2.7287, 19.5833,  ...,  3.4265,  3.2083,  8.2631],\n",
       "        [-2.5935, -3.2850, 11.5474,  ...,  1.9503,  0.4086,  1.3528],\n",
       "        ...,\n",
       "        [ 1.8982, -3.4177, 18.1317,  ...,  2.3102, -1.3643,  4.6152],\n",
       "        [ 2.5531, -3.7130, 12.2859,  ..., -1.5587, -1.1461,  1.1735],\n",
       "        [13.7855, -5.3178, 24.5589,  ..., -1.8891, -0.3919,  6.6007]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save = masked_lm_output.logits[-1]\n",
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paris france bordeaux']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, ElectraForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-generator\")\n",
    "model = ElectraForMaskedLM.from_pretrained(\"google/electra-small-generator\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].topk(3, axis=-1)\n",
    "tokenizer.batch_decode(predicted_token_id.indices)\n",
    "\n",
    "# labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# # mask labels of non-[MASK] tokens\n",
    "# labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "# outputs = model(**inputs, labels=labels)\n",
    "# round(outputs.loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd18471ec1c42cd820f8d906b09035e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa002e7203f448559a8f1238b8eff1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3899bc5ac9a74a71996236eca458537a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7a622154d54ffd83664df78007e20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06c834d580c44938463901c9d8e9fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, ElectraForCausalLM, ElectraConfig\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/electra-base-generator\")\n",
    "config = ElectraConfig.from_pretrained(\"google/electra-base-generator\")\n",
    "config.is_decoder = True\n",
    "model = ElectraForCausalLM.from_pretrained(\"google/electra-base-generator\", config=config)\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "prediction_logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7632, 2045, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electra_tokenizer = AutoTokenizer.from_pretrained(\"google/electra-base-generator\")\n",
    "electra_tokenizer(\"Hi there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2028, 2048, 2093, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electra_tokenizer(\"one two Three\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2093, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electra_tokenizer(\"Three\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraTokenizerFast(name_or_path='google/electra-base-generator', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electra_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32099,\n",
       " 32098,\n",
       " 32097,\n",
       " 32096,\n",
       " 32095,\n",
       " 32094,\n",
       " 32093,\n",
       " 32092,\n",
       " 32091,\n",
       " 32090,\n",
       " 32089,\n",
       " 32088,\n",
       " 32087,\n",
       " 32086,\n",
       " 32085,\n",
       " 32084,\n",
       " 32083,\n",
       " 32082,\n",
       " 32081,\n",
       " 32080,\n",
       " 32079,\n",
       " 32078,\n",
       " 32077,\n",
       " 32076,\n",
       " 32075,\n",
       " 32074,\n",
       " 32073,\n",
       " 32072,\n",
       " 32071,\n",
       " 32070,\n",
       " 32069,\n",
       " 32068,\n",
       " 32067,\n",
       " 32066,\n",
       " 32065,\n",
       " 32064,\n",
       " 32063,\n",
       " 32062,\n",
       " 32061,\n",
       " 32060,\n",
       " 32059,\n",
       " 32058,\n",
       " 32057,\n",
       " 32056,\n",
       " 32055,\n",
       " 32054,\n",
       " 32053,\n",
       " 32052,\n",
       " 32051,\n",
       " 32050,\n",
       " 32049,\n",
       " 32048,\n",
       " 32047,\n",
       " 32046,\n",
       " 32045,\n",
       " 32044,\n",
       " 32043,\n",
       " 32042,\n",
       " 32041,\n",
       " 32040,\n",
       " 32039,\n",
       " 32038,\n",
       " 32037,\n",
       " 32036,\n",
       " 32035,\n",
       " 32034,\n",
       " 32033,\n",
       " 32032,\n",
       " 32031,\n",
       " 32030,\n",
       " 32029,\n",
       " 32028,\n",
       " 32027,\n",
       " 32026,\n",
       " 32025,\n",
       " 32024,\n",
       " 32023,\n",
       " 32022,\n",
       " 32021,\n",
       " 32020,\n",
       " 32019,\n",
       " 32018,\n",
       " 32017,\n",
       " 32016,\n",
       " 32015,\n",
       " 32014,\n",
       " 32013,\n",
       " 32012,\n",
       " 32011,\n",
       " 32010,\n",
       " 32009,\n",
       " 32008,\n",
       " 32007,\n",
       " 32006,\n",
       " 32005,\n",
       " 32004,\n",
       " 32003,\n",
       " 32002,\n",
       " 32001,\n",
       " 32000]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.additional_special_tokens_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<pad> <extra_id_0> park offers <extra_id_1> the <extra_id_2> park.</s>']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "sequence_ids = model.generate(input_ids)\n",
    "sequences = tokenizer.batch_decode(sequence_ids)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 32099,  2447,   704, 32098,     8, 32097,  2447,     5,     1]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"there\").input_ids.pop(0) == tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1408, -1.4999,  0.9446,  ..., -1.5783, -0.6470, -0.1584],\n",
       "         [-3.5331,  1.0571, -3.3506,  ...,  1.3799, -2.4383,  0.3520],\n",
       "         [ 0.9348,  3.1438, -1.1558,  ..., -1.0678, -0.6302,  2.6668],\n",
       "         [-2.5261,  0.9624,  3.4867,  ...,  1.6737, -1.1018, -2.6121],\n",
       "         [-1.5808, -0.5618, -0.8802,  ...,  3.5167, -1.5294,  2.5036]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.hidden_states[model.config.num_hidden_layers // 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.nn.Transformer.generate_square_subsequent_mask(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.attention_mask.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "src = torch.rand(32, 10, 512)\n",
    "out = transformer_encoder(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 512,\n",
       "  \"dense_act_fn\": \"relu\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": false,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"num_decoder_layers\": 0,\n",
       "  \"num_heads\": 8,\n",
       "  \"num_layers\": 0,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"transformers_version\": \"4.41.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Config\n",
    "\n",
    "config = T5Config(num_layers=0)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(16).reshape(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"ab\": 1,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 256,\n",
       "  \"dense_act_fn\": \"relu\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": false,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"num_decoder_layers\": 4,\n",
       "  \"num_heads\": 8,\n",
       "  \"num_layers\": 0,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"transformers_version\": \"4.41.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Config, T5ForConditionalGeneration\n",
    "\n",
    "t5 = T5ForConditionalGeneration(T5Config(\n",
    "    num_layers=0,\n",
    "    num_decoder_layers=4,\n",
    "    hidden_size=256,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    ab=1\n",
    "))\n",
    "t5.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "self.model.config.decoder_start_token_id has to be defined. In T5 it is usually set to the pad_token_id. See T5 docs for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe capital of France is\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhatever\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m tokens\u001b[38;5;241m.\u001b[39minput_ids[tokens\u001b[38;5;241m.\u001b[39minput_ids \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mt5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_decoder_input_ids_from_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1833\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.prepare_decoder_input_ids_from_labels\u001b[0;34m(self, labels)\u001b[0m\n\u001b[1;32m   1832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_decoder_input_ids_from_labels\u001b[39m(\u001b[38;5;28mself\u001b[39m, labels: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m-> 1833\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shift_right\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:877\u001b[0m, in \u001b[0;36mT5PreTrainedModel._shift_right\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    874\u001b[0m pad_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder_start_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    878\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself.model.config.decoder_start_token_id has to be defined. In T5 it is usually set to the pad_token_id. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee T5 docs for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     )\n\u001b[1;32m    882\u001b[0m \u001b[38;5;66;03m# shift inputs to the right\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_fx_proxy(input_ids):\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# Item assignment is not supported natively for proxies.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: self.model.config.decoder_start_token_id has to be defined. In T5 it is usually set to the pad_token_id. See T5 docs for more information."
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "# t5(\n",
    "\n",
    "# )\n",
    "tokens = tokenizer([\"The capital of France is\", \"whatever\"], return_tensors=\"pt\", padding=True)\n",
    "tokens.input_ids[tokens.input_ids == tokenizer.pad_token_id] = -100\n",
    "t5.prepare_decoder_input_ids_from_labels(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 256,\n",
       "  \"dense_act_fn\": \"relu\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": false,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"num_decoder_layers\": 4,\n",
       "  \"num_heads\": 8,\n",
       "  \"num_layers\": 0,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"transformers_version\": \"4.41.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 17:00:57 INFO: Reading trees from /u/scr/corpora/ldc/2015/LDC2015T13_eng_news_txt_tbnk-ptb_revised/data/penntree/23/wsj_2300.tree\n"
     ]
    }
   ],
   "source": [
    "from stanza.models.constituency import parse_tree, tree_reader\n",
    "\n",
    "test_trees = tree_reader.read_treebank(\n",
    "        \"/nlp/scr/horatio/data/constituency/en_ptb3_train.mrg\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor([[1]]).expand(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\"train.tree\", \"w\") as f:\n",
    "    for i in range(2, 22):\n",
    "        if i < 10:\n",
    "            i = f\"0{i}\"\n",
    "        for file_name in sorted(os.listdir(f\"/u/scr/corpora/ldc/2015/LDC2015T13_eng_news_txt_tbnk-ptb_revised/data/penntree/{i}/\")):\n",
    "            with open(f\"/u/scr/corpora/ldc/2015/LDC2015T13_eng_news_txt_tbnk-ptb_revised/data/penntree/{i}/{file_name}\") as f2:\n",
    "                f.write(f2.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.ones_like(torch.tensor([[3, 4]] * 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(3, 5))[:, None, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, EncoderDecoderModel, EncoderDecoderConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 4,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.41.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BertConfig(\n",
    "    num_hidden_layers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = EncoderDecoderModel(\n",
    "        config=EncoderDecoderConfig.from_encoder_decoder_configs(\n",
    "            BertConfig(num_hidden_layers=0), BertConfig()\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52777f0bd9c24ac1b09d7c7314e54750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a7014ae4844dda9c2150d9b0f1fb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea6fba630a440d899783b51018b1395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0625edbb4c154f35b5afedf76e15e18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stanford'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstanford\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrees\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtreebank\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stanford'"
     ]
    }
   ],
   "source": [
    "import stanford.nlp.trees.treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza_cache\n",
    "import stanza\n",
    "from stanza.models.constituency import tree_reader\n",
    "import dataclasses\n",
    "from typing import Sequence, Mapping\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "_TRAIN_PICKLE_PATH = \"stanza/ptb_train.pickle\"\n",
    "_DEV_PICKLE_PATH = \"stanza/ptb_dev.pickle\"\n",
    "_TEST_PICKLE_PATH = \"stanza/ptb_test.pickle\"\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class DepParseDataPickle:\n",
    "    input_strs: Sequence[str]\n",
    "    dev_data: Sequence[Mapping[(str, str)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = stanza_cache.cached_ptb_train_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " with open(\"stanza/ptb_train.pickle\", \"rb\") as reader:\n",
    "    pickle_obj = pickle.load(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In an Oct. 19 review of `` The Misanthrope '' at Chicago 's Goodman Theatre ( `` Revitalized Classics Take the Stage in Windy City , '' Leisure & Arts ) , the role of Celimene , played by Kim Cattrall , was mistakenly attributed to Christina Haag .\",\n",
       " 'Ms. Haag plays Elianti .',\n",
       " 'Rolls - Royce Motor Cars Inc. said it expects its U.S. sales to remain steady at about 1,200 cars in 1990 .',\n",
       " 'The luxury auto maker last year sold 1,214 cars in the U.S.',\n",
       " 'Howard Mosher , president and chief executive officer , said he anticipates growth for the luxury auto maker in Britain and Europe , and in Far Eastern markets .',\n",
       " 'BELL INDUSTRIES Inc. increased its quarterly to 10 cents from seven cents a share .',\n",
       " 'The new rate will be payable Feb. 15 .',\n",
       " \"A record date has n't been set .\",\n",
       " 'Bell , based in Los Angeles , makes and distributes electronic , computer and building products .',\n",
       " 'Investors are appealing to the Securities and Exchange Commission not to limit their access to information about stock purchases and sales by corporate insiders .',\n",
       " 'A SEC proposal to ease reporting requirements for some company executives would undermine the usefulness of information on insider trades as a stock - picking tool , individual investors and professional money managers contend .',\n",
       " \"They make the argument in letters to the agency about rule changes proposed this past summer that , among other things , would exempt many middle - management executives from reporting trades in their own companies ' shares .\",\n",
       " 'The proposed changes also would allow executives to report exercises of options later and less often .',\n",
       " 'Many of the letters maintain that investor confidence has been so shaken by the 1987 stock market crash -- and the markets already so stacked against the little guy -- that any decrease in information on insider - trading patterns might prompt individuals to get out of stocks altogether .',\n",
       " \"`` The SEC has historically paid obeisance to the ideal of a level playing field , '' wrote Clyde S. McGregor of Winnetka , Ill. , in one of the 92 letters the agency has received since the changes were proposed Aug. 17 .\",\n",
       " \"`` Apparently the commission did not really believe in this ideal . ''\",\n",
       " \"Currently , the rules force executives , directors and other corporate insiders to report purchases and sales of their companies ' shares within about a month after the transaction .\",\n",
       " 'But about 25 % of the insiders , according to SEC figures , file their reports late .',\n",
       " \"The changes were proposed in an effort to streamline federal bureaucracy and boost compliance by the executives `` who are really calling the shots , '' said Brian Lane , special counsel at the SEC 's office of disclosure policy , which proposed the changes .\",\n",
       " 'Investors , money managers and corporate officials had until today to comment on the proposals , and the issue has produced more mail than almost any other issue in memory , Mr. Lane said .',\n",
       " 'The SEC will probably vote on the proposal early next year , he said .',\n",
       " 'Not all those who wrote oppose the changes .',\n",
       " \"The Committee on Federal Regulation of Securities for the American Bar Association argues , for example , in its lengthy letter to the SEC , that the proposed changes `` would substantially improve the { law } by conforming it more closely to contemporary business realities . ''\",\n",
       " \"What the investors who oppose the proposed changes object to most is the effect they say the proposal would have on their ability to spot telltale `` clusters '' of trading activity -- buying or selling by more than one officer or director within a short period of time .\",\n",
       " 'According to some estimates , the rule changes would cut insider filings by more than a third .',\n",
       " \"The SEC 's Mr. Lane vehemently disputed those estimates .\",\n",
       " 'The rules will eliminate filings policy - making divisions , such as sales , marketing , finance and research and development , Mr. Lane said .',\n",
       " 'The proposed rules also would be tougher on the insiders still required to file reports , he said .',\n",
       " 'Companies would be compelled to publish in annual proxy statements the names of insiders who fail to file reports on time .',\n",
       " \"Considered as a whole , Mr. Lane said , the filings required under the proposed rules `` will be at least as effective , if not more so , for investors following transactions . ''\",\n",
       " 'But Robert Gabele , president of Invest / Net , a North Miami , Fla. , company that packages and sells the insider - trading data , said the proposal is worded so vaguely that key officials may fail to file the reports .',\n",
       " 'Many investors wrote asking the SEC to require insiders to report their purchases and sales immediately , not a month later .',\n",
       " 'But Mr. Lane said that while the SEC regulates who files , the law tells them when to do so .',\n",
       " 'Investors who want to change the required timing should write their representatives in Congress , he added .',\n",
       " 'The SEC would likely be amenable to legislation that required insiders to file transactions on a more timely basis , he said .',\n",
       " \"The nation 's largest pension fund , which oversees $ 80 billion for college employees , plans to offer two new investment options to its 1.2 million participants .\",\n",
       " \"The Teachers Insurance and Annuity Association - College Retirement Equities Fund said it will introduce a stock and bond fund that will invest in `` socially responsible '' companies , and a bond fund .\",\n",
       " 'Both funds are expected to begin operation around March 1 , subject to Securities and Exchange Commission approval .',\n",
       " 'For its employees to sign up for the options , a college also must approve the plan .',\n",
       " 'Some 4,300 institutions are part of the pension fund .',\n",
       " 'The new options carry out part of an agreement that the pension fund , under pressure to relax its strict participation rules and to provide more investment options , reached with the SEC in December .',\n",
       " \"The new `` social choice '' fund will shun securities of companies linked to South Africa , nuclear power and in some cases , Northern Ireland .\",\n",
       " \"Also excluded will be investments in companies with `` significant '' business stemming from weapons manufacture , alcoholic beverages or tobacco .\",\n",
       " 'Sixty percent of the fund will be invested in stocks , with the rest going into bonds or short - term investments .',\n",
       " 'The bond fund will invest in high - grade or medium - grade bonds , mortgages or asset - backed securities , including as much as 15 % in foreign securities .',\n",
       " 'The fund also might buy and sell futures and options contracts , subject to approval by the New York State Insurance Department .',\n",
       " 'Under two new features , participants will be able to transfer money from the new funds to other investment funds or , if their jobs are terminated , receive cash from the funds .',\n",
       " 'The investment choices offered by the pension fund currently are limited to a stock fund , an annuity and a money - market fund .',\n",
       " 'New Brunswick Scientific Co. , a maker of biotechnology instrumentation and equipment , said it adopted an anti-takeover plan giving shareholders the right to purchase shares at half price under certain conditions .',\n",
       " 'The company said the plan , under review for some time , will protect shareholders against `` abusive takeover tactics .',\n",
       " 'W. Ed Tyler , 37 years old , a senior vice president at this printing concern , was elected president of its technology group , a new position .',\n",
       " 'Solo woodwind players have to be creative if they want to work a lot , because their repertoire and audience appeal are limited .',\n",
       " \"The oboist Heinz Holliger has taken a hard line about the problem : He commissions and splendidly interprets fearsome contemporary scores and does some conducting , so he does n't have to play the same Mozart and Strauss concertos over and over again .\",\n",
       " 'Richard Stoltzman has taken a gentler , more audience - friendly approach .',\n",
       " \"Years ago , he collaborated with the new music gurus Peter Serkin and Fred Sherry in the very countercultural chamber group Tashi , which won audiences over to dreaded contemporary scores like Messiaen 's `` Quartet for the End of Time . ''\",\n",
       " 'Today , the pixie - like clarinetist has mostly dropped the missionary work ( though a touch of the old Tashi still survives ) and now goes on the road with piano , bass , a slide show , and a repertoire that ranges from light classical to light jazz to light pop , with a few notable exceptions .',\n",
       " 'Just the thing for the Vivaldi - at - brunch set , the yuppie audience that has embraced New Age as its very own easy listening .',\n",
       " \"But you ca n't dismiss Mr. Stoltzman 's music or his motives as merely commercial and lightweight .\",\n",
       " 'He believes in what he plays , and he plays superbly .',\n",
       " \"His recent appearance at the Metropolitan Museum , dubbed `` A Musical Odyssey , '' was a case in point .\",\n",
       " 'It felt more like a party , or a highly polished jam session with a few friends , than a classical concert .',\n",
       " \"Clad in his trademark black velvet suit , the soft - spoken clarinetist announced that his new album , `` Inner Voices , '' had just been released , that his family was in the front row , and that it was his mother 's birthday , so he was going to play her favorite tune from the record .\",\n",
       " \"He launched into Saint - Saens 's `` The Swan '' from `` Carnival of the Animals , '' a favorite encore piece for cellists , with lovely , glossy tone and no bite .\",\n",
       " \"Then , as if to show that he could play fast as well , he offered the second movement from Saint - Saens 's Sonata for Clarinet , a whimsical , puckish tidbit that reflected the flip side of the Stoltzman personality .\",\n",
       " 'And so it went through the first half : an ingeniously chosen potpourri of pieces , none longer than five minutes , none that would disturb or challenge a listener .',\n",
       " 'Mr. Stoltzman introduced his colleagues : Bill Douglas , pianist / bassoonist / composer and an old buddy from Yale , and jazz bassist Eddie Gomez .',\n",
       " \"An improvisational section was built around pieces by Mr. Douglas , beginning with `` Golden Rain , '' a lilting , laid - back lead in to the uptempo `` Sky , '' which gave Mr. Stoltzman the opportunity to wail in a high register and show off his fleet fingers .\",\n",
       " \"Bach 's `` Air '' followed .\",\n",
       " \"Mr. Stoltzman tied the composer in by proclaiming him `` the great improviser of the 18th century , '' and then built on the image by joining with Mr. Douglas in some Bach two - part inventions , cleverly arranged for clarinet and bassoon by Mr. Douglas .\",\n",
       " \"Keeping the mood light , the two then chanted and chortled their way through some murderous polyrhythms , devised by Mr. Douglas as an alternative to Hindemith 's dry theory - teaching techniques , and then , with Mr. Gomez , soared and improvised on the composer 's tight `` Bebop Etudes . ''\",\n",
       " \"The end of the first half , however , brought what the standing - room - only crowd seemed to be waiting for : the pop singer Judy Collins , who appears on `` Inner Voices . ''\",\n",
       " \"Glamorous and pure - voiced as ever , Ms. Collins sang Joni Mitchell 's `` For Free '' -- about an encounter with a street - corner clarinetist , to which Mr. Stoltzman contributed a clarinet obligatto -- and Mr. Douglas 's lush setting of a Gaelic blessing , `` Deep Peace . ''\",\n",
       " \"`` Deep Peace '' also featured a slide show of lovely but predictable images of clouds , beaches , deserts , sunsets , etc .\",\n",
       " \"It was all too mellow to be believed , but they probably would have gotten away with it , had they not felt compelled to add Ms. Collins 's signature tune , `` Amazing Grace , '' and ask for audience participation .\",\n",
       " 'That went over the permissible line for warm and fuzzy feelings .',\n",
       " 'Was this why some of the audience departed before or during the second half ?',\n",
       " 'Or was it because Ms. Collins had gone ?',\n",
       " \"Either way it was a pity , because Mr. Stolzman offered the most substantial music of the evening just after intermission : Steve Reich 's `` New York Counterpoint , '' one of a series of Reich works that juxtapose a live performer with recorded tracks of his or her own playing .\",\n",
       " \"( Mr. Reich 's new `` Different Trains '' for string quartet uses the technique magisterially . )\",\n",
       " \"Mr. Stoltzman must have worried that his audience might not be able to take it : He warned us in advance that `` New York Counterpoint '' lasts 11 1/2 minutes .\",\n",
       " \"He also unfortunately illustrated this intricate , jazzy tapestry with Mr. Pearson 's images , this time of geometric or repeating objects , in a kitschy mirroring of the musical structure that was thoroughly distracting from Mr. Reich 's piece and Mr. Stoltzman 's elegant execution of it .\",\n",
       " 'The rest of the concert was more straight jazz and mellow sounds written by Charlie Parker , Ornette Coleman , Bill Douglas and Eddie Gomez , with pictures for the Douglas pieces .',\n",
       " 'It was enjoyable to hear accomplished jazz without having to sit in a smoke - filled club , but like the first half , much of it was easy to take and ultimately forgettable .',\n",
       " 'Is this the future of chamber music ?',\n",
       " 'Managers and presenters insist that chamber music concerts are a hard sell , but can audiences really enjoy them only if the music is purged of threatening elements , served up in bite - sized morsels and accompanied by visuals ?',\n",
       " \"What 's next ?\",\n",
       " 'Slides to illustrate Shostakovich quartets ?',\n",
       " 'It was not an unpleasant evening , certainly , thanks to the high level of performance , the compositional talents of Mr. Douglas , and the obvious sincerity with which Mr. Stoltzman chooses his selections .',\n",
       " 'But it was neither deep nor lasting : light entertainment that was no substitute for an evening of Brahms .',\n",
       " 'Ms. Waleson is a free - lance writer based in New York .',\n",
       " \"One of Ronald Reagan 's attributes as President was that he rarely gave his blessing to the claptrap that passes for `` consensus '' in various international institutions .\",\n",
       " \"In fact , he liberated the U.S. from one of the world 's most corrupt organizations -- UNESCO .\",\n",
       " 'This is the U.N. group that managed to traduce its own charter of promoting education , science and culture .',\n",
       " 'Ever since , the remaining members have been desperate for the United States to rejoin this dreadful group .',\n",
       " \"Now UNESCO apologists are lobbying President Bush to renege on President Reagan 's decision to depart .\",\n",
       " 'But we can think of many reasons to stay out for the foreseeable future and well beyond .',\n",
       " 'The U.S. , along with Britain and Singapore , left the agency when its anti-Western ideology , financial corruption and top leadership got out of hand .',\n",
       " \"The personal antics of agency Director Amadou - Mahtar M'Bow drew much attention , such as when several of his top aides were uncovered as KGB plants and ejected from France and when a mysterious office fire was set just before Congress sent accountants to trace U.S. funds .\",\n",
       " \"Mr. M'Bow was an extreme case , but even his replacement , the more personally genial Spanish biochemist Federico Mayor , has had little success at achieving reforms .\",\n",
       " \"Several ridiculous projects continue , including the `` New International Economic Order , '' which means redistributionism from the West to pay for everyone else 's statism .\",\n",
       " \"The Orwellian `` New World Information Order '' would give government officials rights against the press ; journalists would be obliged to kowtow to their government , which would have licensing and censorship powers and , indeed , duties to block printing of `` wrong '' ideas .\",\n",
       " \"UNESCO somehow converted the founding U.N. ideals of individual rights and liberty into `` peoples ' rights . ''\",\n",
       " \"Million - dollar conferences were held to chew on subjects such as `` ethical responsibilities of scientists in support of disarmament '' and `` the impact of the activities of transnational corporations . ''\",\n",
       " 'The agency was so totally subverted from the high principles of its founding that even the Soviets now wonder about an agency that seemed so congenial to them .',\n",
       " \"Glasnost may be partly responsible , but Soviet Foreign Minister Eduard Shevardnadze last year admitted , `` The exaggerated ideological approach undermined tolerance intrinsic to UNESCO . ''\",\n",
       " 'UNESCO is now holding its biennial meetings in Paris to devise its next projects .',\n",
       " \"Mr. Mayor 's hope that references to `` press freedom '' would survive unamended seems doomed to failure ; the current phrasing is `` educating the public and media to avoid manipulation . ''\",\n",
       " \"He has n't been able to replace the M'Bow cabal .\",\n",
       " 'Soviets remain in charge of education programs , a former head of an African military tribunal for executions is in charge of culture , and a hard - line Polish communist in exile directs the human - rights and peace division .',\n",
       " \"Of the agency 's 2,750 staff members , 230 are in the field working on actual projects , such as literacy and oceanographic research .\",\n",
       " 'The position of the United States , which once contributed 25 % of the budget , is that nothing has changed .',\n",
       " \"John Bolton , the assistant secretary of state for international organizations , told Congress that the continuing `` statist , restrictive , nondemocratic '' programs make rejoining any time soon `` extremely unlikely . ''\",\n",
       " \"This has n't much bothered the UNESCO delegates , who last week could n't even agree to raise funds by selling off a fancy 19th - century French chateau the agency somehow owns .\",\n",
       " 'Other countries , including West Germany , may have a hard time justifying continued membership .',\n",
       " 'We see an even stronger argument against UNESCO than its unsurprising failure to reform .',\n",
       " 'This is that the Reagan Revolution spanning Eastern Europe and Tiananmen Square shows the power of ideas unencumbered by international civil servants or government functionaries .',\n",
       " 'Free markets , free minds and free elections have an appeal that seems to get muddled only when delivered through U.N. organizations -- which of course are made up largely of governments that fear these principles at home .',\n",
       " 'The Babelists of the United Nations are experts at obfuscation .',\n",
       " \"This can have its purposes at times , but there 's no reason to cloud the importance and allure of Western concepts of freedom and justice .\",\n",
       " 'We can see plenty of reasons to stay out , and none to rejoin UNESCO .',\n",
       " 'Researchers at Plant Genetic Systems N.V. in Belgium said they have developed a genetic engineering technique for creating hybrid plants for a number of key crops .',\n",
       " 'The researchers said they have isolated a plant gene that prevents the production of pollen .',\n",
       " 'The gene thus can prevent a plant from fertilizing itself .',\n",
       " 'Such so - called male - sterile plants can then be fertilized by pollen from another strain of the plant , thereby producing hybrid seed .',\n",
       " \"The new generation of plants will possess the flourishing , high - production trait known as `` hybrid vigor , '' similar to that now seen in hybrid corn .\",\n",
       " \"`` The development could have a dramatic effect on farm production , especially cotton , '' said Murray Robinson , president of Delta & Pine Land Co. , a Southwide Inc. subsidiary that is one of the largest cotton seed producers in the U.S. .\",\n",
       " 'On a commercial scale , the sterilization of the pollen - producing male part has only been achieved in corn and sorghum feed grains .',\n",
       " \"That 's because the male part , the tassel , and the female , the ear , are some distance apart on the corn plant .\",\n",
       " 'In a labor - intensive process , the seed companies cut off the tassels of each plant , making it male sterile .',\n",
       " 'They sow a row of male - fertile plants nearby , which then pollinate the male - sterile plants .',\n",
       " 'The first hybrid corn seeds produced using this mechanical approach were introduced in the 1930s and they yielded as much as 20 % more corn than naturally pollinated plants .',\n",
       " 'The vast majority of the U.S. corn crop now is grown from hybrid seeds produced by seed companies .',\n",
       " 'A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .',\n",
       " 'The male part , the anthers of the plant , and the female , the pistils , of the same plant are within a fraction of an inch or even attached to each other .',\n",
       " 'The anthers in these plants are difficult to clip off .',\n",
       " 'In China , a great number of workers are engaged in pulling out the male organs of rice plants using tweezers , and one - third of rice produced in that country is grown from hybrid seeds .',\n",
       " 'At Plant Genetic Systems , researchers have isolated a pollen - inhibiting gene that can be inserted in a plant to confer male sterility .',\n",
       " \"Jan Leemans , research director , said this gene was successfully introduced in oil - producing rapeseed plants , a major crop in Europe and Canada , using as a carrier a `` promoter gene '' developed by Robert Goldberg at the University of California in Los Angeles .\",\n",
       " 'The sterilizing gene is expressed just before the pollen is about to develop and it deactivates the anthers of every flower in the plant .',\n",
       " \"Mr. Leemans said this genetic manipulation does n't hurt the growth of that plant .\",\n",
       " 'The researchers also pulled off a second genetic engineering trick in order to get male - sterile plants in large enough numbers to produce a commercial hybrid seed crop .',\n",
       " 'They attached a second gene , for herbicide resistance , to the pollen - inhibiting gene .',\n",
       " 'Both genes are then inserted into a few greenhouse plants , which are then pollinated and allowed to mature and produce seed .',\n",
       " 'The laws of heredity dictate that half of the plants springing from these greenhouse - produced seeds will be male sterile and herbicide resistant and half will be male fertile and herbicide susceptible .',\n",
       " 'The application of herbicide would kill off the male - fertile plants , leaving a large field of male - sterile plants that can be cross-pollinated to produce hybrid seed .',\n",
       " 'Mr. Leemans said the hybrid rapeseeds created with this genetic engineering yield 15 % to 30 % more output than the commercial strains used currently .',\n",
       " \"`` This technique is applicable to a wide variety of crops , '' he said , and added that some modifications may be necessary to accommodate the peculiarities of each type of crop .\",\n",
       " 'He said the company is experimenting with the technique on alfalfa , and plans to include cotton and corn , among other crops .',\n",
       " 'He said that even though virtually all corn seeds currently planted are hybrids , the genetic approach will obviate the need for mechanical emasculation of anthers , which costs U.S. seed producers about $ 70 million annually .',\n",
       " 'In recent years , demand for hybrid seeds has spurred research at a number of chemical and biotechnology companies , including Monsanto Co. , Shell Oil Co. and Eli Lilly & Co .',\n",
       " \"One technique developed by some of these companies involves a chemical spray supposed to kill only a plant 's pollen .\",\n",
       " \"But there have been problems with chemical sprays damaging plants ' female reproductive organs and concern for the toxicity of such chemical sprays to humans , animals and beneficial insects .\",\n",
       " \"However , Paul Johanson , Monsanto 's director of plant sciences , said the company 's chemical spray overcomes these problems and is `` gentle on the female organ . ''\",\n",
       " 'Biosource Genetics Corp. , Vacaville , Calif. , is developing a spray containing a gene that spreads from cell to cell and interferes with the genes that are responsible for producing pollen .',\n",
       " \"This gene , called `` gametocide , '' is carried into the plant by a virus that remains active for a few days .\",\n",
       " \"Robert Erwin , president of Biosource , called Plant Genetic 's approach `` interesting '' and `` novel , '' and `` complementary rather than competitive . ''\",\n",
       " \"`` There is a large market out there hungry for hybrid seeds , '' he said .\",\n",
       " \"Mr. Robinson of Delta & Pine , the seed producer in Scott , Miss. , said Plant Genetic 's success in creating genetically engineered male steriles does n't automatically mean it would be simple to create hybrids in all crops .\",\n",
       " \"That 's because pollination , while easy in corn because the carrier is wind , is more complex and involves insects as carriers in crops such as cotton .\",\n",
       " \"`` It 's one thing to say you can sterilize , and another to then successfully pollinate the plant , '' he said .\",\n",
       " 'Nevertheless , he said , he is negotiating with Plant Genetic to acquire the technology to try breeding hybrid cotton .',\n",
       " \"A bitter conflict with global implications has erupted between Nomura Securities Co. and Industrial Bank of Japan , two of the world 's most powerful financial companies .\",\n",
       " \"The clash is a sign of a new toughness and divisiveness in Japan 's once - cozy financial circles .\",\n",
       " \"Not only are Japan 's financial institutions putting their enormous clout to work ; increasingly they 're squaring off against one another in unprecedented public fashion .\",\n",
       " 'Already , the consequences are being felt by other players in the financial markets -- even governments .',\n",
       " 'What triggered the latest clash was a skirmish over the timing of a New Zealand government bond issue .',\n",
       " \"Nomura was attempting to organize the 50 billion - yen ( $ 352 million ) borrowing in Japan at a time when many Japanese banks , led by Industrial Bank of Japan , were pressuring the Wellington government to help them recover loans made to a defunct investment bank that had been owned by New Zealand 's civil - service pension fund .\",\n",
       " 'Unwilling to put up new money for New Zealand until those debts are repaid , most banks refused even to play administrative roles in the new financing , forcing an embarrassed Nomura to postpone it this week .',\n",
       " \"The dispute shows clearly the global power of Japan 's financial titans .\",\n",
       " \"Aside from Nomura 's injured pride , the biggest victim so far has been the New Zealand government .\",\n",
       " \"Barred by its budget law from making any new domestic bond issues , Wellington 's Debt Management Office had been casting abroad to raise the 3 billion New Zealand dollars ( US$ 1.76 billion ) to NZ$ 4 billion it needs to come up with by the end of its fiscal year next June 30 .\",\n",
       " \"With Japan 's cash - flush banks aligned against it , though , raising money may be difficult .\",\n",
       " 'Not only can they block Wellington from raising money in Japan , bankers here say , but as the largest underwriters in the Eurobond market , they might be able to scuttle borrowings there , too .',\n",
       " \"New Zealand 's finance minister , David Caygill , lashed out at such suggestions .\",\n",
       " \"He told reporters in Wellington Tuesday that the government had n't guaranteed the loans to DFC New Zealand Ltd. , an investment bank 80 % - owned by the National Provident Fund , and would n't bail it out .\",\n",
       " \"`` It may very well be what the Japanese banks want , '' he told Radio New Zealand .\",\n",
       " \"`` I think it would be irresponsible and I am not about to be blackmailed by Japanese banks or any other international interests . ''\",\n",
       " \"No less significant than the Japanese banks ' attempt to cut off funds to pressure a foreign government are the implications of a confrontation between Japan securities and banking industries .\",\n",
       " 'Anxiety is rising over recent government proposals to eventually lower the strict barriers that now separate -- and protect -- the two industries from each other .',\n",
       " 'Both sides are jealously guarding their turf , and relations have been at a flashpoint for months .',\n",
       " 'The banks badly want to break into all aspects of the securities business .',\n",
       " \"Meanwhile , the securities companies -- most of them smaller than the banks -- are seeking access only to limited kinds of banking that would n't open them to the full brunt of competition from the banks .\",\n",
       " \"Nomura , the world 's biggest securities company largely by virtue of its protected home field , and Industrial Bank of Japan , Japan 's most innovative and aggressive bank in capital markets abroad , captain the opposing sides .\",\n",
       " 'And their suspicions of each other run deep .',\n",
       " 'In the past year , both have tried to stretch the limits of their businesses .',\n",
       " 'Nomura started a credit - card venture with American Express Co. that allowed cardholders to use their Nomura securities accounts like a bank account , attracting the wrath of banks .',\n",
       " 'And Industrial Bank of Japan started up a London securities subsidiary that sells Japanese stocks to non-Japanese institutions overseas , a move that stirred the anger of the stock brokerage firms .',\n",
       " 'The New Zealand bond issue simply has brought the two institutions face - to - face .',\n",
       " \"ITEL CORP. reported third - quarter earnings , which were mistakenly shown in the Quarterly Earnings Surprises table in yesterday 's edition to be lower than the average of analysts ' estimates .\",\n",
       " \"On a pretax basis , Itel 's third - quarter earnings of 30 cents a share were actually 7.14 % higher than the average of estimates .\",\n",
       " 'Raymond E. Ross , 53 years old , formerly group vice president , U.S. plastics machinery , at this machine tool , plastics machinery and robots concern , was named senior vice president , industrial systems , succeeding David A. Entrekin , who resigned Monday .',\n",
       " 'John A. Conlon Jr. , 45 , was named a managing director at this investment - banking company .',\n",
       " 'He will be in charge of research , equity sales and trading , and the syndicate operation of Rothschild .',\n",
       " 'Mr. Conlon was executive vice president and director of the equity division of the international division of Nikko Securities Co .',\n",
       " \"As Yogi Berra might say , it 's deja vu all over again .\",\n",
       " \"Crouched at shortstop , Bert Campaneris , once Oakland 's master thief , effortlessly scoops up a groundball and flips it to second .\",\n",
       " \"In the outfield , Paul Blair , the Orioles ' eight - time Gold Glove winner , elegantly shags a fly .\",\n",
       " 'On the mound , former Red Sox great Luis Tiant , the wily master of 1,001 moves , throws an off - speed strike .',\n",
       " \"`` Babies , kiddies , '' growls their manager -- a fellow named Earl Weaver , who , in a different time , handled four World Series teams and now handles the Gold Coast Suns .\",\n",
       " \"`` Old - time kiddies , '' he says .\",\n",
       " 'Perhaps .',\n",
       " 'But for the next few months , these boys of summers long past are going to be reveling in an Indian summer of the soul .',\n",
       " \"Now that the baseball season is officially over , you see , it 's time for a new season to begin .\",\n",
       " 'Today is the debut of the Senior Professional Baseball Association , a new eight - team pro sports circuit , modeled after the highly successful senior tennis and golf tours and complete with good salaries , a cable television contract and even expansion plans .',\n",
       " 'One hundred and ninety two former greats , near greats , hardly knowns and unknowns begin a 72 - game , three - month season in spring - training stadiums up and down Florida .',\n",
       " \"For everyone involved , it 's one more swig of that elixir of youth , baseball .\",\n",
       " \"`` Someone always makes you quit , '' says legendary St. Louis Cardinals centerfielder Curt Flood , the league 's commissioner .\",\n",
       " \"`` You feel you want one more -- one more at - bat , one more hit , one more game . ''\",\n",
       " 'Until the baby - faced heroes of today reclaim these ballparks for spring training , there is one more .',\n",
       " 'And not just for the players .',\n",
       " \"It 's one more for the baseball - loving lawyers , accountants and real estate developers who ponied up about $ 1 million each for the chance to be an owner , to step into the shoes of a Gene Autry or have a beer with Rollie Fingers .\",\n",
       " \"`` Nothing can be better than this , '' says Don Sider , owner of the West Palm Beach Tropics .\",\n",
       " 'Early in the morning Mr. Sider , an estate lawyer , pores over last wills and testaments .',\n",
       " 'Midmorning , he dons an orange - and - blue uniform and , for fun , may field a bunt from Dave Kingman .',\n",
       " \"It 's one more , too , for the fans who dream of a season that never ends .\",\n",
       " \"`` I feel like a little kid , '' says a gleeful Alex de Castro , a car salesman , who has stopped by a workout of the Suns to slip six Campaneris cards to the Great Man Himself to be autographed .\",\n",
       " \"The league 's promoters hope retirees and tourists will join die - hard fans like Mr. de Castro and pack the stands to see the seniors .\",\n",
       " 'The league is the brainchild of Colorado real estate developer James Morley -- once a minor - leaguer himself -- who says he had the idea last January while lying on a beach in Australia .',\n",
       " 'When he sent letters offering 1,250 retired major leaguers the chance of another season , 730 responded .',\n",
       " 'Eventually , about 250 made the trip to Florida to compete for the available slots .',\n",
       " '( Players have to be 35 or older , except for catchers , who are eligible at 32 because life behind the plate is so rough . )',\n",
       " 'For some players , the lure is money -- up to $ 15,000 a month .',\n",
       " 'Others , just released from the majors , hope the senior league will be their bridge back into the big - time .',\n",
       " \"But as they hurl fireballs that smolder rather than burn , and relive old duels in the sun , it 's clear that most are there to make their fans cheer again or recapture the camaraderie of seasons past or prove to themselves and their colleagues that they still have it -- or something close to it .\",\n",
       " '`` My fastball is good .',\n",
       " \"Real good , '' says 39 - year - old Pete Broberg , working in the midday heat of the Tropics camp .\",\n",
       " \"Mr. Broberg , who started with the now - defunct Washington Senators , says that when he left baseball in 1978 , he `` never looked back . ''\",\n",
       " 'For a long time , he ignored baseball altogether , even the sports pages .',\n",
       " \"Now Mr. Broberg , a lawyer , claims he 'd play for free .\",\n",
       " \"`` You ca n't give it up that easily , '' he says .\",\n",
       " \"`` I tried . ''\",\n",
       " \"The nagging memory of one afternoon fourteen years ago drove Jim Gideon , a lean 36 - year - old righthander to take a four - month leave from selling insurance in Texas to try out for Mr. Weaver 's team .\",\n",
       " \"`` It does n't replace pitching in the majors , but it proves to me that I would have been able to play if I 'd stayed healthy , '' he says .\",\n",
       " 'Back in 1975 , late in the season , a then - 21 Mr. Gideon made his only major league appearance , five and two - thirds innings for the Texas Rangers against the Chicago White Sox .',\n",
       " \"He gave up seven hits , walked five and did n't get a decision .\",\n",
       " 'Arm troubles forced him back to the minors the next year .',\n",
       " \"`` There 's a satisfaction in going against the rules , '' offers Will McEnaney , once a stopper with Cincinnati 's Big Red Machine .\",\n",
       " \"He means the rule that a player ca n't cut it after a certain age .\",\n",
       " 'These days he hustles to house - painting jobs in his Chevy pickup before and after training with the Tropics .',\n",
       " \"While sipping a beer after practice , he vividly recounts getting the Red Sox 's Carl Yastrzemski to pop out to end the 1975 World Series , and repeating the feat against the Yankees ' Roy White in 1976 .\",\n",
       " \"Some of the game 's reigning philosophers dislike the idea of middle - aged men attempting a young man 's sport .\",\n",
       " \"`` I personally do n't enjoy seeing players who I remember vividly from their playing days running about and being gallant about their deficiencies , '' says Roger Angell , New Yorker magazine 's resident baseball sage .\",\n",
       " \"`` I feel people should be allowed to remember players as they were . ''\",\n",
       " \"Worse , says baseball author Lawrence Ritter , `` Someone will get a heart attack and that will be the end of the whole story . ''\",\n",
       " 'But the ballplayers disagree .',\n",
       " 'Most are trim .',\n",
       " 'Some have been training for months ; others only recently left active status .',\n",
       " \"( No one has worked out the players ' average age , but most appear to be in their late 30s . )\",\n",
       " \"And there 's pride .\",\n",
       " \"`` I 'm not going to look stupid , '' vows former Pittsburgh Pirate second baseman Rennie Stennett , sweat dotting his brow as he prepares for some practice swings .\",\n",
       " \"`` It 's going to be a tough league , '' promises the 47 - year - old Mr. Campaneris .\",\n",
       " \"`` There will be a lot of malice . ''\",\n",
       " \"Men who have played hard all their lives are n't about to change their habits , he says .\",\n",
       " \"Nonetheless , one ca n't help wonder whether the game will be just a little bit slower .\",\n",
       " \"At the weatherbeaten Pompano Beach municipal stadium , Mr. Blair , the 45 - year - old former Oriole , knows his power is n't what it used to be .\",\n",
       " 'So he adjusts .',\n",
       " 'He no longer crowds the plate .',\n",
       " \"He 's not thinking about home runs anymore , just base hits .\",\n",
       " \"Still , `` how sweet it is , '' he says , savoring the fat sound of the well - hit line drive that bounces off the center field wall .\",\n",
       " \"And do n't expect many complete games by pitchers -- perhaps three out of 288 , laughs Mr. Fingers , the former Oakland reliever .\",\n",
       " \"Expect `` tricky '' stuff from pitchers , says Mr. Weaver , the manager .\",\n",
       " 'Expect brushbacks but no beanballs , says Mr. McEnaney .',\n",
       " \"Even expect stolen bases , says the wiry and fit Mr. Campaneris : `` If you know how to slide , it 's no problem , '' he says .\",\n",
       " 'And expect slower fastballs .',\n",
       " \"`` I 'm not so young anymore , '' concedes the cigar - chomping , 48 - year - old Mr. Tiant .\",\n",
       " \"`` I wo n't be throwing 90 mph , but I will throw 80 - plus , '' he says .\",\n",
       " \"White - haired Pedro Ramos , at 54 the league 's oldest player and a pitcher - coach with the Suns , has lost even more speed .\",\n",
       " 'Stuffing a wad of Red Man into his cheek , he admits the fastball he brought into the majors in 1955 has become a slowball .',\n",
       " 'Its maximum velocity is 72 mph .',\n",
       " \"But he is n't worried .\",\n",
       " 'He will compensate with the guile learned from his years in the majors .',\n",
       " 'He has good control .',\n",
       " 'He will keep the ball down , move it around .',\n",
       " 'After all , he says , `` Even to make love , you need experience .',\n",
       " \"Alltel Corp. said it will acquire the 55 % of Pond Branch Telephone Company Inc. 's cellular franchise that it does n't own already .\",\n",
       " \"Terms were n't disclosed .\",\n",
       " 'Alltel holds 45 % of the franchise , which has operations in Aiken , S.C. , and Augusta , Ga .',\n",
       " 'Alltel , which provides local telephone service in 25 states , said it exercised its right of first refusal following an offer from an undisclosed third party to acquire the majority position in the franchise .',\n",
       " 'Stewart & Stevenson Services Inc. said it received two contracts totaling $ 19 million to build gas - turbine generators .',\n",
       " 'The separate contracts were from Paragould Light & Water Commission , a utility in Paragould , Ark. , and PSE Inc. , a cogeneration - plant operator in Houston .',\n",
       " 'Stewart & Stevenson makes equipment powered with diesel and gas turbines .',\n",
       " 'Liberty National Bancorp said its acquisition of Florence Deposit Bank , Florence , Ky. , first announced in April , has been completed in a transaction valued at $ 13.1 million .',\n",
       " \"Liberty National exchanged about 78.64 shares of its common stock for each of Florence Deposit 's 5,600 shares outstanding .\",\n",
       " 'Liberty National , a bank holding company , has assets exceeding $ 3 billion .',\n",
       " 'Hani Zayadi was appointed president and chief executive officer of this financially troubled department store chain , effective Nov. 15 , succeeding Frank Robertson , who is retiring early .',\n",
       " \"Mr. Zayadi was previously president and chief operating officer of Zellers Inc. , a retail chain that is owned by Toronto - based Hudson 's Bay Co. , Canada 's largest department store operator .\",\n",
       " 'Tuesday , October 31 , 1989',\n",
       " \"The key U.S. and foreign annual interest rates below are a guide to general levels but do n't always represent actual transactions .\",\n",
       " 'PRIME RATE : 10 1/2 % .',\n",
       " 'The base rate on corporate loans at large U.S. money center commercial banks .',\n",
       " 'FEDERAL FUNDS : 9 % high , 8 13/16 % low , 8 7/8 % near closing bid , 8 15/16 % offered .',\n",
       " 'Reserves traded among commercial banks for overnight use in amounts of $ 1 million or more .',\n",
       " 'Source : Fulton Prebon ( U.S.A . ) Inc .',\n",
       " 'DISCOUNT RATE : 7 % .',\n",
       " 'The charge on loans to depository institutions by the New York Federal Reserve Bank .',\n",
       " 'CALL MONEY : 9 3/4 % to 10 % .',\n",
       " 'The charge on loans to brokers on stock exchange collateral .',\n",
       " 'COMMERCIAL PAPER placed directly by General Motors Acceptance Corp. : 8.55 % 30 to 44 days ; 8.25 % 45 to 59 days ; 8.40 % 60 to 89 days ; 8 % 90 to 119 days ; 7.90 % 120 to 149 days ; 7.80 % 150 to 179 days ; 7.55 % 180 to 270 days .',\n",
       " 'COMMERCIAL PAPER : High - grade unsecured notes sold through dealers by major corporations in multiples of $ 1,000 : 8.62 % 30 days ; 8.55 % 60 days ; 8.45 % 90 days .',\n",
       " 'CERTIFICATES OF DEPOSIT : 8.09 % one month ; 8.04 % two months ; 8.03 % three months ; 7.96 % six months ; 7.92 % one year .',\n",
       " 'Average of top rates paid by major New York banks on primary new issues of negotiable C.D.s , usually on amounts of $ 1 million and more .',\n",
       " 'The minimum unit is $ 100,000 .',\n",
       " 'Typical rates in the secondary market : 8.53 % one month ; 8.50 % three months ; 8.30 % six months .',\n",
       " 'BANKERS ACCEPTANCES : 8.49 % 30 days ; 8.44 % 60 days ; 8.27 % 90 days ; 8.12 % 120 days ; 8.05 % 150 days ; 7.98 % 180 days .',\n",
       " 'Negotiable , bank - backed business credit instruments typically financing an import order .',\n",
       " 'LONDON LATE EURODOLLARS : 8 3/4 % to 8 5/8 % one month ; 8 3/4 % to 8 5/8 % two months ; 8 11/16 % to 8 9/16 % three months ; 8 9/16 % to 8 7/16 % four months ; 8 1/2 % to 8 3/8 % five months ; 8 7/16 % to 8 5/16 % six months .',\n",
       " 'LONDON INTERBANK OFFERED RATES ( LIBOR ) : 8 3/4 % one month ; 8 11/16 % three months ; 8 7/16 % six months ; 8 7/16 % one year .',\n",
       " 'The average of interbank offered rates for dollar deposits in the London market based on quotations at five major banks .',\n",
       " 'FOREIGN PRIME RATES : Canada 13.50 % ; Germany 9 % ; Japan 4.875 % ; Switzerland 8.50 % ; Britain 15 % .',\n",
       " \"These rate indications are n't directly comparable ; lending practices vary widely by location .\",\n",
       " 'TREASURY BILLS : Results of the Monday , October 30 , 1989 , auction of short - term U.S. government bills , sold at a discount from face value in units of $ 10,000 to $ 1 million : 7.78 % 13 weeks ; 7.62 % 26 weeks .',\n",
       " 'FEDERAL HOME LOAN MORTGAGE CORP . ( Freddie Mac ) : Posted yields on 30 - year mortgage commitments for delivery within 30 days .',\n",
       " '9.78 % , standard conventional fixed - rate mortgages ; 7.875 % , 2 % rate capped one - year adjustable rate mortgages .',\n",
       " 'Source : Telerate Systems Inc .',\n",
       " 'FEDERAL NATIONAL MORTGAGE ASSOCIATION ( Fannie Mae ) : Posted yields on 30 year mortgage commitments for delivery within 30 days ( priced at par ) 9.75 % , standard conventional fixed - rate mortgages ; 8.75 % , 6/2 rate capped one - year adjustable rate mortgages .',\n",
       " 'Source : Telerate Systems Inc .',\n",
       " 'MERRILL LYNCH READY ASSETS TRUST : 8.63 % .',\n",
       " 'Annualized average rate of return after expenses for the past 30 days ; not a forecast of future returns .',\n",
       " \"Canada 's gross domestic product rose an inflation - adjusted 0.3 % in August , mainly as a result of service - industry growth , Statistics Canada , a federal agency , said .\",\n",
       " 'The August GDP was up 2.4 % from its year - earlier level .',\n",
       " \"GDP is the total value of a nation 's output of goods and services .\",\n",
       " 'Statistics Canada said service - industry output in August rose 0.4 % from July .',\n",
       " 'Output of goods - producing industries increased 0.1 % .',\n",
       " 'Separately , Statistics Canada reported that its industrial - product price index dropped 0.2 % in September , its third consecutive monthly decline .',\n",
       " 'It also reported a 2.6 % decline in its raw - materials price index for September .',\n",
       " 'Columbia Pictures Entertainment Inc. was dropped , effective today , from the recreational products and services industry group of the Dow Jones Equity Market Index .',\n",
       " 'Columbia Pictures is being acquired by Sony Corp. , which is based in Japan .',\n",
       " \"People 's Savings Financial Corp. said it will buy back as much as 10 % of its 2.4 million shares outstanding because the stock is undervalued .\",\n",
       " \"The holding company said it has been `` unfairly associated '' with other banks in New England that have had major loan losses in recent quarters .\",\n",
       " \"The company said its People 's Savings Bank unit does n't have a `` large exposure to construction and commercial loans that have caused the loan - loss problems in many of the banks .\",\n",
       " 'A seat on the Chicago Mercantile Exchange was sold for $ 410,000 , down $ 6,000 from the previous sale Oct .',\n",
       " 'Seats currently are quoted at $ 400,000 bid , $ 425,000 asked .',\n",
       " 'The record price for a full membership on the exchange is $ 550,000 , set March 9 .',\n",
       " 'In a surprise move , the British government cleared the way for a bidding war for Jaguar PLC by agreeing to remove an obstacle to a takeover of the auto maker .',\n",
       " \"Trade and Industry Secretary Nicholas Ridley told the House of Commons yesterday that he will relinquish the government 's so - called golden share in the company as long as Jaguar shareholders agree .\",\n",
       " 'The golden share restricts any individual holding to 15 % and expires at the end of 1990 .',\n",
       " \"It was in Jaguar 's best interests `` for the company 's future to be assured and the present climate of uncertainty resolved as quickly as possible , '' Mr. Ridley said .\",\n",
       " \"Mr. Ridley 's decision fires the starting pistol for perhaps a costly contest between the world 's auto giants for Britain 's leading luxury - car maker .\",\n",
       " 'Both General Motors Corp. and Ford Motor Co. have been trying to amass 15 % stakes in Jaguar .',\n",
       " 'Ford , which already has an unwelcome 13.2 % holding , is prepared to bid for the entire company and had lobbied the government to lift the takeover restrictions early .',\n",
       " 'GM has been negotiating a friendly transaction with Jaguar that likely would involve joint ventures and an eventual stake of just under 30 % .',\n",
       " \"But the government 's action , which caught Jaguar management flat - footed , may scuttle the GM minority deal by forcing it to fight for all of Jaguar .\",\n",
       " \"`` I ca n't believe they ( GM ) will let Ford have a free run , '' said Stephen Reitman , a European auto industry analyst at UBS - Phillips & Drew .\",\n",
       " \"`` I am sure they will be going for a full bid . ''\",\n",
       " 'Many investors certainly believe a bidding war is imminent .',\n",
       " \"Jaguar shares skyrocketed yesterday after Mr. Ridley 's announcement , following their temporary suspension on London 's Stock Exchange .\",\n",
       " 'In late trading , the shares were up a whopping 122 pence ( $ 1.93 ) -- a 16.3 % gain -- to a record 869 pence on very heavy volume of 9.7 million shares .',\n",
       " 'In the U.S. over - the - counter market , Jaguar shares trading as American Depositary Receipts closed at $ 13.625 , up $ 1.75 .',\n",
       " 'Analysts expect Ford will make the first move , perhaps today , with an initial offer of about 900 pence ( $ 14.25 ) a share .',\n",
       " 'Such a proposal values Jaguar at more than # 1.6 billion ( $ 2.53 billion ) .',\n",
       " 'Speculation about a takeover fight has sent Jaguar shares soaring in the past six weeks .',\n",
       " \"The share price was languishing at about 400 pence before Ford 's Sept. 19 announcement of its interest in a minority stake .\",\n",
       " \"Ford is `` in the driving seat at the moment , '' observed Bob Barber , an auto analyst at brokers James Capel & Co .\",\n",
       " \"An aggressive Ford bid for Jaguar would put pressure on GM to make a better offer as the British company 's `` white knight . ''\",\n",
       " \"Such a countermove could end Jaguar 's hopes for remaining independent and British - owned .\",\n",
       " \"But it is n't clear how long GM would be willing to fight Ford for Jaguar .\",\n",
       " \"Because of their longstanding rivalry , GM just `` wants to make sure Ford pays a huge packet for ( Jaguar ) , '' said John Lawson , an auto analyst at London 's Nomura Research Institute .\",\n",
       " 'People close to the GM - Jaguar talks agreed that Ford now may be able to shut out General Motors .',\n",
       " \"`` It 's either going to be a shootout , or there only may be one player in town , '' one person said .\",\n",
       " \"Another person close to the talks said , `` It is very hard to justify paying a silly price for Jaguar if an out - and - out bidding war were to start now . ''\",\n",
       " \"In a statement , Jaguar 's board said they `` were not consulted about the ( Ridley decision ) in advance and were surprised at the action taken . ''\",\n",
       " \"The statement emphasized that holders representing 75 % of the shares voting at a special shareholders ' meeting must agree to lift the takeover restrictions .\",\n",
       " \"Jaguar officials in the U.S. noted that Ford , as Jaguar 's largest shareholder , now has the power to call for such a meeting .\",\n",
       " 'U.S. auto analysts also noted that Ford is in the best position to benefit from the large number of Jaguar shares that have moved over the past month into the hands of arbitragers waiting for the highest takeover bid .',\n",
       " \"Jaguar 's own defenses against a hostile bid are weakened , analysts add , because fewer than 3 % of its shares are owned by employees and management .\",\n",
       " \"Ford officials in the U.S. declined to comment on the British government 's action or on any plans to call a special Jaguar shareholders meeting .\",\n",
       " \"But GM officials said they , too , were surprised by the move , which left them to `` consider all our options and explore matters further . ''\",\n",
       " \"Although GM has U.S. approval to buy up to 15 % of Jaguar 's stock , it has n't yet disclosed how many shares it now owns .\",\n",
       " 'In a prepared statement , GM suggested its plans for Jaguar would be more valuable in the long run than the initial windfalls investors might reap from a hostile Ford bid .',\n",
       " \"`` Our intensive discussions with Jaguar , at their invitation , '' GM said , `` have as their objectives to create a cooperative business relationship with Jaguar that would provide for the continued independence of this great British car company , to ensure a secure future for its employees and to provide an attractive long - term return for its shareholders . ''\",\n",
       " \"Jaguar was shocked by Mr. Ridley 's decision , because management had believed the government would n't lift the golden share without consulting the company first .\",\n",
       " 'Indeed , the government is taking a calculated risk .',\n",
       " \"Mr. Ridley 's announcement set off a howl of protests from members of the opposition Labor Party , who accused the Thatcher administration of backing down on promised protection for a privatized company .\",\n",
       " 'The British government retained the single golden share after selling its stake in Jaguar in',\n",
       " \"The Conservative government 's decision may reflect its desire to shed a politically sensitive issue well before the next election , expected in late 1991 .\",\n",
       " \"`` It 's now a very good time politically to get this over and done with , '' observed Daniel Jones , professor of motor industry management at the University of Cardiff in Wales .\",\n",
       " \"The government , already buffeted by high interest rates and a slowing economy , has been badly hurt by last week 's shake - up in Mrs. Thatcher 's cabinet .\",\n",
       " \"At the same time , the government did n't want to appear to favor GM by allowing a minority stake that might preclude a full bid by Ford .\",\n",
       " 'Mr. Ridley hinted at this motive in answering questions from members of Parliament after his announcement .',\n",
       " \"He said he was giving up the golden share `` to clear the way so the playing field is level between all contestants . ''\",\n",
       " 'Bradley A. Stertz in Detroit contributed to this article .',\n",
       " \"Dow Chemical Co. , Midland , Mich. , and Eli Lilly & Co. , Indianapolis , said they completed the formation of Dow Elanco , a joint venture combining their plant - sciences businesses as well as Dow 's industrial pest - control business .\",\n",
       " 'The companies said Dow Elanco will be the largest research - based agricultural concern in North America , with projected first - year revenue of $ 1.5 billion .',\n",
       " 'Dow will own 60 % of the venture , with Eli Lilly holding the rest .',\n",
       " 'The venture will be based in Indianapolis .',\n",
       " 'William A. Wise , 44 years old , president of the El Paso Natural Gas Co. unit of this energy and natural - resources concern , was named to the additional post of chief executive officer , succeeding Travis H. Petty , 61 , who continues as a vice chairman of the parent .',\n",
       " 'Erwin Tomash , the 67 - year - old founder of this maker of data communications products and a former chairman and chief executive , resigned as a director .',\n",
       " 'Dataproducts is fighting a hostile tender offer by DPC Acquisition Partners , a group led by New York - based Crescott Investments Associates .',\n",
       " 'Under the circumstances , Dataproducts said , Mr. Tomash said he was unable to devote the time required because of other commitments .',\n",
       " 'Mr. Tomash will remain as a director emeritus .',\n",
       " 'The company had no comment on whether a replacement would be named .',\n",
       " 'Robert Q. Marston , president emeritus , University of Florida , and a director of this maker of medical devices , was named chairman .',\n",
       " \"Dr. Marston , 66 years old , succeeds Alexander T. Daignault , 72 , who did n't stand for re-election due to mandatory board retirement policy .\",\n",
       " 'SFE Technologies said William P. Kuehn was elected chairman and chief executive officer of this troubled electronics parts maker .',\n",
       " 'The 45 - year - old Mr. Kuehn , who has a background in crisis management , succeeds Alan D. Rubendall , 45 .',\n",
       " \"Jerome J. Jahn , executive vice president and chief financial officer , said Mr. Rubendall was resigning by `` mutual agreement '' with the board .\",\n",
       " \"`` He is going to pursue other interests , '' Mr. Jahn said .\",\n",
       " \"Mr. Rubendall could n't be reached .\",\n",
       " 'Mr. Kuehn , the company said , will retain the rest of the current management team .',\n",
       " 'For the nine months ended July 29 , SFE Technologies reported a net loss of $ 889,000 on sales of $ 23.4 million .',\n",
       " 'That compared with an operating loss of $ 1.9 million on sales of $ 27.4 million in the year - earlier period .',\n",
       " 'In national over - the - counter trading , SFE Technologies shares closed yesterday at 31.25 cents a share , up 6.25 cents .',\n",
       " 'Sales of new cars in Europe fell 4.2 % in September from a year earlier and analysts say the market could continue to soften in the months ahead .',\n",
       " 'After a stronger - than - expected pace early this year , analysts say the market , after a series of sharp swings in recent months , now shows signs of retreating .',\n",
       " \"Statistics from 12 countries which normally account for 94 % of non-communist Europe 's passenger car sales showed new car registrations totaled 911,606 in September , down 21 % from August and down 4.2 % for the year to date .\",\n",
       " 'Tokyo stocks rebounded Tuesday from two consecutive daily losses in relatively active dealings .',\n",
       " 'London shares also rose , while trading in Frankfurt , West Germany , ended higher .',\n",
       " 'In Tokyo , the Nikkei index of 225 selected issues was up 132.00 points to 35549.44 .',\n",
       " 'The index fell 109.85 Monday .',\n",
       " 'Volume on the First Section was estimated at 900 million shares , up from 582 million shares Monday .',\n",
       " 'Advancing issues outnumbered decliners 542 to 362 , while 208 issues were unchanged .',\n",
       " 'Small - lot buying targeted at incentive - backed issues pushed up the Nikkei .',\n",
       " 'But other sectors failed to attract investor interest and remained sluggish , making overall trading appear mixed .',\n",
       " 'Individuals and corporations , as well as dealers trading for their own account , actively bought Tuesday .',\n",
       " \"An official at Wako Securities said these investors feel the need to make quick profits , despite destabilizing external factors , such as political uncertainty tied to the ruling party 's fate at next year 's Lower House elections - an event which could directly affect the stock market .\",\n",
       " 'The Tokyo Stock Price Index of all issues listed in the First Section , which declined 5.16 on Monday , was up 16.05 , or 0.60 % , at 2692.65 on Tuesday .',\n",
       " 'The Second Section index , which fell 21.44 points Monday , was up 6.84 points , or 0.19 % , to close at 3642.90 .',\n",
       " 'Second Section volume was estimated at 14 million shares , unchanged from Monday .',\n",
       " 'Institutional investors mostly remained on the sidelines Tuesday .',\n",
       " 'A fund manager at a life - insurance company said three factors make it difficult to read market direction .',\n",
       " 'First , he said , domestic interest rates are likely to stay at higher levels as increased anticipation of inflation followed rising consumer prices reported last week .',\n",
       " 'Second , the dollar is showing persistent strength despite a slowdown in the U.S. economy shown by economic indicators .',\n",
       " \"Third , oil prices have n't declined although supply has been increasing .\",\n",
       " \"The topic that attracted participants ' attention was Mitsubishi Estate 's purchase of 51 % of Rockefeller Center Properties , announced late Monday in New York .\",\n",
       " 'Mitsubishi Estate ended the day at 2680 , up 150 .',\n",
       " 'The gains also sparked buying interest in other real - estate companies , traders said .',\n",
       " 'Sumitomo Realty & Development rose 40 to 2170 .',\n",
       " 'Heiwa Real Estate gained 40 to 2210 .',\n",
       " 'Investor focus shifted quickly , traders said .',\n",
       " 'Many of the morning - session winners turned out to be losers by afternoon .',\n",
       " 'In other stock - market news , the Tokyo Stock Exchange said that for the week ended Friday , the balance of margin buying rose 189.8 billion yen ( $ 1.34 billion ) , to 7.160 trillion yen ( $ 50.46 billion ) .',\n",
       " 'The balance of short positions outstanding fell 159.7 billion yen , to 779.8 billion yen .',\n",
       " \"In London , prices finished at intraday peaks , comforted by a reassuring early performance on Wall Street and news that the British government will waive its `` golden share '' in auto maker Jaguar .\",\n",
       " 'But trading was very sketchy , as investment decision makers remain wary from gyrations and upsets of recent weeks .',\n",
       " \"`` Volume has been appalling , '' said a dealer at a British brokerage concern .\",\n",
       " '`` The market was dragged up by the scruff of its neck by Wall Street and by market makers getting caught short .',\n",
       " \"No one wants stock on their books . ''\",\n",
       " 'Meanwhile , the broad - based Financial Times 100 - share index added 30.4 points to end at 2142.6 , while reaching its minimum of 2120.5 a half hour into the session .',\n",
       " 'At the close , the narrower 30 - share index was up 19.7 points to 1721.4 .',\n",
       " 'Volume totaled a modest 334.5 million shares , up from 257.8 million shares Monday .',\n",
       " 'The market also moved at early afternoon on news that Jaguar shares were being temporarily suspended at 746 pence ( $ 11.80 ) each .',\n",
       " 'Secretary of State for Trade and Industry Nicholas Ridley said later in the day that the government would abolish its golden share in Jaguar , the luxury auto maker being stalked by General Motors and Ford Motor .',\n",
       " \"The golden share dates from Jaguar 's public offering in 1984 and was designed to protect the company from takeover .\",\n",
       " 'The golden share was scheduled to expire at the beginning of',\n",
       " \"But although the golden share has been waived , a hostile bidder for Jaguar would still have to alter the British concern 's articles of association which ban shareholdings of more than 15 % .\",\n",
       " 'Jaguar shares closed at 869 pence , up 122 pence , on hefty turnover of 9.7 million shares .',\n",
       " 'As the London trading session drew to a close , the market was still listening to the parliamentary debate on the economy , with new Chancellor of the Exchequer John Major expected to clarify his approach to the British economy and currency issues .',\n",
       " 'On the Frankfurt Stock Exchange , share prices closed higher in fairly thin trading , as selective buying by foreigners helped propel prices .',\n",
       " 'The DAX index closed at 1472.76 , up from 1466.29 .',\n",
       " 'Despite the modest gains , traders said the market remains dull , with investors remaining cautiously on the sidelines .',\n",
       " \"Contributing to the market 's reserved stance was the release later in the day of new data on the health of the U.S. economy , in the form of the U.S. index of leading indicators .\",\n",
       " 'Additionally , the end of the month position - squaring might have also played a minor role , traders said .',\n",
       " 'Elsewhere , share prices closed higher in Amsterdam , Brussels , Milan and Paris .',\n",
       " 'Prices were mixed in Zurich and lower in Stockholm .',\n",
       " 'Stocks closed higher in Hong Kong , Manila , Singapore , Sydney and Wellington , but were lower in Seoul .',\n",
       " 'Taipei was closed for a holiday .',\n",
       " \"Here are price trends on the world 's major stock markets , as calculated by Morgan Stanley Capital International Perspective , Geneva .\",\n",
       " 'To make them directly comparable , each index is based on the close of 1969 equaling 100 .',\n",
       " 'The percentage change is since year - end .',\n",
       " 'French consumer prices rose 0.2 % in September from the previous month and were up 3.4 % from a year earlier , according to definitive figures from the National Statistics Institute .',\n",
       " \"The state agency 's figures confirm previous estimates and leave the index at 178.9 , up from 178.5 in August and 173.1 a year earlier .\",\n",
       " 'The index is based on 1980 equaling 100 .',\n",
       " 'A breakdown showed that food prices were the most active part of growth with a rise of 0.6 % .',\n",
       " 'An official linked the gain essentially to higher prices for beef and pork .',\n",
       " 'He said summer drought problems that had hit several southern agricultural regions had stopped being a major source of price pressure in September .',\n",
       " \"Japan 's index of leading indicators rose to 63.6 in August , above the so - called boom - or - bust line of 50 for the first time since May , the Economic Planning Agency said .\",\n",
       " \"The leading index recovered from July 's revised level of 36.6 on strong performances in consumer durables and machinery orders , among other factors , according to an agency spokeswoman .\",\n",
       " 'The index is intended to measure future economic performance .',\n",
       " 'A figure above 50 indicates the economy is likely to expand ; one below 50 indicates a contraction may be ahead .',\n",
       " 'Metromedia Co. said its Metromedia Long Distance unit has been renamed Metromedia - ITT Long Distance , reflecting acquisitions from ITT Corp. , which licenses its name to closely held Metromedia .',\n",
       " 'Metromedia said its unit is the fifth - largest provider of long - distance communications service in the U.S. , with projected 1989 revenue of more than $ 550 million .',\n",
       " 'Metromedia , headed by John W. Kluge , has interests in telecommunications , robotic painting , computer software , restaurants and entertainment .',\n",
       " \"South Korean consumer prices rose 5 % in the first 10 months of this year , matching the government 's target for the entire year , according to the Bank of Korea and the Economic Planning Board .\",\n",
       " 'According to reports released by the two government agencies , domestic consumer and wholesale prices each rose by 0.2 % in October from the previous month .',\n",
       " 'As a result , consumer prices for the first 10 months of 1989 surged by 5 % and wholesale prices by 1.3 % .',\n",
       " 'The South Korean government had been projecting a 5 % consumer price increase for the entire year .',\n",
       " 'Martin Marietta Corp. said it won a $ 38.2 million contract from the U.S. Postal Service to manufacture and install automated mail - sorting machines .',\n",
       " 'Under terms of the three - year contract , Martin Marietta said it will make and install 267 of the new machines at 156 postal offices .',\n",
       " 'The new machines are capable of sorting by zip code up to 10,000 large flat mail pieces , including magazines and parcels , an hour .',\n",
       " 'Thomas A. Donovan , 37 years old , formerly vice president , West Coast operations , at this hazardous - waste - site remediation concern , was named executive vice president and chief operating officer , both newly created posts , and a director , filling a vacancy .',\n",
       " 'Canonie said it anticipates naming Mr. Donovan to succeed Richard F. Brissette , 55 , as president and chief executive officer , effective March 1 .',\n",
       " 'Mr. Brissette will remain a Canonie board member and will be a consultant to the company .',\n",
       " 'Yields on savings - type certificates of deposit dropped slightly in the week ended yesterday .',\n",
       " 'The average yield on a six - month CD of $ 50,000 or less was 7.90 % , compared with 7.94 % a week earlier .',\n",
       " 'The average one - year savings - type CD was down to 7.99 % from 8.01 % , according to Banxquote Money Markets , a New York information service that tracks CD yields .',\n",
       " \"`` This week was uneventful for the CD market , '' said Norberto Mehl , chairman of Banxquote .\",\n",
       " \"`` The major banks have n't even reacted to sharp rises in the three - month Treasury bill rates '' in the past two weeks .\",\n",
       " 'Banks that adjusted payouts on CDs in the most recent week made only fractional moves , he said .',\n",
       " 'The CD trend runs counter to the direction of short - term interest rates at the Treasury bill auction Monday .',\n",
       " 'The average six - month bill was sold with a yield of 8.04 % , up from 7.90 % .',\n",
       " 'The average three - month issue rose to 8.05 % from 7.77 % .',\n",
       " 'Typically , banks offer CD yields higher than those on Treasury bills , which are considered the safest short - term investments ; banks need a competitive edge to sell their products .',\n",
       " 'But when market interest rates move up rapidly , increases in bank CD yields sometimes lag .',\n",
       " 'Most yields on short - term jumbo CDs , those with denominations over $ 90,000 , also moved in the opposite direction of Treasury bill yields .',\n",
       " 'The average six - month yield on a jumbo CD was at 7.90 % , down from 7.93 % , Banxquote said .',\n",
       " 'For longer - term CDs , yields were up .',\n",
       " 'The average two - year and five - year jumbos were up 0.02 of a percentage point to 7.91 % and 7.96 % , respectively .',\n",
       " 'However , CDs sold through major broker - dealer networks were up slightly almost across the board .',\n",
       " 'The average six - month CD in that category added 0.05 percentage point to 8.35 % , for example .',\n",
       " 'Mr. Mehl attributed the rise specifically to the Treasury bill increase .',\n",
       " 'Among the major banks surveyed by Banxquote in six regions of the country , 8.33 % is the highest yield available .',\n",
       " \"It is offered by the flagship banks of New York 's Manufacturers Hanover Corp. in the one - year maturity only .\",\n",
       " \"The yield is offered across a range of maturities at San Francisco 's BankAmerica Corp. and Wells Fargo & Co .\",\n",
       " \"Just two weeks ago , BankAmerica 's yields in many of those maturities was 8.61 % .\",\n",
       " 'Still , on average , the major California banks have the highest yields on CDs , according to Banxquote .',\n",
       " 'The average yield there on six - month issues is 8.32 % .',\n",
       " 'I had to reach back to French 101 when the monsieur avec clipboard leaned over my shoulder during the coffee phase of dinner and asked whether I wanted to ride in a montgolfiere .',\n",
       " 'I was a last - minute ( read interloping ) attendee at a French journalism convention and so far the festivities had been taken up entirely by eating , drinking , smoking , sleeping and drinking .',\n",
       " 'The man with the clipboard represented a halfhearted attempt to introduce a bit of les sportif into our itinerary .',\n",
       " 'But as the French embody a Zen - like state of blase when it comes to athletics ( try finding a Nautilus machine in Paris ) , my fellow conventioners were having none of it .',\n",
       " \"The diners at my table simply lit more Gauloises and scoffed at the suggestion of interrupting a perfectly good Saturday morning to go golfing or even montgolfing ( ballooning to you ; the brothers Montgolfier , French of course , were the world 's first hot - air balloonists ) .\",\n",
       " 'Back in the U.S.A. this kind of chi - chi airborne activity wins heartwarmingly covetous responses .',\n",
       " 'As in : `` You went ballooning ? ? ! !',\n",
       " \"In France ? ? ! ! ''\",\n",
       " \"Americans it seems have followed Malcolm Forbes 's hot - air lead and taken to ballooning in a heady way .\",\n",
       " 'During the past 25 years , the number of balloonists ( those who have passed a Federal Aviation Authority lighter - than - air test ) have swelled from a couple hundred to several thousand , with some estimates running as high as 10,000 .',\n",
       " \"Some 30 balloon shows are held annually in the U.S. , including the world 's largest convocation of ersatz Phineas Foggs -- the nine - day Albuquerque International Balloon Fiesta that attracts some 800,000 enthusiasts and more than 500 balloons , some of which are fetchingly shaped to resemble Carmen Miranda , Garfield or a 12 - story - high condom .\",\n",
       " '( The condom balloon was denied official entry status this year . )',\n",
       " \"But in Epinal , a gray 16th - century river town adjacent to France 's Vosges mountain region , none of these Yankee - come - lately enthusiasms for things aloft was evident .\",\n",
       " 'Ballooning at the de rigueur hour of 6 a.m. held all the attraction for most people of sunrise root - canal work .',\n",
       " 'Feeling the naggings of a culture imperative , I promptly signed up .',\n",
       " 'The first thing anybody will tell you about ballooning is that it requires zip in the way of athletic prowess , or even a measure of derring - do .',\n",
       " \"( So long as you do n't look down . )\",\n",
       " 'They will also tell you that even if you hate heights , you can still balloon .',\n",
       " \"( I still say do n't look down .\",\n",
       " 'At least not when you are ascending . )',\n",
       " \"What they wo n't tell you is not to go aloft in anything you do n't want to get wet .\",\n",
       " \"I 'm not referring to the traditional champagne drenching during the back - on - terra - firma toast .\",\n",
       " \"I 'm talking about landing in a canal .\",\n",
       " 'In a porous wicker basket .',\n",
       " 'With a pilot who speaks no English .',\n",
       " 'To wit , my maiden voyage ( and novitiates are referred to as virgins ) began at dawn on a dew - sodden fairway and ended at noon in a soggy field .',\n",
       " '( Balloon flights almost always occur at dawn or dusk , when the winds are lightest . )',\n",
       " 'In between came lots of coffee drinking while watching the balloons inflate and lots of standing around deciding who would fly in what balloon and in what order ( the baskets hold no more than four passengers ) .',\n",
       " \"When it was n't my turn in the balloon I followed its progress from the `` chase car , '' listening to the driver holler into a walkie - talkie .\",\n",
       " 'After long stretches of this attendant ground activity came 20 or so lovely minutes of drifting above the Vosges watching the silver mists rise off the river and the French cows amble about the fields .',\n",
       " \"It 's hard not to feel that God 's in his heaven with this kind of bird 's - eye view of the world , even if your pilote in silly plaid beret kept pointing out how `` belle '' it all was .\",\n",
       " 'Eventually little French farmers and their little French farmwives came out of their stone houses and put their hands above their tiny eyes and squinted at us .',\n",
       " 'No wonder .',\n",
       " 'We were coming down straight into their canal .',\n",
       " \"See , the other rule of thumb about ballooning is that you ca n't steer .\",\n",
       " 'And neither can your pilot .',\n",
       " \"You can go only up or down ( by heating the balloon 's air with a propane burner , which does make the top of your head feel hot ) and ride the air currents .\",\n",
       " 'Which makes the chase car necessary .',\n",
       " 'Most balloonists seldom go higher than 2,000 feet and most average a leisurely 5 - 10 miles an hour .',\n",
       " 'When the balloon is cruising along at a steady altitude there is little sense of motion .',\n",
       " 'Only when one is ascending -- or in our case descending a tad trop rapidement -- does one feel , well , airborne in a picnic basket .',\n",
       " \"`` What 's he doing ? '' hissed my companion , who was the only other English - speaking member of the convention and whose knuckles were white .\",\n",
       " \"`` Attention , '' yelled our pilot as our basket plunged into the canal .\",\n",
       " \"`` You bet attention , '' I yelled back , leaping atop the propane tanks , `` I 'm wearing alligator loafers ! ''\",\n",
       " 'Our pilot simply laughed , fired up the burner and with another blast of flame lifted us , oh , a good 12 - inches above the water level .',\n",
       " 'We scuttled along for a few feet before he plunged us into the drink again .',\n",
       " 'Eventually we came to rest in a soggy patch of field where we had the exquisite pleasure of scrambling out of the basket into the mud while the French half of our ballooning tag team scrambled in .',\n",
       " 'I looked at my watch .',\n",
       " 'Barely half - an - hour aloft .',\n",
       " 'Back in the chase car , we drove around some more , got stuck in a ditch , enlisted the aid of a local farmer to get out the trailer hitch and pull us out of the ditch .',\n",
       " 'We finally rendezvoused with our balloon , which had come to rest on a dirt road amid a clutch of Epinalers who watched us disassemble our craft -- another half - an - hour of non-flight activity -- that included the precision routine of yanking the balloon to the ground , punching all the air out of it , rolling it up and cramming it and the basket into the trailer .',\n",
       " \"It was the most exercise we 'd had all morning and it was followed by our driving immediately to the nearest watering hole .\",\n",
       " 'This meant returning to the golf course , where we watched a few French duffers maul the first tee while we sat under Cinzano umbrellas , me nursing an espresso and my ego .',\n",
       " 'A whole morning of ballooning and I had been off the ground barely 30 minutes .',\n",
       " \"Still , I figured the event 's envy - quotient back in the U.S.A. was near peerless .\",\n",
       " 'As for the ride back to camp , our pilot and all the other French - speaking passengers clambered into the chase car .',\n",
       " 'My American companion and I were left to ride alfresco in the wicker basket .',\n",
       " \"As we streaked by a blase gendarme , I could n't resist rearing up on my soggy loafers and saluting .\",\n",
       " 'Ms. de Vries is a free - lance writer .',\n",
       " \"Treasury Undersecretary David Mulford defended the Treasury 's efforts this fall to drive down the value of the dollar , saying it helped minimize damage from the 190 - point drop in the stock market Oct. 13 .\",\n",
       " \"Testifying before a House subcommittee , Mr. Mulford said that if the Treasury had n't intervened in foreign - exchange markets in September and early October to reduce the dollar 's value , the plunge in the stock market might have provoked a steep fall in the currency that might have `` unhinged financial markets . ''\",\n",
       " \"Mr. Mulford , responding to critics of intervention , also said intervention is `` highly visible , '' is taken seriously by financial markets and works better than `` was recognized some time ago . ''\",\n",
       " 'Differences between the Treasury and the Federal Reserve on the usefulness of intervention to help restrain the dollar resurfaced at the hearing .',\n",
       " \"Fed Vice Chairman Manuel Johnson , who had dissented from the Treasury 's policy , told lawmakers , `` I became convinced about what looked to me like an attempt to push the dollar down against the fundamentals in the market . ''\",\n",
       " \"Intervention , he added , is useful only to smooth disorderly markets , not to fundamentally influence the dollar 's value .\",\n",
       " 'Rep. John LaFalce ( D. , N.Y . ) said Mr. Johnson refused to testify jointly with Mr. Mulford and instead asked to appear after the Treasury official had completed his testimony .',\n",
       " \"A Fed spokesman denied Mr. LaFalce 's statement .\",\n",
       " \"Mr. Mulford said reports of tension between the Treasury and Fed have been exaggerated , insisting that they involved `` nuances . ''\",\n",
       " \"Mr. Johnson also said that `` in the scheme of things , these things are minor . ''\",\n",
       " 'On other matters , Mr. Mulford said West Germany is contributing to imbalances in the world economy because of its success as an exporter .',\n",
       " \"`` The solution is stronger domestic growth { in Germany } , '' he said .\",\n",
       " \"But because the growth of the German economy has been stronger than expected , Mr. Mulford said , it 's difficult for the U.S. to argue that Germany ought to adopt more stimulative monetary and fiscal policies .\",\n",
       " \"Germany 's trade surplus is largely with other European countries rather than with the U.S. , Mr. Mulford acknowledged .\",\n",
       " 'But nonetheless U.S. companies might be more successful in European markets if not for the German export push , he said .',\n",
       " 'Five officials of this investment banking firm were elected directors : E. Garrett Bewkes III , a 38 - year - old managing director in the mergers and acquisitions department ; Michael R. Dabney , 44 , a managing director who directs the principal activities group which provides funding for leveraged acquisitions ; Richard Harriton , 53 , a general partner who heads the correspondent clearing services ; Michael Minikes , 46 , a general partner who is treasurer ; and William J. Montgoris , 42 , a general partner who is also senior vice president of finance and chief financial officer .',\n",
       " 'The board increased by one to 26 members .',\n",
       " 'In the past year , one inside director resigned , while three others retired .',\n",
       " 'Some U.S. allies are complaining that President Bush is pushing conventional - arms talks too quickly , creating a risk that negotiators will make errors that could affect the security of Western Europe for years .',\n",
       " 'Concerns about the pace of the Vienna talks -- which are aimed at the destruction of some 100,000 weapons , as well as major reductions and realignments of troops in central Europe -- also are being registered at the Pentagon .',\n",
       " 'Mr. Bush has called for an agreement by next September at the latest .',\n",
       " 'But some American defense officials believe the North Atlantic Treaty Organization should take more time to examine the long - term implications of the options being considered .',\n",
       " 'For one thing , Pentagon officials , who asked not to be identified , worry that the U.S. will have a much tougher time persuading Europeans to keep some short - range nuclear weapons on their soil once Soviet armored forces are thinned out .',\n",
       " 'At the same time , they contend that a reduction of NATO forces under a treaty will increase the possibility of a conventional Soviet attack unless the West retains a residual force of nuclear weapons in Europe .',\n",
       " \"Allies concerned about the deadline include the British , French and smaller NATO allies , some of whom do n't have adequate staffs to provide quick answers to the questions being raised by what generally are considered the most complex arms - control talks ever attempted .\",\n",
       " \"So far , no ally has complained openly , preserving the impression that NATO is in line with the Bush position that a quick agreement bringing Soviet conventional forces down to parity with NATO is the West 's top bargaining priority .\",\n",
       " \"But even though NATO negotiators have only 10 months left under the Bush timetable , they are still wrestling over such seemingly fundamental questions as `` What is a tank ? ''\",\n",
       " \"Five of the six categories of weapons under negotiation have n't even been defined .\",\n",
       " 'Tanks currently are defined as armored vehicles weighing 25 tons or more that carry large guns .',\n",
       " 'The Soviets complicated the issue by offering to include light tanks , which are as light as 10 tons .',\n",
       " 'Oleg A. Grinevsky , the chief Soviet negotiator in the conventional - arms talks , argued that this would mean the Soviets would have to destroy some 1,800 tanks , while the U.S. would lose none because it has no light tanks in Europe .',\n",
       " 'But the issue is stickier than it seems .',\n",
       " 'France , Britain and Italy all have light tanks they would like to keep out of the talks .',\n",
       " 'And some U.S. Army analysts worry that the proposed Soviet redefinition is aimed at blocking the U.S. from developing lighter , more transportable , high - technology tanks .',\n",
       " 'Defining combat aircraft is even tougher .',\n",
       " \"The Soviets insisted that aircraft be brought into the talks , then argued for exempting some 4,000 Russian planes because they are `` solely defensive . ''\",\n",
       " \"NATO has n't budged from its insistence that any gun - carrying plane has offensive capability .\",\n",
       " \"The dispute over that issue , according to one U.S. official , is a `` potential treaty stopper , '' and only President Bush and Soviet leader Mikhail Gorbachev may be able to resolve it .\",\n",
       " 'Accounting problems raise more knotty issues .',\n",
       " 'Greece and Turkey , for example , are suspected of overstating their arsenals in hopes that they can emerge from the arms - reduction treaty with large remaining forces to deter each other .',\n",
       " \"Other nations are n't sure how many weapons they have in their own arsenals .\",\n",
       " \"`` It 's just going to be sloppy , both on our side and theirs { the Warsaw Pact 's } , '' says one NATO analyst .\",\n",
       " 'So far , neither the Bush administration nor arms - control experts in Congress seem moved by arguments that these problems may take more time to thrash out than President Bush has allowed .',\n",
       " 'They argue that the bigger danger would be that the West would delay action so long that the Soviets might back away from the current conciliatory attitude .',\n",
       " \"`` So what if you miss 50 tanks somewhere ? '' asks Rep. Norman Dicks ( D. , Wash. ) , a member of the House group that visited the talks in Vienna .\",\n",
       " '`` The bottom line is that if we can get that { Warsaw Pact } superiority brought down to parity , we ought to keep pressing ahead as quickly as possible .',\n",
       " 'I worry more about things becoming so unraveled on the other side that they might become unable to negotiate .',\n",
       " 'International Lease Finance Corp. announced a leasing contract with charter carrier American Trans Air Inc. , in a transaction involving six Boeing Co. 757 - 200s .',\n",
       " 'The value of the jets , including spares , is in excess of $ 250 million .',\n",
       " 'Two of the 757 - 200s are new aircraft to be delivered to American Trans Air , the main subsidiary of Amtran Inc. , in December 1991 and January 1992 .',\n",
       " 'Four of the planes were purchased by International Lease from Singapore Airlines in a previously announced transaction .',\n",
       " 'Delivery of the first aircraft is set for early November , a second for December and two for April 1990 .',\n",
       " \"Norway 's unemployment rate for October was 3.6 % , unchanged from September but up from 2.6 % in the same month last year .\",\n",
       " 'The figure excludes a record number employed by extraordinary government work programs , the Labor Directorate announced Tuesday .',\n",
       " \"Including those in the state programs , there were 143,800 Norwegians , or about 6.5 % of the work force , without permanent employment in October , up from September 's 136,800 .\",\n",
       " 'The number of people registered as jobless at the end of October declined by 900 from September to 78,600 .',\n",
       " 'Those employed in state - funded special programs increased by 7,400 to 65,200 in the same period , the Directorate said .',\n",
       " 'In October 1988 , there were 40,800 fewer employed by government programs .',\n",
       " 'Coca - Cola Co. , aiming to boost soft - drink volume in Singapore , said it is discussing a joint venture with Fraser & Neave Ltd. , its bottling franchisee in that country .',\n",
       " \"The venture would be the latest in Coke 's rapid expansion of overseas investment .\",\n",
       " 'So far this year , it has put nearly $ 700 million into bottling operations in Australia , New Zealand and France .',\n",
       " \"The move also reflects Coke 's eagerness to have a hand in developing the soft - drink markets in Pacific Basin countries .\",\n",
       " 'Aside from Europe , the Pacific division is where Coke will be focusing much of its attention for years to come .',\n",
       " \"That 's because when Coke looks to the Pacific area , it sees an economic and demographic gold mine .\",\n",
       " 'In countries such as Taiwan , South Korea and Singapore , economies are growing , resulting in a rise in disposable income that consumers can use for soft drinks .',\n",
       " 'And unlike Europe and the U.S. , where populations are aging , the Pacific Basin countries have growing proportions of youths -- the heaviest consumers of Coca - Cola and other sodas .',\n",
       " 'A Coca - Cola spokesman said it is too early to say how the joint venture would be structured , or how much the company would invest in the transaction .',\n",
       " 'In the past , however , Coke has typically taken a minority stake in such ventures .',\n",
       " \"By acquiring stakes in bottling companies in the U.S. and overseas , Coke has been able to improve bottlers ' efficiency and production , and in some cases , marketing .\",\n",
       " 'Coke has tended to increase its control when results were sluggish in a given country .',\n",
       " \"That does n't appear to be the case in Singapore , a country of about three million people with a relatively high soft - drink consumption rate -- a key indicator of Coke 's success in a market .\",\n",
       " 'In Singapore , per - capita consumption is about one - third that of the U.S. .',\n",
       " \"And combining Fraser & Neave 's own soft drinks with Coca - Cola 's gives the Singapore company more than half the share of the soda market there , Coke said .\",\n",
       " \"Fraser & Neave , which also has interests in packaging , beer and dairy products , holds the Coke licenses for Malaysia and Brunei , where per - capita consumption is n't as high as in Singapore .\",\n",
       " 'Coke could be interested in more quickly developing some of the untapped potential in those markets .',\n",
       " \"A Coke spokesman said he could n't say whether that is the direction of the talks .\",\n",
       " \"Coke said the joint - venture arrangement , which needs approval from both companies ' boards , should be completed early next year .\",\n",
       " 'AMERICAN BRANDS Inc. , Old Greenwich , Conn. , said it increased its quarterly 11 % to 68 cents a share from 61 cents , payable Dec. 1 to stock of record Nov. 10 .',\n",
       " \"The increase follows the company 's report of strong earnings for the third quarter , and reflects what American Brands called its `` tradition of sharing earnings growth '' with shareholders .\",\n",
       " 'American Brands is a consumer products company with core businesses in tobacco , distilled spirits and life insurance .',\n",
       " 'As of Sept. 30 , American Brands had 95.2 million shares outstanding .',\n",
       " 'Giovanni Agnelli & Co. announced a transaction that will strengthen its indirect control of Fiat S.p . A. and will admit Prince Karim Aga Khan as its first non-family shareholder .',\n",
       " \"Giovanni Agnelli , a limited partnership that is the master holding company for Fiat 's Agnelli family , owns approximately 75 % of the shares in Istituto Finanziario Industriale , which in turn owns approximately 40 % of Fiat , Italy 's biggest private - sector industrial group .\",\n",
       " 'The company said Maria Sole Agnelli Teodorani , sister of Fiat Chairman Giovanni Agnelli , agreed to trade her shares in IFI for new ordinary shares in the limited partnership , which will give her control of 4.67 % of Giovanni Agnelli & Co .',\n",
       " \"The Aga Khan , meanwhile , agreed to trade some of his stake in Luxembourg - based Ifint S.A. , another Agnelli family company , for 7.45 % of Giovanni Agnelli & Co. 's capital .\",\n",
       " 'His new stake would be in the form of preferred shares , which receive higher dividends but have voting rights only in extraordinary shareholders assemblies .',\n",
       " \"The Aga Khan owns 10 % of Ifint 's capital , while IFI owns 23 % .\",\n",
       " \"As a result of the transaction , which is expected to be approved at a shareholders meeting Nov. 24 , Giovanni Agnelli & Co. will control 79.18 % of IFI 's ordinary shares .\",\n",
       " 'Its capital will also be raised to 232.4 billion lire ( $ 172.5 million ) from the current 204.3 billion lire .',\n",
       " 'IFI also has nonvoting preferred shares , which are quoted on the Milan stock exchange .',\n",
       " \"The value of the two transactions was n't disclosed , but an IFI spokesman said no cash would change hands .\",\n",
       " \"The move strengthens the existing links between the Agnellis and the Aga Khan , the head of the world 's Ismaili Moslems who is a longtime family friend and frequently goes sailing with Mr. Agnelli .\",\n",
       " \"Mr. Agnelli and the Aga Khan also have some business ties , and a spokesman for the Agnelli company did n't rule out that the current agreement could lead to further collaboration .\",\n",
       " \"For instance , Ifint earlier this year bought an 18 % stake in Alisarda , the Aga Khan 's airline , which flies between Italy and Sardinia .\",\n",
       " \"Giovanni Agnelli & Co. , which was formed in January 1987 as a way of keeping the Agnellis ' controlling stake in Fiat together despite an ever - growing family tree , has been playing a more active role in the Agnelli group of late .\",\n",
       " 'It raised financing of 300 billion lire for the purchase this summer by another Agnelli - related group of the food concern Galbani S.p . A. , by selling a chunk of its IFI shares to Mediobanca S.p . A.',\n",
       " 'Mediobanca said during the weekend that it agreed to sell the shares back to Giovanni Agnelli for 333 billion lire .',\n",
       " \"Your Oct. 2 page - one article on people riding so - called `` railbikes '' on railroad tracks was a disservice to your readers .\",\n",
       " 'It unfortunately encourages others to engage in a highly dangerous and illegal activity that only a very few are doing now .',\n",
       " 'And it treats such activities in a frivolous , cavalier fashion , with total indifference to common sense and public safety .',\n",
       " 'Saul Resnick',\n",
       " 'Vice President',\n",
       " 'Public Affairs',\n",
       " 'Conrail',\n",
       " 'MCI Communications Corp. said it received a three - year contract valued at more than $ 15 million to provide network , credit - card and other telecommunications services to Drexel Burnham Lambert Inc .',\n",
       " 'Congressional Democrats and the Bush administration agreed on a compromise minimum - wage bill , opening the way for the first wage - floor boost in more than nine years .',\n",
       " 'The agreement ended a long impasse between the congressional leaders and the White House over the wage issue .',\n",
       " \"President Bush in June vetoed a measure passed by Congress and said he would n't accept any minimum - wage rise that went beyond limits he set early in this year 's debate on the issue .\",\n",
       " 'The compromise was a somewhat softened version of what the White House had said it would accept .',\n",
       " 'Under the agreement with the House and Senate leaders , the minimum wage would rise from the current $ 3.35 an hour to $ 4.25 an hour by April 1991 .',\n",
       " \"Employers could also pay a subminimum `` training wage '' for 90 days to new workers who are up to 19 years old , and then for another 90 days if the company institutes a specific training program for the newcomers .\",\n",
       " 'White House officials were delighted that the compromise includes the concept of a training wage , which Mr. Bush has fought for throughout the year .',\n",
       " \"`` For the first time in history , we have a training wage that will be part '' of the nation 's labor laws , said Roger Porter , assistant to the president for economic and domestic policy .\",\n",
       " 'White House aides said that although they made a small compromise on the length of a training wage , the final minimum - wage increase will meet the standards set by Mr. Bush .',\n",
       " \"The bill vetoed by the president in June , which the House failed to override , would have lifted the minimum wage to $ 4.55 an hour by late 1991 , with a training wage for up to two months , generally for a worker 's first job .\",\n",
       " 'Mr. Bush had been holding out for a bill boosting the wage floor to $ 4.25 an hour by the end of 1991 , coupled with a six - month training wage for workers newly hired by any employer .',\n",
       " 'Under the compromise , the $ 4.25 level would be reached nine months earlier , while the training subminimum would be shorter , unless it is tied to a training plan .',\n",
       " 'Democrats argued that the training wage was a way of allowing employers to pay less than the minimum wage , while new workers need far less than six months to be trained for their jobs .',\n",
       " 'Democrats had been negotiating with some Republican congressional leaders on a compromise lately .',\n",
       " 'With congressional elections next year , GOP leaders have worried about opposing a minimum - wage rise for low - paid workers at a time when Congress is moving toward a capital - gains tax cut that would directly benefit wealthier taxpayers .',\n",
       " 'Republicans have been imploring the White House to compromise on the wage issue .',\n",
       " 'In the Senate , Edward Kennedy ( D. , Mass. ) , chairman of the Labor Committee , and Pete Domenici , ( R. , N.M . ) ranking minority member of the Budget Committee , have been working on a compromise , and their soundings showed that the Senate appeared to be heading toward enough strength to override another Bush veto , a Democratic staff official said .',\n",
       " \"The House is scheduled to vote this week on the compromise , as a substitute to a new Democratic bill , itself watered down from last spring 's version .\",\n",
       " 'The Senate will probably vote not long afterward .',\n",
       " 'Some Democrats thought they might have compromised too much .',\n",
       " \"Rep. Austin Murphy ( D. , Pa. ) , chairman of the House labor standards subcommittee , said they might have done better `` if we 'd held their feet to the fire . ''\",\n",
       " \"Mr. Kennedy suggested Democrats `` yielded a great deal '' on the size of the increase , but he cited concessions from the White House on the training wage , which he said make it `` less harsh . ''\",\n",
       " \"With only 16 - year - olds to 19 - year - olds eligible , 68 % of workers getting less than $ 4.25 an hour , who are adults , wo n't be subject to the training wage , he said .\",\n",
       " \"The AFL - CIO , which previously opposed the administration 's subminimum idea , said the compromise has `` adequate safeguards , so the youth are not exploited and older workers are not displaced . ''\",\n",
       " 'Gerald F. Seib contributed to this article .',\n",
       " \"Moody 's Investors Service Inc. said it lowered the ratings on about $ 3.2 billion of Houston Lighting & Power Co. 's securities because of the company 's low levels of interest coverage and internal cash generation .\",\n",
       " 'Houston Lighting is a unit of Houston Industries Inc. , a utility holding company in Houston .',\n",
       " \"Downgraded by Moody 's were Houston Lighting 's first - mortgage bonds and secured pollution - control bonds to single - A - 3 from single - A - 2 ; unsecured pollution - control bonds to Baa - 1 from single - A - 3 ; preferred stock to single - A - 3 from single - A - 2 ; a shelf registration for preferred stock to a preliminary rating of single - A - 3 from a preliminary rating of single - A - 2 ; two shelf registrations for collateralized debt securities to a preliminary rating of single - A - 3 from a preliminary rating of single - A - 2 , and the unit 's rating for commercial paper to Prime - 2 from Prime - 1 .\",\n",
       " \"Moody 's said Houston Lighting 's current situation has some positive aspects , including managing `` very well '' the construction and commercial operation risks of Units 1 and 2 of the South Texas Project nuclear power plant .\",\n",
       " \"Capital requirements will be declining and no new generating facilities will be required for several years , Moody 's said .\",\n",
       " 'Scott C. Smith , formerly vice president , finance , and chief financial officer of this media concern , was named senior vice president .',\n",
       " 'Mr. Smith , 39 , retains the title of chief financial officer .',\n",
       " 'Armstrong World Industries Inc. agreed in principle to sell its carpet operations to Shaw Industries Inc .',\n",
       " \"The price was n't disclosed but one analyst estimated that it was $ 150 million .\",\n",
       " \"Armstrong , which has faced a takeover threat from the Belzberg family of Canada since July , said that disposing of the carpet business would improve `` total financial performance . ''\",\n",
       " 'The move also would allow the company to concentrate on core businesses , which include ceramic tile , floor coverings and furniture .',\n",
       " 'Moreover , such a sale could help Armstrong reassure its investors and deter the Belzbergs , who own a 9.85 % stake in the Lancaster , Pa. , company .',\n",
       " 'Analysts expect Armstrong to use proceeds of the sale to reduce debt , buy back stock or perhaps finance an acquisition .',\n",
       " \"The carpet division had 1988 sales of $ 368.3 million , or almost 14 % of Armstrong 's $ 2.68 billion total revenue .\",\n",
       " 'The company has been manufacturing carpet since 1967 .',\n",
       " 'Recently it upgraded its plants so that it could make stain - resistant products with higher quality dyes .',\n",
       " \"For the past year or two , the carpet division 's operating profit margins have hovered around 5 % , high by industry standards , but disappointing compared with the 13 % to 19 % margins for two of Armstrong 's chief businesses , flooring and building products .\",\n",
       " 'Analysts hailed the planned transaction as being beneficial to Armstrong and Shaw , the market leader in the U.S. carpet industry , with an estimated 17 % to 20 % share .',\n",
       " \"Shaw , based in Dalton , Ga. , has annual sales of about $ 1.18 billion , and has economies of scale and lower raw - material costs that are expected to boost the profitability of Armstrong 's brands , sold under the Armstrong and Evans - Black names .\",\n",
       " \"Yesterday , in composite trading on the New York Stock Exchange , Shaw 's shares closed ex-dividend at $ 26.125 , up $ 2.25 .\",\n",
       " \"Armstrong 's shares , also listed on the Big Board , closed at $ 39.125 , up 12.5 cents .\",\n",
       " 'Yesterday , Armstrong reported flat earnings for the third quarter and nine months , worsened by the stock dilution of an employee stock ownership plan adopted earlier this year .',\n",
       " 'For the quarter , earnings were $ 47 million , or 92 cents a share , including a one - time gain of $ 5.9 million .',\n",
       " 'In the year - ago quarter , earnings were $ 42.9 million , or 93 cents a share .',\n",
       " 'Yesterday , Armstrong announced an agreement to sell its small Applied Color Systems unit to a subsidiary of the Swiss company , Brauerei Eichof Ltd .',\n",
       " \"The price was n't disclosed .\",\n",
       " 'Armstrong expects to close the sale of the color unit in late November and the carpet sale in December , with the gains to be applied to fourth quarter or first - quarter results .',\n",
       " \"The government 's primary economic - forecasting gauge rose a slight 0.2 % in September , but economists said the report offered little new information on the degree to which the U.S. economy is slowing .\",\n",
       " 'The small increase in the index of leading indicators , which had climbed 0.5 % in August but was unchanged in July , does lend support to the view that the economy has slowed noticeably .',\n",
       " \"However , it does n't give much of a clue as to whether a recession is on the horizon .\",\n",
       " \"`` I do n't think it provides much new information on the economy , '' said Richard Rippe , economist at Dean Witter Reynolds Inc .\",\n",
       " 'So far this year , the index of leading indicators has risen in four months , fallen in four months and remained unchanged in the other month .',\n",
       " 'In another report yesterday , the Commerce Department said sales of new single - family houses plunged 14 % in September to an annual rate of 618,000 from 719,000 in August .',\n",
       " 'The declines were particularly pronounced in the Northeast and in the South , where Hurricane Hugo was a factor .',\n",
       " \"Although September 's weakness followed two strong months for home sales , the decline supports other indications that the drop in mortgage rates earlier this year has had only a limited beneficial effect on the housing market .\",\n",
       " 'The September drop was the largest since a 19 % drop in January 1982 , but monthly changes in this measure are even less reliable than those in other economic indicators .',\n",
       " 'Because the figures are based on a small sample , the department said it is 90 % confident only that new - home sales fell somewhere between 5 % and 23 % during the month .',\n",
       " 'The department also said it takes four months to establish a trend .',\n",
       " 'So far this year , 534,000 newly built homes have been sold , down 4.5 % from the like months of 1988 .',\n",
       " 'The index of leading indicators got a major boost in September from a surge in consumer expectations as measured by the University of Michigan .',\n",
       " 'This measure had dropped sharply in August .',\n",
       " 'The Commerce Department said that as a result of a new adjustment to the formula used to calculate the index , the influence of this component has been reduced .',\n",
       " 'Of the 11 components to the index , only three others rose in September : the money supply , the length of the average work week and stock prices .',\n",
       " 'Several components that track the health of the manufacturing sector of the economy turned down in September .',\n",
       " 'These include new orders for manufactured consumer goods , lead times on vendor deliveries , orders for new plant and equipment , and backlogs of orders for durable goods .',\n",
       " \"Meanwhile , the National Association of Manufacturers said yesterday a recent poll of 53 executives on its board found that 61 % do n't expect a recession to occur until 1991 or later .\",\n",
       " 'The remainder expect a downturn to begin sometime in',\n",
       " 'Although manufacturers often are quick to call for lower interest rates , 60 % of the executives said they would prefer that the Fed keep inflation - fighting as its top priority even if that means higher rates .',\n",
       " 'The other 40 % said the Fed ought to worry less about inflation and bring interest rates down .',\n",
       " 'All the figures are adjusted to remove usual seasonal patterns .',\n",
       " \"Here are the net contributions of the components of the Commerce Department 's index of leading indicators .\",\n",
       " 'After various adjustments , they produced a 0.5 % rise in the index for August and a 0.2 % rise for September .',\n",
       " 'September , and the change from August , are : from 1.11 in the previous month .',\n",
       " 'Boston Edison Co. said it will take a previously reported $ 60 million charge against earnings in the fourth quarter .',\n",
       " 'The charge resulted from a settlement approved yesterday by the Massachusetts Department of Public Utilities .',\n",
       " \"As expected , the settlement limits rate increases for three years and ties future charges to customers for operation of the troubled Pilgrim Nuclear Power Station to that plant 's performance .\",\n",
       " \"In its order , the state regulatory agency said the company `` must be held accountable for the mistakes made in the management of the plant 's operation . ''\",\n",
       " 'Pilgrim had been closed for 32 months .',\n",
       " \"The average interest rate rose to 8.3875 % at Citicorp 's $ 50 million weekly auction of 91 - day commercial paper , or corporate IOUs , from 8.337 % at last week 's sale .\",\n",
       " 'Bids totaling $ 515 million were submitted .',\n",
       " 'Accepted bids ranged from 8.38 % to 8.395 % .',\n",
       " \"Citicorp also said that the average rate rose to 8.0087 % at its $ 50 million auction of 182 - day commercial paper from 7.962 % at last week 's sale .\",\n",
       " 'Bids totaling $ 475 million were submitted .',\n",
       " 'Accepted bids ranged from 8 % to 8.019 % .',\n",
       " 'The bank holding company will auction another $ 50 million in each maturity next Tuesday .',\n",
       " \"An imaginative novelist writing a thriller about amateur spy - chasing might invent a Clifford Stoll , but it 's unlikely .\",\n",
       " \"It 's also unnecessary .\",\n",
       " 'Amateur spy - chaser Clifford Stoll is a real person , or as he might waggishly put it , a surreal person .',\n",
       " 'He is 37 , an astronomer with impressive credentials , and something of a genius at making computers do his bidding .',\n",
       " \"He once described himself as a `` Berkeley Hippie , '' and played the role well ; obligatory ragged jeans , a thicket of long hair and rejection of all things conventional , including , for a time at least , formal marriage to his `` sweetheart , '' Martha Matthews .\",\n",
       " 'He also is an entertaining writer , combining wisecracks and wordplay with programmatic detail and lucid explanations of how computers work .',\n",
       " \"In `` The Cuckoo 's Egg '' ( Doubleday , 326 pages , $ 19.95 ) , he spins a remarkable tale of his efforts over 18 months to catch a computer spy .\",\n",
       " 'The result last spring was the arrest by West German authorities of five young West Germans , accused of stealing information from computers in the U.S. and Europe and selling it to the Soviet KGB .',\n",
       " 'One of them , 25 - year - old Markus Hess of Hannover , allegedly used the international telecommunications network to break into more than 30 high - security computers in the U.S. , searching for secrets .',\n",
       " \"He probably did n't penetrate any top - secret files , but the KGB in East Berlin was willing to pay two of his associates , Peter Carl and Dirk Brezinski , $ 15,000 for some of the material Hess collected .\",\n",
       " 'They promised yet more for really good stuff .',\n",
       " \"Mr. Stoll draws his title from the cuckoo 's habit of laying eggs in the nests of other birds , making them surrogate parents .\",\n",
       " 'The computer spy had discovered that a popular editing / electronic mail program called Gnu - Emacs could do tricks with the widely used Unix operating system created by AT&T .',\n",
       " \"Using Gnu - Emacs , the spy could substitute a bogus `` atrun '' program for the one that routinely cleans up the Unix system every five minutes .\",\n",
       " \"Once his cuckoo 's egg was laid , he could enter Unix and become a `` super-user , '' with access to everything .\",\n",
       " \"Mr. Stoll was scanning the heavens at the Keck observatory of the Lawrence Berkeley Laboratory in 1986 when his grant ran low and he was asked to switch to helping run the lab 's computers .\",\n",
       " \"He discovered a 75 - cent discrepancy in the charges made to various departments for computer time and traced it to a user named `` Hunter , '' who had no valid billing address .\",\n",
       " 'Mr. Stoll suspected the intruder was one of those precocious students who has fun breaking into computers .',\n",
       " 'But after much tracking , it became evident to Mr. Stoll , through various clues , that the hacker was not on the Berkeley campus or even in California .',\n",
       " 'Finding him became an obsession for Mr. Stoll .',\n",
       " \"He made a midnight requisition of all the printers he could lay hands on so that he could monitor all the telephone lines coming into the lab 's computers .\",\n",
       " 'After discovering that the hacker had taken over the dormant account of a legitimate user named Joe Sventek , he rigged up an alarm system , including a portable beeper , to alert him when Sventek came on the line .',\n",
       " 'Some nights he slept under his desk .',\n",
       " 'His boss complained about neglect of other chores .',\n",
       " 'The hacker was pawing over the Berkeley files but also using Berkeley and other easily accessible computers as stepping stones to the network of computers used by the military and national security agencies .',\n",
       " 'The White Sands missile range and CIA contractor Mitre Inc. were among the targets .',\n",
       " 'When the hacker moved , Mr. Stoll moved too , calling up other systems managers to alert them but keeping his own system open to avoid arousing suspicion .',\n",
       " 'Sometimes , if the hacker seemed to be into a sensitive file , he would drag his keychain across the terminal to create static or slow the system down to frustrate his quarry .',\n",
       " 'The FBI initially showed little interest , and he had the impression other federal security agencies were tangled up in legal red tape .',\n",
       " 'The CIA told him it does not do domestic counterespionage .',\n",
       " 'One learns a lot from this book , or seems to , about crippling federal bureaucracy .',\n",
       " \"`` Seems to '' because it 's possible that the CIA and the National Security Agency were more interested than they let on to Mr. Stoll .\",\n",
       " 'Finally , he got help .',\n",
       " 'Tymnet is a major network linking computers .',\n",
       " \"One of its international specialists , Steve White , took a quick interest in Mr. Stoll 's hunt , ultimately tracing the hacker to West Germany .\",\n",
       " 'The West Germans then took over and finally found Markus Hess .',\n",
       " 'Eventually , Mr. Stoll was invited to both the CIA and NSA to brief high - ranking officers on computer theft .',\n",
       " 'He savored the humor of his uncombed appearance among these buttoned - up chaps .',\n",
       " 'Back in Berkeley , he was violently scolded by a left - wing lady friend for consorting with such people .',\n",
       " 'He became angry in return .',\n",
       " \"He had developed a hatred for the hacker and a grudging appreciation of the federal `` spooks '' who make national security their business .\",\n",
       " \"At several different levels , it 's a fascinating tale .\",\n",
       " 'Mr. Melloan is deputy editor of the Journal .',\n",
       " 'Mips Computer Systems Inc. today will unveil a new general - purpose computer that will compete with more expensive machines from companies such as Sun Microsystems Inc. and Digital Equipment Corp .',\n",
       " 'The closely held Sunnyvale , Calif. , company also will announce an agreement to supply computers to Control Data Corp. , which will sell Mips machines under its own label .',\n",
       " 'The new Mips machine , called the RC6280 , will cost $ 150,000 for a basic system .',\n",
       " 'The computer processes 55 million instructions per second and uses only one central processing chip , unlike many rival machines using several processors .',\n",
       " 'The machine employs reduced instruction - set computing , or RISC , technology .',\n",
       " 'At that price , an analyst familiar with the machine said , the computer offers up to 10 times the performance of similar machines .',\n",
       " \"`` In the price range it 's a tremendously high - performing product , '' said Sandy Gant , an analyst at the market - research firm InfoCorp .\",\n",
       " 'The machine is part of an effort by Mips to establish itself as a supplier of computers , not just of integrated - circuit technology .',\n",
       " 'Mips also wants to wedge into markets other than traditional RISC applications such as engineering ; Mips said the new machine will also be used by businesses and for communications .',\n",
       " \"`` This clearly demonstrates that Mips is a systems company rather than just a chip company , '' said Mips Vice President John Hime .\",\n",
       " 'The Control Data deal is a boon for Mips because it gives the the five - year - old company one more ally as it battles more established electronic concerns such as Sun , Hewlett - Packard Co. , Motorola Inc. and Intel Corp. for the emerging market for RISC machines .',\n",
       " 'RISC technology speeds up a computer by simplifying the internal software .',\n",
       " \"For Mips , which expects revenue of $ 100 million this year , big - name allies such as Control Data are essential to attract software developers to the company 's RISC architecture .\",\n",
       " \"`` The thing it says about Mips is that they 're on a roll right now , '' said Ms. Gant at InfoCorp .\",\n",
       " \"`` They 're getting some major wins , '' she added .\",\n",
       " \"Last month , for example , Mips agreed to supply its computers to Nixdorf Computer AG of West Germany and France 's Groupe Bull .\",\n",
       " \"Sony Corp. , Tandem Computers Inc. and Digital Equipment have agreed to sell MIPS computers and companies such as Japan 's NEC Corp. and West Germany 's Siemens A.G. have agreed to make Mips chips under license .\",\n",
       " \"Today 's agreement gives Control Data a machine to compete against Digital and other general - purpose computer makers , said John Logan , a computer - market analyst at Aberdeen Group Inc. of Boston .\",\n",
       " 'The machine is essentially a mainframe computer , he said .',\n",
       " \"`` Suddenly CDC ( Control Data ) has a competitive product to fight back against the VAX9000 , '' a machine Digital announced last month , he added .\",\n",
       " 'Control Data , based in Minneapolis , Minn. , expects its sales of Mips systems , including the new RC6280 , to amount to more than $ 100 million by the end of 1991 , Mips said .',\n",
       " 'Nixdorf , Bull and others will also sell versions of the machine , said Mips President Robert Miller .',\n",
       " 'Mips will start shipping its new machine in the first quarter of 1990 , he said .',\n",
       " 'The machine uses a single processor , which makes it easier to program than competing machines using several processors .',\n",
       " 'The computer can process 13.3 million calculations called floating - point operations every second .',\n",
       " 'The machine can run software written for other Mips computers , the company said .',\n",
       " 'Another fight is brewing between Congress and the Bush administration over how to pay for the savings - and - loan bailout without adding to the federal budget deficit .',\n",
       " \"In a hearing before the House Ways and Means Committee , the General Accounting Office and the Congressional Budget Office , which both are arms of Congress , advised the new S&L bailout agency to abandon plans to raise temporary working capital through debt issued from an agency that would n't be counted on the federal budget .\",\n",
       " 'Officials of the Resolution Trust Corp. have said privately that such a plan was the most likely alternative to raise short - term cash for the bailout .',\n",
       " 'Instead , the GAO and the Congressional Budget Office said , the RTC should consider using Treasury debt , which is less expensive and subject to oversight by Congress .',\n",
       " 'The spending could be exempted from meeting deficit - reduction targets in the Gramm - Rudman budget law .',\n",
       " 'The RTC has projected that it will require between $ 50 billion to $ 100 billion in temporary working capital .',\n",
       " 'The borrowing to raise these funds would be paid off as assets of sick thrifts are sold .',\n",
       " 'The new S&L law allows the RTC to issue notes for as much as 85 % of the value of the assets it holds .',\n",
       " \"But higher interest rates paid on off - budget debt could add billions to the bailout costs , and would n't be subject to congressional scrutiny , Ways and Means members argued .\",\n",
       " \"`` To allow this massive level of unfettered federal borrowing without prior congressional approval would be irresponsible , '' said Rep. Fortney Stark ( D. , Calif. ) , who has introduced a bill to limit the RTC 's authority to issue debt .\",\n",
       " 'The RTC will have to sell or merge hundreds of insolvent thrifts over the next three years .',\n",
       " 'The new S&L bailout law allows $ 50 billion to be spent to sell or merge sick S&Ls and their assets , but that is a net cost .',\n",
       " 'In the meantime , the agency must raise cash to maintain assets , such as real estate , until they can be sold .',\n",
       " 'Then the short - term debt is paid off through the proceeds of selling the assets .',\n",
       " 'David Mullins , assistant secretary of the Treasury , said that the working capital is necessary to reduce the final costs of the bailout , by allowing the agency to sell savings and loans without their bad assets , then hold the assets until they can be sold under favorable conditions .',\n",
       " \"He said it has n't yet been determined how the RTC will raise the cash , but the administration does n't want it to be included on the federal budget , because it would `` distort '' the budget process by requiring either exemptions from Gramm - Rudman or big increases in the budget deficit .\",\n",
       " 'But the worst possibility would be raising no working capital , he said .',\n",
       " \"`` If working capital financing is not provided , '' he said , `` the RTC may have to slow { S&L sales } or dump acquired assets through fire sales .\",\n",
       " 'Panhandle Eastern Corp. said it applied , on behalf of two of its subsidiaries , to the Federal Energy Regulatory Commission for permission to build a 352 - mile , $ 273 million pipeline system from Pittsburg County , Okla. , to Independence , Miss .',\n",
       " 'The natural gas pipeline concern said the 500 million cubic feet a day capacity pipeline would be built by a proposed joint venture between two Panhandle Eastern units - Texas Eastern Transmission Corp. and Trunkline Gas Co .',\n",
       " 'Texas Eastern Transmission will build and operate the system , which will connect the Arkoma Basin with several interstate pipelines .',\n",
       " 'Now was that a quarter cup or a half cup ?',\n",
       " \"Not a gripping question , unless you 're the pastry chef of this city 's Chez Panisse restaurant and you 've just lost your priceless personal dessert notebook .\",\n",
       " 'Chez Panisse was listed among the top 30 restaurants in the world this year by Connoisseur magazine .',\n",
       " \"The tattered black binder , bulging with 18 years ' worth of recipes held together by rubber bands , was in chef Lindsey Shere 's purse when it was stolen from her house recently .\",\n",
       " \"The Berkeley police do n't have any leads but doubt the crime was driven by a passion for sweets .\",\n",
       " \"Instead , they figure the culprit probably took money from Ms. Shere 's wallet and discarded all the tips in the five - by - eight - inch looseleaf .\",\n",
       " \"Chez Panisse , whose founder , Alice Waters , is considered the inventor of the cooking style known as California cuisine and whose patrons make reservations a month in advance , has n't exactly subjected diners to vanilla ice cream because of the theft .\",\n",
       " \"For one thing , Ms. Shere can draw on her cookbook , published by Random House four years ago , which is teeming with recipes for such specialties as kiwi sherbet , gooseberry fool ( a creamy dish made with crushed stewed berries ) and hazelnut `` oeufs a la neige . ''\",\n",
       " 'For another , sympathetic fans have sent Ms. Shere copies of her recipes clipped from magazines over the years .',\n",
       " \"Still , the restaurant 's ever - changing menu of five - course dinners -- it supposedly has n't repeated a meal since opening in 1971 -- requires constant improvisation .\",\n",
       " 'And that puts added pressure on Chez Panisse dessert - menu planners .',\n",
       " \"`` We make what we know how to make , '' says business manager Richard Mazzera .\",\n",
       " \"Many in the Bay Area 's pastry community express disbelief that Ms. Shere kept only one copy of such valuable notes , but she has received moral support from Baker 's Dozen , a group of California pastry chefs that meets regularly to discuss issues like how to keep meringues from weeping and how bovine eating habits affect butter texture .\",\n",
       " \"Ms. Shere has offered a $ 500 reward for the book 's return but figures she 'll have to reinvent many recipes from scratch .\",\n",
       " \"`` It 's an overwhelming job , '' she says .\",\n",
       " '`` There are so many possible proportions when you consider how many things are made out of eggs and butter and milk .',\n",
       " \"Newport Electronics Inc. named a new slate of officers , a move that follows replacement of the company 's five incumbent directors last week .\",\n",
       " 'Milton B. Hollander , 60 years old , was named chief executive officer , succeeding Barrett B. Weekes .',\n",
       " \"Mr. Hollander 's Stamford , Conn. - based High Technology Holding Co. acquired most of its 49.4 % stake in Newport in August .\",\n",
       " 'Mr. Hollander was named chairman last week , succeeding Mr. Weekes , who was among the ousted directors .',\n",
       " 'The company has declined requests to discuss the changes , but Mr. Weekes has said that Mr. Hollander wanted to have his own team .',\n",
       " 'Scott Wakeman was named president and chief operating officer of U.S. operations , titles that had been held by Mr. Weekes .',\n",
       " 'Mr. Wakeman was vice president of the instrument and controls division of closely held Omega Engineering Inc. , another company controlled by Mr. Hollander .',\n",
       " \"A company spokesman did n't know Mr. Wakeman 's age .\",\n",
       " \"James R. Lees , 51 , vice president of Newport 's European operations , was named executive vice president and chief operating officer of European operations , assuming some former duties of Mr. Weekes .\",\n",
       " 'Arthur B. Crozier , 34 , an attorney , was named secretary , succeeding John Virtue , who was another of the ousted directors .',\n",
       " 'UNIFIRST Corp. declared a 2 - for - 1 stock split .',\n",
       " 'The Wilmington , Mass. , garment service company also boosted its quarterly dividend 20 % to three cents a share adjusted for the split .',\n",
       " 'The dividend had been five cents a share .',\n",
       " 'The split and quarterly dividend will be payable Jan. 3 to stock of record Nov. 16 , the company said .',\n",
       " 'The split will raise the number of shares outstanding to about 10.2 million .',\n",
       " 'Separately , UniFirst reported that net income rose 21 % to $ 3 million , or 29 cents a share adjusted for the split , for the fourth quarter ended Aug. 26 .',\n",
       " 'A year earlier UniFirst earned $ 2.4 million , or 24 cents a share adjusted for the split .',\n",
       " 'Sales rose to $ 52.4 million from $ 50.1 million .',\n",
       " 'Fibreboard Corp. said it completed the previously reported sale of approximately 27,500 acres of timberland near Truckee , Calif. , to closely held Sierra Pacific Industries Corp. , Arcata , Calif. , for $ 32.5 million .',\n",
       " 'The lumber , insulation and fireproofing concern said the transaction , which includes a swap of other timber interests , would result in a $ 13.5 million after - tax gain , to be recorded in the fourth quarter .',\n",
       " 'Healthcare International Inc. said it reached a 120 - day standstill agreement with its HealthVest affiliate calling for Healthcare to pay HealthVest $ 5 million right away and additional amounts in the future .',\n",
       " 'Under the agreement , Healthcare , a manager of health - care facilities , said it would pay HealthVest $ 3.9 million in overdue rent and mortgage payments and repay $ 1.1 million in funds that HealthVest advanced for construction work on facilities .',\n",
       " \"In return , HealthVest agreed that it wo n't exercise its rights and remedies against Healthcare during the 120 - day period .\",\n",
       " 'After the payment , Healthcare still will be $ 6.5 million in arrears on rent and mortgage payments to HealthVest , a real estate investment trust whose portfolio consists largely of properties operated by Healthcare .',\n",
       " 'Healthcare has given HealthVest a 12 % note for that overdue amount , to be repaid over three years .',\n",
       " 'In addition , Healthcare agreed to make monthly rent and mortgage payments of $ 2.7 million to $ 3 million to HealthVest during the standstill period , to be paid when Healthcare successfully completes asset sales .',\n",
       " 'Because Healthcare actually owes HealthVest $ 4.2 million in rent and mortgage payments each month , the amount due above the amount paid will be added to the three - year note .',\n",
       " \"The funds should help ease a cash bind at HealthVest , which has been unable to pay its debts because Healthcare has n't made complete rent and mortgage payments since July .\",\n",
       " 'A spokesman said HealthVest has paid two of the three banks it owed interest to in October and is in negotiations with the third bank .',\n",
       " 'Healthcare , which has been in a severe liquidity bind , said it is able to make the payments because it completed a transaction with Greenery Rehabilitation Group Inc. in which Greenery purchased stock and warrants for $ 500,000 and loaned Healthcare $ 9 million .',\n",
       " \"The loan is backed by Healthcare 's 5.4 % stake in HealthVest and interest in certain facilities .\",\n",
       " 'I was pleased to note that your Oct. 23 Centennial Journal item recognized the money - fund concept as one of the significant events of the past century .',\n",
       " 'Actually , about two years ago , the Journal listed the creation of the money fund as one of the 10 most significant events in the world of finance in the 20th century .',\n",
       " \"But the Reserve Fund , America 's first money fund , was not named , nor were the creators of the money - fund concept , Harry Brown and myself .\",\n",
       " 'We innovated telephone redemptions , daily dividends , total elimination of share certificates and the constant $ 1 pershare pricing , all of which were painfully thought out and not the result of some inadvertence on the part of the SEC .',\n",
       " 'President',\n",
       " 'The Reserve Fund',\n",
       " \"The crowning moment in the career of Joseph F. O'Kicki came as 300 local and state dignitaries packed into his elegant , marble - columned courtroom here last year for his swearing in as President Judge of Cambria County .\",\n",
       " 'Baskets of roses and potted palms adorned his bench .',\n",
       " 'The local American Legion color guard led the way .',\n",
       " 'As the judge marched down the center aisle in his flowing black robe , he was heralded by a trumpet fanfare .',\n",
       " 'To many , it was a ceremony more befitting a king than a rural judge seated in the isolated foothills of the southern Allegheny Mountains .',\n",
       " \"But then Judge O'Kicki often behaved like a man who would be king -- and , some say , an arrogant and abusive one .\",\n",
       " 'While his case may be extreme , it reflects the vulnerability of many small communities to domineering judges .',\n",
       " \"Last March , nine months after the judge 's swearing - in , the state attorney general 's office indicted him on a sweeping array of charges alleging more than 10 years of `` official oppression '' in Cambria County , a depressed steel and mining community in western Pennsylvania .\",\n",
       " 'The allegations , ranging from theft and bribery to coercion and lewdness , paint a disquieting picture .',\n",
       " \"According to testimony in a public , 80 - page grand - jury report handed up to the state attorney general , Judge O'Kicki extorted cash from lawyers , muscled favorable loans from banks and bullied local businesses for more than a decade .\",\n",
       " \"Prosecutors , in an indictment based on the grand jury 's report , maintain that at various times since 1975 , he owned a secret and illegal interest in a beer distributorship ; plotted hidden ownership interests in real estate that presented an alleged conflict of interest ; set up a dummy corporation to buy a car and obtain insurance for his former girlfriend ( now his second wife ) ; and maintained 54 accounts in six banks in Cambria County .\",\n",
       " 'In testimony recorded in the grand jury report , court employees said the judge , now 59 years old , harassed his secretaries , made imperial demands on his staff and hounded anyone who crossed him .',\n",
       " 'Bailiffs claimed they were required to chauffeur him to and from work , mow his lawn , chop his wood , fix his car and even drop by his house to feed his two grown mutts , Dixie and Husky .',\n",
       " 'One former bailiff charged that the judge double - crossed him by reneging on a promise of a better paying job after pocketing a $ 500 bribe .',\n",
       " 'Some of the allegations are simply bizarre .',\n",
       " \"Two former secretaries told the grand jury they were summoned to the judge 's chambers on separate occasions to take dictation , only to find the judge in his bikini underwear .\",\n",
       " 'One secretary testified that the judge once called her to his office while wearing nothing at all .',\n",
       " \"The judge , suspended from his bench pending his trial , which began this week , vehemently denies all the allegations against him , calling them `` ludicrous '' and `` imaginative , political demagoguery . ''\",\n",
       " \"He blames the indictment on local political feuding , unhappiness with his aggressive efforts to clear the courthouse 's docket and a vendetta by state investigators and prosecutors angered by some of his rulings against them .\",\n",
       " \"`` I do n't know whose toes I 've stepped on , '' says the judge .\",\n",
       " \"`` I 'll find out , eventually , who pushed the state police buttons into action . ''\",\n",
       " 'Even if only some of the allegations stand up , however , they provide ample testimony to the awesome power of judges in rural communities .',\n",
       " 'That power can sometimes be abused , particularly since jurists in smaller jurisdictions operate without many of the restraints that serve as corrective measures in urban areas .',\n",
       " 'Lawyers and their clients who frequently bring business to a country courthouse can expect to appear before the same judge year after year .',\n",
       " 'Fear of alienating that judge is pervasive , says Maurice Geiger , founder and director of the Rural Justice Center in Montpelier , Vt. , a public interest group that researches rural justice issues .',\n",
       " \"As a result , says Mr. Geiger , lawyers think twice before appealing a judge 's ruling , are reluctant to mount , or even support , challenges against him for re-election and are usually loath to file complaints that might impugn a judge 's integrity .\",\n",
       " \"Judge O'Kicki , a stern and forbidding - looking man , has been a fixture in the local legal community for more than two decades .\",\n",
       " 'The son of an immigrant stonemason of Slovenian descent , he was raised in a small borough outside Ebensburg , the Cambria County seat , and put himself through the University of Pittsburgh Law School .',\n",
       " 'He graduated near the top of his class , serving on the school law review with Richard Thornburgh , who went on to become governor of Pennsylvania and , now , U.S. Attorney General .',\n",
       " \"It was also in law school that Mr. O'Kicki and his first wife had the first of seven daughters .\",\n",
       " 'He divorced his first wife three years ago and married the daughter of his court clerk .',\n",
       " \"Last year , Pennsylvania Supreme Court Justice John P. Flaherty called Mr. O'Kicki one of the finest judges `` not only in Pennsylvania but in the United States . ''\",\n",
       " 'Clearly , the judge has had his share of accomplishments .',\n",
       " 'After practicing law locally , he was elected to his first 10 - year term as judge in 1971 ; in 1981 , he was effectively re-elected .',\n",
       " \"Six years ago , Judge O'Kicki was voted president of the Pennsylvania Conference of State Trial Judges by the state 's 400 judges .\",\n",
       " 'He has been considered several times for appointments to federal district and appellate court vacancies in Pennsylvania .',\n",
       " \"And when he ran unsuccessfully for a state appellate court seat in 1983 , the Pennsylvania Bar Association rated him `` one of the best available , '' after interviewing local lawyers .\",\n",
       " \"`` He probably was the smartest guy who ever sat on our bench , '' says a former president of Cambria County 's 150 - member bar association , who , like most lawyers in Cambria County , refuses to talk about the judge publicly .\",\n",
       " \"`` He 's sharp as a tack .\",\n",
       " \"He could grasp an issue with the blink of an eye . ''\",\n",
       " \"For more than a decade , virtually no one complained about Judge O'Kicki .\",\n",
       " \"`` What about those institutions that are supposed to be the bedrock of society , the banks and the bar association ... ? '' wrote a columnist for the Tribune - Democrat , a newspaper in nearby Johnstown , shortly after the scandal became public .\",\n",
       " \"`` If only a banker or a lawyer had spoken out years ago , the judicial process would n't be under the taint it is today . ''\",\n",
       " 'Officials with the Pennsylvania Judicial Inquiry and Review Board , the arm of the state that investigates judicial misconduct , counter that they had no inkling of anything amiss in Ebensburg .',\n",
       " \"`` Nobody told us ; nobody called us , '' says an official close to the case who asked not to be named .\",\n",
       " \"`` Nobody had the guts to complain . ''\",\n",
       " 'Certainly not the lawyers .',\n",
       " 'Johnstown attorney Richard J. Green Jr. shelled out $ 500 in loans to the judge over five years , he said in testimony to the grand jury .',\n",
       " \"`` The judge never made a pretense of repaying the money , '' said Mr. Green .\",\n",
       " 'Eventually , Mr. Green testified , he began ducking out of his office rather than face the judge when he visited .',\n",
       " \"When Mr. Green won a $ 240,000 verdict in a land condemnation case against the state in June 1983 , he says Judge O'Kicki unexpectedly awarded him an additional $ 100,000 .\",\n",
       " 'Mr. Green thought little of it , he told the grand jury , until the judge walked up to him after the courtroom had cleared and suggested a kickback .',\n",
       " \"`` Do n't you think I ought to get a commission ... or part of your fee in this case ? '' Mr. Green said the judge asked him .\",\n",
       " 'Appalled , Mr. Green never paid the money , he testified .',\n",
       " \"But he did n't complain to the state 's Judicial Inquiry and Review Board , either , saying later that he feared retribution .\",\n",
       " \"Mr. O'Kicki said he will respond to Mr. Green 's allegation at his trial .\",\n",
       " \"Like most of Cambria County 's lawyers and residents who had dealings with the judge , Mr. Green declined to be interviewed for this article .\",\n",
       " 'And no one with a complaint about the judge would allow his name to be printed .',\n",
       " \"`` I do n't have anything much to say , and I think that 's what you 're going to find from everyone else you talk to up here , '' says local attorney Edward F. Peduzzi .\",\n",
       " \"Says another lawyer : `` The practice of law is a matter of biting one 's lip when you live in a small community .\",\n",
       " \"One had best not dance on top of a coffin until the lid is sealed tightly shut . ''\",\n",
       " 'The judge was considered imperious , abrasive and ambitious , those who practiced before him say .',\n",
       " 'He sipped tea sweetened with honey from his high - backed leather chair at his bench , while scribbling notes ordering spectators to stop whispering or to take off their hats in his courtroom .',\n",
       " \"Four years ago , he jailed all nine members of the Cambria County School Board for several hours after they defied his order to extend the school year by several weeks to make up for time lost during a teachers ' strike .\",\n",
       " \"Visitors in his chambers say he could cite precisely the years , months , weeks and days remaining until mandatory retirement would force aside the presiding president judge , giving Judge O'Kicki the seniority required to take over as the county 's top court administrator .\",\n",
       " 'The judge , they say , was fiercely proud of his abilities and accomplishments .',\n",
       " \"`` My name is judge , '' Judge O'Kicki told a car salesman in Ebensburg when he bought a new red Pontiac Sunbird in October 1984 , according to the grand - jury report .\",\n",
       " \"The dealership dutifully recorded the sale under the name `` Judge O'Kicki . ''\",\n",
       " \"Yet , despite the judge 's imperial bearing , no one ever had reason to suspect possible wrongdoing , says John Bognato , president of Cambria County 's 150 - member bar association .\",\n",
       " \"`` The arrogance of a judge , his demeanor , the way he handles people are not a basis for filing a complaint , '' says Mr. Bognato .\",\n",
       " \"`` Until this came up and hit the press , there was never any indication that he was doing anything wrong . ''\",\n",
       " \"State investigators dispute that view now , particularly in light of the judge 's various business dealings in Cambria County .\",\n",
       " \"The judge came under scrutiny in late 1987 , after the state attorney general 's office launched an unrelated investigation into corruption in Cambria County .\",\n",
       " 'The inquiry soon focused on the judge .',\n",
       " 'Even his routine business transactions caused trouble , according to the grand jury report .',\n",
       " \"When the judge bought his new Sunbird from James E. Black Pontiac - Cadillac in Ebensburg five years ago , the dealership had `` certain apprehensions '' about the judge 's reputation , according to the grand - jury report .\",\n",
       " \"The dealership took the extra step of having all the paper work for the transaction pre-approved by Ebensburg 's local lender , Laurel Bank .\",\n",
       " \"Then , as an additional precaution , the car dealership took the judge 's photograph as he stood next to his new car with sales papers in hand -- proof that he had received the loan documents .\",\n",
       " 'But when the judge received his payment book , he disavowed the deal .',\n",
       " \"`` There was no loan , there is no loan , there never shall be a loan , '' the judge wrote the bank on his judicial stationery , according to the report .\",\n",
       " 'Later , the judge went a step farther .',\n",
       " 'After Laurel Bank tried to repossess the car , a vice president asked him to intervene in an unrelated legal dispute involving a trust account .',\n",
       " 'The judge wrote again .',\n",
       " \"`` I find myself in an adversary relationship with Laurel Bank , and I am not inclined to extend myself as far as any favors are concerned , '' the judge wrote back in a letter attached to the grand jury 's report .\",\n",
       " \"`` Perhaps if my personal matters can be resolved with Laurel bank in the near future , I may be inclined to reconsider your request ... . ''\",\n",
       " \"The judge now says it was `` unfortunate '' that he chose to write the letter but says `` there was certainly no intent to extort there . ''\",\n",
       " 'The bank acquiesced .',\n",
       " \"It refinanced the judge 's loan , lowered its interest rate and accepted a trade - in that had n't originally been part of the deal -- a beat up 1981 Chevy Citation the dealer had to repair before it could be resold .\",\n",
       " \"The incident was n't the only time the judge got special treatment from his local bank .\",\n",
       " \"Two years later , he wrote to complain that the interest he was paying on an unsecured $ 10,000 loan was `` absolutely onerous . ''\",\n",
       " \"Paul L. Kane , Laurel 's president at the time , quickly responded .\",\n",
       " \"The bank , he wrote back , was `` immediately '' lowering the rate by 3.5 % , `` as a concession to you . ''\",\n",
       " \"The judge says he ca n't discuss in detail how he will defend himself at his trial , although he contends that if he were as corrupt as state prosecutors believe , he would be far wealthier than he is .\",\n",
       " 'His seven - bedroom cedar and brick house outside of Johnstown is up for sale to pay for his lawyers .',\n",
       " 'The judge says he is confident he will return to his old bench .',\n",
       " 'Already , he notes , the 76 charges originally filed against him have been trimmed to 27 .',\n",
       " 'Most of the allegations no longer pending were ethics charges withdrawn by state prosecutors as part of a pre-trial agreement .',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join(tree.leaf_labels()) for tree in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 15339, 1917], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "hidden_states = outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"Hello what's up\"], padding=True, return_tensors=\"pt\")\n",
    "seq_length = inputs.input_ids.shape[1]\n",
    "input_ids = inputs.input_ids.repeat(seq_length, 1)\n",
    "# masked_input_ids = torch.cat(\n",
    "#             (\n",
    "#                 input_ids[:, :-1],\n",
    "#                 torch.ones(seq_length, 4, dtype=int),\n",
    "#                 input_ids[:, None, -1],\n",
    "#             ),\n",
    "#             dim=-1,\n",
    "#         )\n",
    "masked_input_ids = input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_mask = torch.triu(torch.ones(seq_length, seq_length + 4, dtype=bool))\n",
    "mask_token_mask[torch.arange(seq_length - 1), -1] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264,     2],\n",
       "        [    0, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264,     2],\n",
       "        [    0, 31414, 50264, 50264, 50264, 50264, 50264, 50264, 50264,     2],\n",
       "        [    0, 31414,    99, 50264, 50264, 50264, 50264, 50264, 50264,     2],\n",
       "        [    0, 31414,    99,    18, 50264, 50264, 50264, 50264, 50264,     2],\n",
       "        [    0, 31414,    99,    18,    62, 50264, 50264, 50264, 50264, 50264]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_input_ids[mask_token_mask] = tokenizer.mask_token_id\n",
    "masked_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[35.6104, -3.9388, 23.6486,  ...,  2.4729,  4.9608, 12.6227],\n",
       "         [ 1.7506, -4.2102, 17.5566,  ..., -2.6690,  1.2127,  4.6466],\n",
       "         [ 1.6603, -4.1409, 11.9915,  ..., -1.3049,  1.6145,  3.0707],\n",
       "         [ 5.1464, -3.0260, 11.8240,  ...,  2.9685,  1.7550,  6.0786],\n",
       "         [-2.1542, -3.6670, 12.1562,  ..., -0.9428, -0.3162,  0.7386],\n",
       "         [15.1374, -5.2574, 25.3391,  ..., -2.4480, -0.2756,  7.2451]],\n",
       "\n",
       "        [[35.6104, -3.9388, 23.6486,  ...,  2.4729,  4.9608, 12.6227],\n",
       "         [ 1.7506, -4.2102, 17.5566,  ..., -2.6690,  1.2127,  4.6466],\n",
       "         [ 1.6603, -4.1409, 11.9915,  ..., -1.3049,  1.6145,  3.0707],\n",
       "         [ 5.1464, -3.0260, 11.8240,  ...,  2.9685,  1.7550,  6.0786],\n",
       "         [-2.1542, -3.6670, 12.1562,  ..., -0.9428, -0.3162,  0.7386],\n",
       "         [15.1374, -5.2574, 25.3391,  ..., -2.4480, -0.2756,  7.2451]],\n",
       "\n",
       "        [[35.6104, -3.9388, 23.6486,  ...,  2.4729,  4.9608, 12.6227],\n",
       "         [ 1.7506, -4.2102, 17.5566,  ..., -2.6690,  1.2127,  4.6466],\n",
       "         [ 1.6603, -4.1409, 11.9915,  ..., -1.3049,  1.6145,  3.0707],\n",
       "         [ 5.1464, -3.0260, 11.8240,  ...,  2.9685,  1.7550,  6.0786],\n",
       "         [-2.1542, -3.6670, 12.1562,  ..., -0.9428, -0.3162,  0.7386],\n",
       "         [15.1374, -5.2574, 25.3391,  ..., -2.4480, -0.2756,  7.2451]],\n",
       "\n",
       "        [[35.6104, -3.9388, 23.6486,  ...,  2.4729,  4.9608, 12.6227],\n",
       "         [ 1.7506, -4.2102, 17.5566,  ..., -2.6690,  1.2127,  4.6466],\n",
       "         [ 1.6603, -4.1409, 11.9915,  ..., -1.3049,  1.6145,  3.0707],\n",
       "         [ 5.1464, -3.0260, 11.8240,  ...,  2.9685,  1.7550,  6.0786],\n",
       "         [-2.1542, -3.6670, 12.1562,  ..., -0.9428, -0.3162,  0.7386],\n",
       "         [15.1374, -5.2574, 25.3391,  ..., -2.4480, -0.2756,  7.2451]],\n",
       "\n",
       "        [[35.6104, -3.9388, 23.6486,  ...,  2.4729,  4.9608, 12.6227],\n",
       "         [ 1.7506, -4.2102, 17.5566,  ..., -2.6690,  1.2127,  4.6466],\n",
       "         [ 1.6603, -4.1409, 11.9915,  ..., -1.3049,  1.6145,  3.0707],\n",
       "         [ 5.1464, -3.0260, 11.8240,  ...,  2.9685,  1.7550,  6.0786],\n",
       "         [-2.1542, -3.6670, 12.1562,  ..., -0.9428, -0.3162,  0.7386],\n",
       "         [15.1374, -5.2574, 25.3391,  ..., -2.4480, -0.2756,  7.2451]],\n",
       "\n",
       "        [[35.6104, -3.9388, 23.6486,  ...,  2.4729,  4.9608, 12.6227],\n",
       "         [ 1.7506, -4.2102, 17.5566,  ..., -2.6690,  1.2127,  4.6466],\n",
       "         [ 1.6603, -4.1409, 11.9915,  ..., -1.3049,  1.6145,  3.0707],\n",
       "         [ 5.1464, -3.0260, 11.8240,  ...,  2.9685,  1.7550,  6.0786],\n",
       "         [-2.1542, -3.6670, 12.1562,  ..., -0.9428, -0.3162,  0.7386],\n",
       "         [15.1374, -5.2574, 25.3391,  ..., -2.4480, -0.2756,  7.2451]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(masked_input_ids, attention_mask=torch.ones_like(masked_input_ids))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6, 50265])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.6104, 27.5954, 29.2997, 32.0987, 33.3061, 25.3391],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits[torch.arange(seq_length), torch.arange(seq_length), inputs.input_ids.squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "torch.gather(output.logits, -1, inputs.input_ids.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(\"meta-llama/Llama-2-7b-hf\", token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer(\"A very long sentence of many words\", return_tensors=\"pt\")\n",
    "input_ids = output.input_ids\n",
    "attention_mask = output.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,  250,  182,  251, 3645,    9,  171, 1617,    2],\n",
       "        [   0,  250,  182,  251, 3645,    9,  171, 1617,    2],\n",
       "        [   0,  250,  182,  251, 3645,    9,  171, 1617,    2],\n",
       "        [   0,  250,  182,  251, 3645,    9,  171, 1617,    2],\n",
       "        [   0,  250,  182,  251, 3645,    9,  171, 1617,    2],\n",
       "        [   0,  250,  182,  251, 3645,    9,  171, 1617,    2],\n",
       "        [   0,  250,  182,  251, 3645,    9,  171, 1617,    2],\n",
       "        [   0,  250,  182,  251, 3645,    9,  171, 1617,    2],\n",
       "        [   0,  250,  182,  251, 3645,    9,  171, 1617,    2]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = input_ids.shape[1]\n",
    "input_ids.repeat(seq_length, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[11587,     1,     0],\n",
       "        [ 1499,  1499,     1]]), 'attention_mask': tensor([[1, 1, 0],\n",
       "        [1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"batch\", \"text text\"], return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_name_or_path\": \"google-t5/t5-small\",\n",
       "  \"architectures\": [\n",
       "    \"T5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dense_act_fn\": \"relu\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": false,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"n_positions\": 512,\n",
       "  \"num_decoder_layers\": 6,\n",
       "  \"num_heads\": 8,\n",
       "  \"num_layers\": 6,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 200,\n",
       "      \"min_length\": 30,\n",
       "      \"no_repeat_ngram_size\": 3,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"summarize: \"\n",
       "    },\n",
       "    \"translation_en_to_de\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to German: \"\n",
       "    },\n",
       "    \"translation_en_to_fr\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to French: \"\n",
       "    },\n",
       "    \"translation_en_to_ro\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to Romanian: \"\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.41.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, T5Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = T5Model.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "\n",
    "# preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.\n",
    "# This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.\n",
    "decoder_input_ids = model._shift_right(decoder_input_ids)\n",
    "\n",
    "# forward pass\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=torch.ones((1, 3), dtype=int), output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 13.0625,   0.2598, -25.8750,  ..., -25.8750,  45.0000,  23.8750],\n",
       "          [ 16.5000, -13.5000, -23.6250,  ...,   8.5000,   0.2422,  -0.3301],\n",
       "          [-20.3750, -19.5000, -34.0000,  ..., -12.1875,   4.2812,  20.2500],\n",
       "          ...,\n",
       "          [  2.2344,   6.5312,   4.4688,  ...,  13.8125,  -7.5625,   6.5625],\n",
       "          [  6.3125,   7.1250, -24.8750,  ...,   7.6250, -16.6250,  24.7500],\n",
       "          [ 12.6250,   8.1875, -11.6250,  ...,   7.9375,  -7.3125,   0.9453]]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[[ 32.7210,   2.8327, -25.1375,  ..., -21.5066,  63.5949,  33.0159],\n",
       "          [ 18.1687,  10.2138, -46.7164,  ...,  32.7391,  22.4018,  -4.1388],\n",
       "          [ -8.4976, -13.9855, -51.2794,  ...,  -8.7808,  19.4788,  38.9848],\n",
       "          ...,\n",
       "          [ 22.1683,  14.1350,  -2.6477,  ...,   1.3842, -21.5731,  13.5346],\n",
       "          [ -3.8981,   6.0911, -39.3743,  ...,  33.0119, -76.2840,   9.9618],\n",
       "          [ 17.9603,   9.2646, -19.4358,  ...,   5.6072,  -3.0411,  10.6635]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 81.5224,  -9.1201, -32.1771,  ..., -32.2351,  81.7622,  50.3877],\n",
       "          [ 28.1803,  -2.5127, -55.8584,  ...,  49.9840,  11.4867,  27.8667],\n",
       "          [ -6.4261, -11.8284, -58.5355,  ...,   5.1135,  29.6083,  48.4143],\n",
       "          ...,\n",
       "          [ 12.0234,   3.6681, -16.1927,  ...,  10.9624, -40.3441,  15.1812],\n",
       "          [-23.9321,  13.3955, -48.0801,  ...,  60.2107, -83.0028,  17.7861],\n",
       "          [ 15.6339,   8.7929, -21.0993,  ...,   7.0064,  -3.7004,   7.3364]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 91.8965,  11.4654,  -9.2929,  ..., -47.3338,  74.6664,  97.4193],\n",
       "          [ 21.8351,  14.2140, -51.7854,  ...,  71.3156,  10.4096,  59.1427],\n",
       "          [ -5.8344, -14.6716, -76.1706,  ...,  16.2951,  29.4663, 104.8080],\n",
       "          ...,\n",
       "          [ 24.8870,   2.4663,  -5.6490,  ...,   3.3846, -38.4658,   7.2604],\n",
       "          [-33.8449,  -1.8400, -62.9084,  ...,  77.6525, -92.4891,   7.0433],\n",
       "          [  8.0684,   8.7401, -26.0018,  ...,  -0.8513,  12.0340,   6.8173]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 123.9960,   25.9537,    2.6629,  ...,  -59.6953,   91.0901,\n",
       "            111.0369],\n",
       "          [  43.5087,   20.0943,  -57.7684,  ...,   92.7878,   17.4061,\n",
       "             55.9168],\n",
       "          [  11.1409,  -38.2107, -100.5782,  ...,   27.6366,   68.1999,\n",
       "            172.4704],\n",
       "          ...,\n",
       "          [  24.5166,    1.6983,   -7.1084,  ...,   -9.2612,  -73.7525,\n",
       "            -18.5941],\n",
       "          [ -11.6609,    4.6362,  -46.5462,  ...,   79.4954,  -88.6351,\n",
       "             -9.8011],\n",
       "          [  24.9712,   45.2907,  -32.8963,  ...,   -9.7202,   20.4880,\n",
       "             -4.6775]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 217.1234,   37.6286,   -6.7415,  ...,  -72.8824,   84.4733,\n",
       "            109.6581],\n",
       "          [ 109.3001,  -17.9584,  -58.3608,  ...,  108.0717,    8.6459,\n",
       "             45.4246],\n",
       "          [  72.3040,    4.4164, -106.6130,  ...,   45.0551,   46.4229,\n",
       "            197.6886],\n",
       "          ...,\n",
       "          [ -12.2162,  -74.2518,  -28.8717,  ...,  -36.5524, -121.2156,\n",
       "            -26.5364],\n",
       "          [ -34.0481,  -53.6108,  -57.2103,  ...,   91.8958, -136.5668,\n",
       "            -45.0778],\n",
       "          [  35.5076,   65.1443,  -38.9931,  ...,  -26.7318,   31.0284,\n",
       "            -18.6469]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.2828,  0.0979,  0.1090,  ..., -0.2794,  0.2154,  0.0370],\n",
       "          [ 0.1593,  0.0956, -0.0922,  ...,  0.1854,  0.0993,  0.1548],\n",
       "          [ 0.0076,  0.0090, -0.0092,  ...,  0.0050,  0.0019,  0.0076],\n",
       "          ...,\n",
       "          [-0.0119, -0.1228, -0.0638,  ..., -0.0602, -0.1791, -0.0499],\n",
       "          [-0.0342, -0.0511, -0.1488,  ...,  0.3007, -0.2612, -0.1452],\n",
       "          [ 0.0670,  0.1300, -0.0356,  ..., -0.0100,  0.0690, -0.0531]]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_hidden_states = outputs.hidden_states\n",
    "orig_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 13.0625,   0.2598, -25.8750,  ..., -25.8750,  45.0000,  23.8750],\n",
       "          [ 16.5000, -13.5000, -23.6250,  ...,   8.5000,   0.2422,  -0.3301],\n",
       "          [-20.3750, -19.5000, -34.0000,  ..., -12.1875,   4.2812,  20.2500],\n",
       "          ...,\n",
       "          [  2.2344,   6.5312,   4.4688,  ...,  13.8125,  -7.5625,   6.5625],\n",
       "          [  6.3125,   7.1250, -24.8750,  ...,   7.6250, -16.6250,  24.7500],\n",
       "          [ 12.6250,   8.1875, -11.6250,  ...,   7.9375,  -7.3125,   0.9453]]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[[ 32.7210,   2.8327, -25.1375,  ..., -21.5066,  63.5949,  33.0159],\n",
       "          [ 18.1687,  10.2138, -46.7164,  ...,  32.7391,  22.4018,  -4.1388],\n",
       "          [ -8.4976, -13.9855, -51.2794,  ...,  -8.7808,  19.4788,  38.9848],\n",
       "          ...,\n",
       "          [ 22.1683,  14.1350,  -2.6477,  ...,   1.3842, -21.5731,  13.5346],\n",
       "          [ -3.8981,   6.0911, -39.3743,  ...,  33.0119, -76.2840,   9.9618],\n",
       "          [ 17.9603,   9.2646, -19.4358,  ...,   5.6072,  -3.0411,  10.6635]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 81.5224,  -9.1201, -32.1771,  ..., -32.2351,  81.7622,  50.3877],\n",
       "          [ 28.1803,  -2.5127, -55.8584,  ...,  49.9840,  11.4867,  27.8667],\n",
       "          [ -6.4261, -11.8284, -58.5355,  ...,   5.1135,  29.6083,  48.4143],\n",
       "          ...,\n",
       "          [ 12.0234,   3.6681, -16.1927,  ...,  10.9624, -40.3441,  15.1812],\n",
       "          [-23.9321,  13.3955, -48.0801,  ...,  60.2107, -83.0028,  17.7861],\n",
       "          [ 15.6339,   8.7929, -21.0993,  ...,   7.0064,  -3.7004,   7.3364]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 91.8965,  11.4654,  -9.2929,  ..., -47.3338,  74.6664,  97.4193],\n",
       "          [ 21.8351,  14.2140, -51.7854,  ...,  71.3156,  10.4096,  59.1427],\n",
       "          [ -5.8344, -14.6716, -76.1706,  ...,  16.2951,  29.4663, 104.8080],\n",
       "          ...,\n",
       "          [ 24.8870,   2.4663,  -5.6490,  ...,   3.3846, -38.4658,   7.2604],\n",
       "          [-33.8449,  -1.8400, -62.9084,  ...,  77.6525, -92.4891,   7.0433],\n",
       "          [  8.0684,   8.7401, -26.0018,  ...,  -0.8513,  12.0340,   6.8173]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 123.9960,   25.9537,    2.6629,  ...,  -59.6953,   91.0901,\n",
       "            111.0369],\n",
       "          [  43.5087,   20.0943,  -57.7684,  ...,   92.7878,   17.4061,\n",
       "             55.9168],\n",
       "          [  11.1409,  -38.2107, -100.5782,  ...,   27.6366,   68.1999,\n",
       "            172.4704],\n",
       "          ...,\n",
       "          [  24.5166,    1.6983,   -7.1084,  ...,   -9.2612,  -73.7525,\n",
       "            -18.5941],\n",
       "          [ -11.6609,    4.6362,  -46.5462,  ...,   79.4954,  -88.6351,\n",
       "             -9.8011],\n",
       "          [  24.9712,   45.2907,  -32.8963,  ...,   -9.7202,   20.4880,\n",
       "             -4.6775]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 217.1234,   37.6286,   -6.7415,  ...,  -72.8824,   84.4733,\n",
       "            109.6581],\n",
       "          [ 109.3001,  -17.9584,  -58.3608,  ...,  108.0717,    8.6459,\n",
       "             45.4246],\n",
       "          [  72.3040,    4.4164, -106.6130,  ...,   45.0551,   46.4229,\n",
       "            197.6886],\n",
       "          ...,\n",
       "          [ -12.2162,  -74.2518,  -28.8717,  ...,  -36.5524, -121.2156,\n",
       "            -26.5364],\n",
       "          [ -34.0481,  -53.6108,  -57.2103,  ...,   91.8958, -136.5668,\n",
       "            -45.0778],\n",
       "          [  35.5076,   65.1443,  -38.9931,  ...,  -26.7318,   31.0284,\n",
       "            -18.6469]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.2828,  0.0979,  0.1090,  ..., -0.2794,  0.2154,  0.0370],\n",
       "          [ 0.1593,  0.0956, -0.0922,  ...,  0.1854,  0.0993,  0.1548],\n",
       "          [ 0.0076,  0.0090, -0.0092,  ...,  0.0050,  0.0019,  0.0076],\n",
       "          ...,\n",
       "          [-0.0119, -0.1228, -0.0638,  ..., -0.0602, -0.1791, -0.0499],\n",
       "          [-0.0342, -0.0511, -0.1488,  ...,  0.3007, -0.2612, -0.1452],\n",
       "          [ 0.0670,  0.1300, -0.0356,  ..., -0.0100,  0.0690, -0.0531]]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.encoder_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35f18a4e56f4a659e5d767d6d4ad31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a25c64c02c34695988eb37c25352c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/45.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5EncoderModel\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mT5EncoderModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle-t5/t5-11b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/transformers/modeling_utils.py:3380\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3378\u001b[0m         \u001b[38;5;66;03m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[1;32m   3379\u001b[0m         filename \u001b[38;5;241m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[0;32m-> 3380\u001b[0m         resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3381\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\n\u001b[1;32m   3382\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[1;32m   3384\u001b[0m     \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[1;32m   3385\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m   3386\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3387\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[1;32m   3388\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs,\n\u001b[1;32m   3389\u001b[0m     )\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/transformers/utils/hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    414\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1219\u001b[0m     )\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/huggingface_hub/file_download.py:1367\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1365\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1367\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m     _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/huggingface_hub/file_download.py:1884\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1881\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1882\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1884\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1893\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1894\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/huggingface_hub/file_download.py:539\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    537\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[1;32m    541\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/urllib3/response.py:1043\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1043\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1046\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/urllib3/response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 935\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 862\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/urllib3/response.py:845\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import T5EncoderModel\n",
    "\n",
    "model = T5EncoderModel.from_pretrained(\"google-t5/t5-11b\")\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  572, 32099, 10681,    16, 32098,  2447,     1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"why <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "# labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# # the forward function automatically creates the correct decoder_input_ids\n",
    "# loss = model(input_ids=input_ids, labels=labels).loss\n",
    "# loss.item()\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2448, 1], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"himself\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d897153c4e7e4a4e8a859e76c64a97ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 15:22:40 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-06-01 15:22:45 INFO: File exists: /afs/cs.stanford.edu/u/ananthag/stanza_resources/en/default.zip\n",
      "2024-06-01 15:22:54 INFO: Finished downloading models and saved to /afs/cs.stanford.edu/u/ananthag/stanza_resources.\n",
      "2024-06-01 15:22:54 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aecb94bed2e4f789720df4e496c04df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 15:22:55 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus                   |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-06-01 15:22:55 INFO: Using device: cuda\n",
      "2024-06-01 15:22:55 INFO: Loading: tokenize\n",
      "2024-06-01 15:22:55 INFO: Loading: mwt\n",
      "2024-06-01 15:23:16 INFO: Loading: pos\n",
      "2024-06-01 15:23:16 INFO: Loading: lemma\n",
      "2024-06-01 15:23:16 INFO: Loading: constituency\n",
      "2024-06-01 15:23:16 INFO: Loading: depparse\n",
      "2024-06-01 15:23:17 INFO: Loading: sentiment\n",
      "2024-06-01 15:23:17 INFO: Loading: ner\n",
      "2024-06-01 15:23:17 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download(\"en\")\n",
    "nlp = stanza.Pipeline(\"en\", tokenize_pretokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The painting that the artist who lived long ago painted deteriorated\")\n",
    "words = []\n",
    "heads = []\n",
    "relns = []\n",
    "for dep_edge in doc.sentences[0].dependencies:\n",
    "    words.append(dep_edge[2].text)\n",
    "    heads.append(dep_edge[0].id)\n",
    "    relns.append(dep_edge[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The',\n",
       "  'painting',\n",
       "  'that',\n",
       "  'the',\n",
       "  'artist',\n",
       "  'who',\n",
       "  'lived',\n",
       "  'long',\n",
       "  'ago',\n",
       "  'painted',\n",
       "  'deteriorated'],\n",
       " [2, 10, 7, 5, 7, 7, 2, 9, 7, 0, 10],\n",
       " ['det',\n",
       "  'nsubj',\n",
       "  'obj',\n",
       "  'det',\n",
       "  'nsubj',\n",
       "  'nsubj',\n",
       "  'acl:relcl',\n",
       "  'advmod',\n",
       "  'advmod',\n",
       "  'root',\n",
       "  'xcomp'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words, heads, relns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The painting that the artist who lived long ago painted deteriorated .\")\n",
    "words = []\n",
    "heads = []\n",
    "relns = []\n",
    "for dep_edge in doc.sentences[0].dependencies:\n",
    "    words.append(dep_edge[2].text)\n",
    "    heads.append(dep_edge[0].id)\n",
    "    relns.append(dep_edge[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel, EncoderDecoderConfig, BertConfig\n",
    "\n",
    "probe = EncoderDecoderModel(\n",
    "        config=EncoderDecoderConfig.from_encoder_decoder_configs(\n",
    "            BertConfig(\n",
    "                num_hidden_layers=0,\n",
    "                hidden_size=768,\n",
    "                num_attention_heads=4,\n",
    "            ),\n",
    "            BertConfig(\n",
    "                num_hidden_layers=2,\n",
    "                hidden_size=768,\n",
    "                num_attention_heads=4,\n",
    "            ),\n",
    "        )\n",
    "    ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4370237"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in probe.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe.config.tie_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "tokenizer = utils.get_tokenizer(\"gpt2\")\n",
    "model = utils.get_model(\"gpt2\")\n",
    "\n",
    "inputs_batch = tokenizer([\"Hello world\", \"How are you\"], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "outputs = model(**inputs_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:623: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "probe_tokenizer = utils.get_tokenizer(\"google-bert/bert-base-cased\")\n",
    "targets_batch = probe_tokenizer([\"honestly not sure\", \"good\"], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "probe.config.decoder_start_token_id = probe_tokenizer.cls_token_id\n",
    "probe.config.pad_token_id = probe_tokenizer.pad_token_id\n",
    "probe.config.vocab_size = probe.config.decoder.vocab_size\n",
    "probe.config.eos_token_id = probe_tokenizer.sep_token_id\n",
    "\n",
    "out = probe(\n",
    "    encoder_outputs=(outputs.last_hidden_state,),\n",
    "    attention_mask=inputs_batch.attention_mask,\n",
    "    labels=targets_batch.input_ids,\n",
    "    output_attentions=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0] == outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa8483b553f4feca0dd7f0ccd8aa02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'DepParseDataPickle' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m gemma \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m train_input_strs \u001b[38;5;241m=\u001b[39m _get_sentences_from_trees(\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mstanza_cache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcached_ptb_train_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m/juice2/u/ananthag/lm_syntax/stanza_cache.py:102\u001b[0m, in \u001b[0;36mcached_ptb_train_sentences\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_ptb_train_sentences\u001b[39m():\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_TRAIN_PICKLE_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_strs\n",
      "File \u001b[0;32m/juice2/u/ananthag/lm_syntax/stanza_cache.py:93\u001b[0m, in \u001b[0;36m_read_pickle\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_pickle\u001b[39m(path):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[0;32m---> 93\u001b[0m         pickle_obj \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pickle_obj\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'DepParseDataPickle' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import stanza_cache\n",
    "\n",
    "def _get_sentences_from_trees(trees):\n",
    "    return [\" \".join(tree.leaf_labels()) for tree in trees]\n",
    "\n",
    "tokenizer = utils.get_tokenizer(\"google/gemma-7b\")\n",
    "gemma = utils.get_model(\"google/gemma-7b\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from collections.abc import Sequence, Mapping\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class DepParseDataPickle:\n",
    "    input_strs: Sequence[str]\n",
    "    dev_data: Sequence[Mapping[(str, str)]]\n",
    "\n",
    "train_input_strs = _get_sentences_from_trees(\n",
    "    stanza_cache.cached_ptb_train_sentences()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 15.5885,   0.7239,  -0.6495,  ...,  -0.8018,  -2.4763,  -2.0839],\n",
       "          [-24.2487,  -1.1367,   2.4628,  ...,  -7.3612,   3.7618,   6.0081],\n",
       "          [-17.1040,  -0.7375,   3.2747,  ...,  -6.3599,   3.2611,   5.1420],\n",
       "          ...,\n",
       "          [-33.7750,  -0.8051,   1.7591,  ..., -12.3409,   4.3301,   8.7685],\n",
       "          [-52.8275,  -4.8714,   7.0906,  ..., -12.1785,   3.7077,  16.5627],\n",
       "          [-17.2123,  -0.3028,   3.5182,  ...,  -5.5480,   3.6806,   5.7104]]],\n",
       "        device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor([[[ 1.2619,  0.1925, -0.3066,  ..., -0.1372, -0.3350, -0.4170],\n",
       "          [-0.3267,  0.0202,  0.3087,  ..., -0.5101,  0.1824,  0.4498],\n",
       "          [ 0.1364, -0.0973, -0.0407,  ..., -0.7935, -0.3390,  0.0204],\n",
       "          ...,\n",
       "          [-0.0358,  0.4497, -2.5396,  ..., -3.0218,  1.4899, -0.8461],\n",
       "          [-0.1255, -0.2787, -0.8703,  ...,  0.3135,  2.0707,  0.1958],\n",
       "          [-0.1106, -0.0613, -0.1361,  ..., -0.1838,  0.0216,  0.0886]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.4009,  0.0622, -0.2529,  ..., -0.0656, -0.2720, -0.2791],\n",
       "          [ 0.1298, -0.0489,  0.1777,  ..., -0.1739, -0.0083,  0.1762],\n",
       "          [ 0.0599, -0.3159, -0.2678,  ..., -0.6696,  0.0369, -0.0499],\n",
       "          ...,\n",
       "          [ 0.1247,  0.1310, -0.8462,  ..., -1.6153,  0.4264, -0.4266],\n",
       "          [ 0.5698,  0.1283, -0.1675,  ...,  0.2771,  0.6851,  0.1999],\n",
       "          [-0.1015, -0.2024, -0.1248,  ..., -0.1528, -0.1158,  0.0084]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 2.2195e-01,  5.8700e-02, -2.3846e-01,  ..., -2.4911e-02,\n",
       "           -2.5910e-01, -2.7663e-01],\n",
       "          [-3.7896e-03,  2.3673e-02,  2.2093e-01,  ...,  1.2861e-02,\n",
       "            6.3073e-02,  1.9058e-01],\n",
       "          [-1.1286e-01, -9.1842e-02, -1.3523e-01,  ..., -3.6752e-01,\n",
       "            5.3817e-02,  7.6588e-02],\n",
       "          ...,\n",
       "          [ 2.0131e-02,  3.2380e-01, -8.2512e-01,  ..., -1.0615e+00,\n",
       "            4.1062e-01, -3.5111e-01],\n",
       "          [ 5.8625e-01, -3.4435e-01, -9.6803e-02,  ..., -5.6406e-02,\n",
       "            5.8984e-01, -2.9879e-02],\n",
       "          [-1.9440e-01, -6.5682e-02, -9.6574e-02,  ..., -8.8438e-02,\n",
       "            6.8452e-04,  6.4983e-02]]], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.1758,  0.0530, -0.2761,  ..., -0.0450, -0.2571, -0.2942],\n",
       "          [-0.0794,  0.0194,  0.1661,  ..., -0.0125,  0.0237,  0.1905],\n",
       "          [ 0.0074, -0.0424, -0.0168,  ..., -0.2726,  0.0627,  0.1243],\n",
       "          ...,\n",
       "          [ 0.0779,  0.1493, -0.7023,  ..., -0.7376,  0.4258, -0.3178],\n",
       "          [ 0.3637, -0.1130,  0.0162,  ..., -0.1575,  0.4960,  0.0052],\n",
       "          [-0.0848, -0.0889, -0.1303,  ...,  0.1134, -0.0520,  0.0406]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 1.1627e-01,  3.8474e-02, -3.0561e-01,  ..., -7.0529e-02,\n",
       "           -2.3695e-01, -3.0561e-01],\n",
       "          [-1.5207e-01,  1.1910e-01,  1.6708e-01,  ..., -4.3010e-03,\n",
       "            3.9591e-02,  2.1436e-01],\n",
       "          [-8.6396e-03, -5.8505e-04,  1.4542e-01,  ..., -1.3254e-01,\n",
       "            4.1759e-02,  1.8874e-01],\n",
       "          ...,\n",
       "          [ 3.7522e-03, -1.3628e-02, -5.9477e-01,  ..., -3.6536e-01,\n",
       "            4.0025e-01, -2.0796e-01],\n",
       "          [ 1.5793e-01,  2.1475e-02, -4.1228e-03,  ..., -1.5288e-01,\n",
       "            4.8515e-01,  1.6925e-02],\n",
       "          [-7.1956e-02, -2.0711e-02, -1.1288e-01,  ..., -1.2813e-02,\n",
       "           -1.2285e-02,  1.4489e-02]]], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.0880,  0.0456, -0.3470,  ..., -0.0806, -0.2243, -0.2977],\n",
       "          [-0.1639,  0.1966,  0.2402,  ...,  0.0114,  0.0274,  0.2666],\n",
       "          [ 0.0252,  0.0748,  0.2262,  ..., -0.0312, -0.0424,  0.2405],\n",
       "          ...,\n",
       "          [ 0.0690, -0.0007, -0.4416,  ..., -0.2686,  0.3333, -0.1979],\n",
       "          [ 0.1905, -0.0823, -0.0515,  ..., -0.1285,  0.4585,  0.0465],\n",
       "          [-0.0035,  0.0249, -0.0956,  ...,  0.0382, -0.0150,  0.0746]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.0968,  0.0403, -0.3601,  ..., -0.1419, -0.2231, -0.3074],\n",
       "          [-0.1754,  0.1419,  0.2457,  ...,  0.0359,  0.0901,  0.2680],\n",
       "          [-0.0522,  0.0634,  0.2307,  ..., -0.0329, -0.1078,  0.2490],\n",
       "          ...,\n",
       "          [-0.0575,  0.0239, -0.3900,  ..., -0.1809,  0.3440, -0.1663],\n",
       "          [ 0.1556, -0.0840, -0.0917,  ..., -0.1172,  0.4672,  0.0227],\n",
       "          [-0.0348, -0.0643, -0.0503,  ..., -0.0213,  0.0436,  0.0662]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.1033,  0.0431, -0.3790,  ..., -0.1186, -0.2268, -0.2951],\n",
       "          [-0.1378,  0.1311,  0.2186,  ...,  0.0276,  0.0873,  0.2560],\n",
       "          [-0.0881,  0.0396,  0.2641,  ..., -0.0094, -0.0981,  0.2175],\n",
       "          ...,\n",
       "          [-0.0612, -0.0075, -0.3260,  ..., -0.1470,  0.2885, -0.1701],\n",
       "          [ 0.0921, -0.1099, -0.1032,  ..., -0.1773,  0.4975,  0.0145],\n",
       "          [-0.0639, -0.0927,  0.0210,  ..., -0.0458, -0.0112,  0.0494]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.1173,  0.0644, -0.4233,  ..., -0.1439, -0.2167, -0.2640],\n",
       "          [-0.1497,  0.1311,  0.1690,  ..., -0.0010,  0.0687,  0.2705],\n",
       "          [-0.0791, -0.0204,  0.2256,  ...,  0.0068, -0.1150,  0.2157],\n",
       "          ...,\n",
       "          [-0.0673,  0.0015, -0.2920,  ..., -0.0696,  0.2385, -0.1465],\n",
       "          [ 0.0255, -0.1210, -0.1722,  ..., -0.0687,  0.4301, -0.0196],\n",
       "          [-0.0564, -0.0314, -0.0346,  ...,  0.0056, -0.0522, -0.0154]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.1302,  0.0370, -0.4231,  ..., -0.1541, -0.1977, -0.2460],\n",
       "          [-0.1142,  0.0989,  0.1555,  ..., -0.0513,  0.0632,  0.2760],\n",
       "          [-0.0831, -0.0168,  0.1398,  ..., -0.0095, -0.0580,  0.2269],\n",
       "          ...,\n",
       "          [-0.0215, -0.0463, -0.2501,  ..., -0.0473,  0.2317, -0.1488],\n",
       "          [ 0.0434, -0.1123, -0.1409,  ..., -0.0779,  0.3823, -0.0467],\n",
       "          [-0.1302,  0.0197, -0.0231,  ..., -0.0209, -0.0404, -0.0166]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.0701,  0.0475, -0.4340,  ..., -0.1187, -0.1923, -0.2192],\n",
       "          [ 0.0843,  0.1085,  0.1742,  ..., -0.0114,  0.0237,  0.2428],\n",
       "          [ 0.0086, -0.0327,  0.1526,  ..., -0.0044, -0.0705,  0.2195],\n",
       "          ...,\n",
       "          [-0.0183, -0.0469, -0.2727,  ..., -0.0372,  0.2209, -0.1419],\n",
       "          [-0.0092, -0.1024, -0.1594,  ..., -0.0508,  0.3551, -0.0423],\n",
       "          [-0.0604, -0.0118, -0.0221,  ..., -0.0111, -0.0561, -0.0396]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.0450,  0.0836, -0.4505,  ..., -0.0828, -0.1920, -0.2076],\n",
       "          [ 0.0394,  0.0998,  0.1689,  ..., -0.0045,  0.0347,  0.2214],\n",
       "          [ 0.0203, -0.0374,  0.1458,  ..., -0.0433, -0.0500,  0.2128],\n",
       "          ...,\n",
       "          [-0.0077, -0.0292, -0.2482,  ..., -0.0027,  0.2076, -0.1238],\n",
       "          [ 0.0627, -0.0816, -0.1540,  ..., -0.0402,  0.3422, -0.0227],\n",
       "          [-0.0733, -0.0480, -0.0449,  ...,  0.0194, -0.0444, -0.0497]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.0027,  0.1024, -0.4527,  ..., -0.0282, -0.1999, -0.1526],\n",
       "          [ 0.0828,  0.1371,  0.2185,  ...,  0.0070,  0.0442,  0.1860],\n",
       "          [ 0.0102, -0.0633,  0.1457,  ...,  0.0114, -0.0379,  0.2257],\n",
       "          ...,\n",
       "          [-0.0058, -0.0281, -0.2291,  ...,  0.0011,  0.2161, -0.1307],\n",
       "          [ 0.0507, -0.0624, -0.1557,  ..., -0.0199,  0.3517, -0.0332],\n",
       "          [-0.0702, -0.0744, -0.0712,  ...,  0.0380, -0.0510, -0.0440]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.0133,  0.1424, -0.4498,  ...,  0.0119, -0.2144, -0.1221],\n",
       "          [ 0.1491,  0.1725,  0.2655,  ...,  0.0244,  0.0715,  0.1864],\n",
       "          [-0.0482, -0.0726,  0.1519,  ...,  0.0276, -0.0483,  0.2219],\n",
       "          ...,\n",
       "          [-0.0060, -0.0077, -0.2221,  ...,  0.0012,  0.2180, -0.1289],\n",
       "          [ 0.0055, -0.0746, -0.1629,  ...,  0.0298,  0.3509, -0.0375],\n",
       "          [-0.0151, -0.0347, -0.1005,  ...,  0.0683, -0.0658, -0.0172]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.0089,  0.1307, -0.4556,  ...,  0.0235, -0.2263, -0.0944],\n",
       "          [ 0.1387,  0.1767,  0.2749,  ...,  0.0077,  0.0782,  0.1760],\n",
       "          [-0.0804, -0.0042,  0.1438,  ...,  0.0286, -0.0525,  0.2243],\n",
       "          ...,\n",
       "          [-0.0323, -0.0104, -0.2271,  ..., -0.0036,  0.2178, -0.1338],\n",
       "          [-0.0075, -0.0582, -0.1532,  ...,  0.0477,  0.3668, -0.0422],\n",
       "          [ 0.0251, -0.0156, -0.1154,  ..., -0.0140, -0.0526, -0.0232]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.0268,  0.1432, -0.4524,  ...,  0.0351, -0.2403, -0.0612],\n",
       "          [ 0.1344,  0.1618,  0.2855,  ...,  0.0077,  0.0731,  0.1767],\n",
       "          [-0.0466,  0.0181,  0.1061,  ...,  0.0250, -0.0592,  0.2109],\n",
       "          ...,\n",
       "          [-0.0294, -0.0341, -0.2261,  ..., -0.0272,  0.2138, -0.1325],\n",
       "          [ 0.0173, -0.0811, -0.1667,  ...,  0.0416,  0.3655, -0.0303],\n",
       "          [ 0.0869,  0.0118, -0.1152,  ...,  0.0439, -0.0636,  0.0110]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.1831,  0.1831, -0.4371,  ...,  0.0137, -0.2413, -0.0174],\n",
       "          [ 0.1385,  0.1386,  0.2722,  ...,  0.0127,  0.0736,  0.1870],\n",
       "          [-0.0236, -0.0087,  0.0812,  ...,  0.0418, -0.0667,  0.2218],\n",
       "          ...,\n",
       "          [-0.0444, -0.0415, -0.2015,  ..., -0.0165,  0.2144, -0.1241],\n",
       "          [ 0.0036, -0.0722, -0.1365,  ...,  0.0520,  0.3453, -0.0383],\n",
       "          [ 0.1043, -0.0093, -0.1629,  ...,  0.0437, -0.0508,  0.0089]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.4967,  0.2292, -0.3791,  ..., -0.0051, -0.2514,  0.0376],\n",
       "          [ 0.0285,  0.0680,  0.2053,  ...,  0.0297,  0.0172,  0.1785],\n",
       "          [-0.0329, -0.0523,  0.0786,  ...,  0.0467, -0.0734,  0.2305],\n",
       "          ...,\n",
       "          [-0.0227, -0.0705, -0.1984,  ..., -0.0063,  0.2197, -0.1328],\n",
       "          [-0.0672, -0.1173, -0.1488,  ...,  0.0729,  0.3298, -0.0588],\n",
       "          [ 0.1745, -0.0096, -0.1406,  ...,  0.0167, -0.0454, -0.0122]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-1.0633e+00,  2.2006e-01, -3.9203e-01,  ..., -7.4466e-02,\n",
       "           -2.4735e-01,  9.7053e-02],\n",
       "          [ 5.2380e-02,  5.8642e-02,  1.7966e-01,  ...,  6.8749e-02,\n",
       "            2.8337e-02,  1.8095e-01],\n",
       "          [-2.3525e-02, -4.1422e-04,  7.9785e-02,  ...,  5.8831e-02,\n",
       "           -7.9036e-02,  2.3987e-01],\n",
       "          ...,\n",
       "          [ 6.1077e-02, -4.8156e-02, -2.1550e-01,  ...,  2.8128e-02,\n",
       "            2.2817e-01, -1.0794e-01],\n",
       "          [-1.4353e-02, -1.3949e-01, -1.5433e-01,  ...,  7.8608e-02,\n",
       "            3.1077e-01, -7.4931e-02],\n",
       "          [ 2.9942e-01, -9.5538e-02, -5.8094e-02,  ...,  3.9685e-02,\n",
       "           -1.2437e-01,  1.5356e-02]]], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-1.6118,  0.2644, -0.4691,  ..., -0.1570, -0.1910,  0.1212],\n",
       "          [ 0.0416,  0.0853,  0.1492,  ...,  0.1131,  0.0055,  0.1341],\n",
       "          [-0.0592,  0.0722,  0.0972,  ...,  0.0441, -0.1071,  0.2192],\n",
       "          ...,\n",
       "          [ 0.1012, -0.0590, -0.1907,  ...,  0.0644,  0.2107, -0.1247],\n",
       "          [ 0.1300, -0.1716, -0.1174,  ...,  0.0752,  0.2966, -0.1251],\n",
       "          [ 0.4821, -0.1249, -0.0150,  ...,  0.1312, -0.1250, -0.0507]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-2.2872,  0.1048, -0.5184,  ..., -0.0598, -0.1453,  0.0229],\n",
       "          [-0.0295,  0.1393,  0.0732,  ...,  0.1621,  0.0545,  0.0574],\n",
       "          [-0.1169,  0.0383,  0.0753,  ..., -0.0373, -0.1192,  0.2000],\n",
       "          ...,\n",
       "          [ 0.1858, -0.0480, -0.2241,  ...,  0.0304,  0.2055, -0.1559],\n",
       "          [ 0.1679, -0.0710, -0.0441,  ...,  0.0771,  0.4216, -0.0661],\n",
       "          [ 0.7875, -0.1216,  0.0258,  ...,  0.0999, -0.1220, -0.0434]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-2.3579,  0.1317, -0.4454,  ..., -0.0054, -0.0593, -0.0520],\n",
       "          [ 0.0596,  0.1779,  0.0361,  ...,  0.2064,  0.1123,  0.1889],\n",
       "          [-0.0187,  0.0950, -0.0900,  ..., -0.0313, -0.0876,  0.2438],\n",
       "          ...,\n",
       "          [ 0.2085, -0.0053, -0.1938,  ...,  0.0289,  0.2145, -0.1562],\n",
       "          [ 0.0368, -0.0223,  0.0188,  ..., -0.0348,  0.3067, -0.0957],\n",
       "          [ 0.7940, -0.2256,  0.0185,  ...,  0.0347,  0.0715, -0.0459]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-2.0753, -0.0318, -0.5709,  ..., -0.1555,  0.0420, -0.1380],\n",
       "          [ 0.0042,  0.1734,  0.0144,  ...,  0.2642,  0.2247,  0.2300],\n",
       "          [ 0.0758,  0.1280, -0.1371,  ..., -0.0589, -0.1143,  0.3152],\n",
       "          ...,\n",
       "          [ 0.2058, -0.0642, -0.2552,  ..., -0.0491,  0.2834, -0.1880],\n",
       "          [ 0.0631, -0.0558,  0.1105,  ...,  0.0293,  0.2460, -0.1567],\n",
       "          [ 0.7561, -0.1461,  0.0458,  ..., -0.0402,  0.2170, -0.1450]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-1.5338,  0.0860, -0.3328,  ..., -0.1804,  0.1222, -0.0406],\n",
       "          [-0.1164,  0.1823,  0.0586,  ...,  0.2820,  0.2212,  0.1702],\n",
       "          [ 0.0924,  0.0602, -0.2096,  ..., -0.1932, -0.1749,  0.3970],\n",
       "          ...,\n",
       "          [ 0.1839, -0.0454, -0.2401,  ...,  0.0914,  0.2197, -0.2794],\n",
       "          [ 0.0432, -0.1157,  0.0442,  ...,  0.0126,  0.3738, -0.1693],\n",
       "          [ 0.8971, -0.2951, -0.0128,  ..., -0.1657,  0.3507, -0.2124]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.4599, -0.1434, -0.2359,  ..., -0.4732,  0.1350, -0.0128],\n",
       "          [-0.1555,  0.1385,  0.1199,  ...,  0.2973,  0.3103,  0.1255],\n",
       "          [ 0.0147,  0.0340, -0.2175,  ..., -0.1602, -0.1563,  0.3101],\n",
       "          ...,\n",
       "          [ 0.1611,  0.0500, -0.2117,  ...,  0.0310,  0.1758, -0.3523],\n",
       "          [-0.1100,  0.0142,  0.1002,  ..., -0.0908,  0.3797, -0.1252],\n",
       "          [ 0.9922, -0.3069, -0.0898,  ..., -0.2309,  0.3957, -0.2347]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 4.4290,  0.2515, -1.0816,  ..., -0.8482,  0.1379, -0.4068],\n",
       "          [-0.5212,  0.2964,  0.1218,  ...,  0.3878,  0.5556,  0.0340],\n",
       "          [-0.0456,  0.1571, -0.3227,  ..., -0.0802, -0.1368,  0.4139],\n",
       "          ...,\n",
       "          [-0.1072,  0.1502, -0.5027,  ...,  0.0158,  0.4259, -0.2766],\n",
       "          [-0.7608,  0.0831,  0.2647,  ..., -0.1330,  0.3535, -0.1079],\n",
       "          [ 1.0439, -0.2330, -0.0719,  ..., -0.2026,  0.5734, -0.3771]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 5.8900e+00, -9.5689e-01,  2.9250e-01,  ..., -2.0696e+00,\n",
       "            8.3639e-01, -9.1010e-02],\n",
       "          [-2.9951e+00,  6.6334e-01,  3.8070e-01,  ...,  8.3821e-01,\n",
       "            5.7298e-01, -2.3356e-01],\n",
       "          [-1.0855e+00,  1.3627e-01, -2.9216e-01,  ..., -2.6860e-02,\n",
       "           -1.4908e-01,  3.5076e-01],\n",
       "          ...,\n",
       "          [-7.4033e-01,  1.1933e-01, -4.5438e-01,  ...,  2.1340e-01,\n",
       "            4.1097e-01, -1.1193e-01],\n",
       "          [-4.0641e+00, -1.3597e-04, -1.7661e-01,  ..., -2.1970e-01,\n",
       "            3.5832e-01, -1.1821e-01],\n",
       "          [ 1.5227e-01, -1.1683e-01, -5.2525e-01,  ...,  2.3018e-01,\n",
       "            1.2015e+00, -1.0352e+00]]], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[  6.8824,  -0.9620,  -0.4997,  ...,   0.4188,   3.3050,   1.6475],\n",
       "          [ -8.5272,   3.0381,   0.8714,  ...,   2.4759,   2.0016,  -0.9838],\n",
       "          [  1.7360,  -0.0290,  -0.7515,  ...,  -0.2600,   0.3232,   0.8154],\n",
       "          ...,\n",
       "          [ -4.8721,   0.9721,  -2.7095,  ...,   1.7548,   3.7453,  -0.1428],\n",
       "          [-18.2543,   0.3423,  -0.9523,  ...,  -2.6070,  -0.1010,   2.1408],\n",
       "          [ 15.2199,   0.4229,  -2.3181,  ...,   3.5585,   1.3741,  -3.9689]]],\n",
       "        device='cuda:0', grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = train_input_strs[0]\n",
    "inputs = tokenizer(sent, return_tensors=\"pt\").to('cuda')\n",
    "outputs = gemma(**inputs, output_hidden_states=True)\n",
    "outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "\n",
    "head_word_dataset = dataset.HeadWordDataset(\"train\", \"google/gemma-7b\", gemma.config.num_hidden_layers, gemma.config.hidden_size, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 28, 3072)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_word_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_input_strs[0].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cache_hidden_states\n",
    "\n",
    "len(cache_hidden_states.get_word_hidden_states([train_input_strs[0]], tokenizer, gemma)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(5, 0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.ones(5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-large\", use_fast=False)\n",
    "roberta = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   133,  7674,    14,     5,  3025, 10122, 23677,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[(1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8)]\n"
     ]
    }
   ],
   "source": [
    "# sent = \"The fact that my brother said who his friend trusted our uncle at the party surprised my daughter yesterday afternoon\"\n",
    "import utils\n",
    "# sent = \"The officials who the coaches had bribed handsomely reviewed the contract that the lawyers had written.\"\n",
    "# regions = [\"The officials\", \"who the coaches had bribed handsomely\", \"reviewed the contract\", \"that the lawyers had written\", \".\"]\n",
    "# print(regions)\n",
    "regions = sent.split(\" \")\n",
    "sent = \"The painting that the artist painted deteriorated\"\n",
    "def _get_all_target_idxs(regions, tokenized):\n",
    "    all_target_idxs = []\n",
    "    st = 0\n",
    "    sent_tokens = tokenized.input_ids[0].tolist()\n",
    "    for idx, region in enumerate(regions):\n",
    "        region = region.lstrip().rstrip()\n",
    "        if not region:\n",
    "            all_target_idxs.append((st, st))\n",
    "            continue\n",
    "        word_tokenized = utils.get_tokenized_word(roberta_tokenizer, region, idx)\n",
    "        st_curr, en_curr = utils.get_idxs(word_tokenized, sent_tokens, st)\n",
    "        all_target_idxs.append((st_curr, en_curr))\n",
    "        st = en_curr\n",
    "    return all_target_idxs\n",
    "tokenized = roberta_tokenizer(sent, return_tensors=\"pt\")\n",
    "print(tokenized)\n",
    "print(_get_all_target_idxs(regions, tokenized))\n",
    "seq_length = tokenized.input_ids.shape[1]\n",
    "repeated = tokenized.input_ids.repeat(seq_length, 1)\n",
    "mask_token_mask = torch.triu(torch.ones(seq_length, seq_length, dtype=bool))\n",
    "mask_token_mask[torch.arange(seq_length - 1), -1] = False\n",
    "repeated[mask_token_mask] = roberta_tokenizer.mask_token_id\n",
    "masked_attention_mask = torch.ones_like(repeated)\n",
    "output = roberta(repeated, attention_mask=masked_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>.</s>)-, the?\"',\n",
       " '</s>The(31PhotoA2By[',\n",
       " ' New Great end first World American best world last next',\n",
       " ' is of was by depicts, shows from has and',\n",
       " ' changed made won inspired will was broke is started turned',\n",
       " ' artist jury man police two judge woman museum family artists',\n",
       " ' is has was did created made painted had showed commissioned',\n",
       " '.:</s> is... was?, in',\n",
       " '</s><s> \" ( the. I- The,']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs = torch.nn.functional.log_softmax(output.logits, dim=-1)\n",
    "top_k = torch.topk(log_probs[torch.arange(seq_length), torch.arange(seq_length)], 10, dim=-1)\n",
    "detokenized = roberta_tokenizer.batch_decode(top_k.indices)\n",
    "detokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "determiner_noun_agreement_1.jsonl\n",
      "determiner_noun_agreement_2.jsonl\n",
      "determiner_noun_agreement_irregular_1.jsonl\n",
      "determiner_noun_agreement_irregular_2.jsonl\n",
      "determiner_noun_agreement_with_adj_2.jsonl\n",
      "determiner_noun_agreement_with_adj_irregular_1.jsonl\n",
      "determiner_noun_agreement_with_adj_irregular_2.jsonl\n",
      "determiner_noun_agreement_with_adjective_1.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import blimp_dataset\n",
    "\n",
    "for file_name in sorted(os.listdir(\"../blimp/data\")):\n",
    "    term = blimp_dataset.BlimpDataset(f\"../blimp/data/{file_name}\").linguistics_term\n",
    "    if term == \"determiner_noun_agreement\":\n",
    "        print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FULL_WORD_FILTER = {\n",
    "    \"have\",\n",
    "    \"has\",\n",
    "    \"had\",\n",
    "    \"having\",\n",
    "    \"being\",\n",
    "    \"be\",\n",
    "    \"is\",\n",
    "    \"am\",\n",
    "    \"are\",\n",
    "    \"was\",\n",
    "    \"will\",\n",
    "    \"were\",\n",
    "    \"do\",\n",
    "    \"does\"\n",
    "}\n",
    "_ENDS_WITH_FILTER = {\"'d\", \"'m\", \"'re\", \"'ll\", \"n't\"}\n",
    "\n",
    "def _check_if_valid(words, word):\n",
    "    index = words.index(word)\n",
    "    assert index\n",
    "    preceding = words[index - 1]\n",
    "    if preceding != \"that\" and preceding != \"who\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "dataset = blimp_dataset.BlimpDataset(\"../blimp/data/distractor_agreement_relative_clause.jsonl\")\n",
    "remaining = []\n",
    "filtered = []\n",
    "for good, _ in dataset:\n",
    "    valid = True\n",
    "    words = good.split(\" \")\n",
    "    for word in _FULL_WORD_FILTER:\n",
    "        if word in words:\n",
    "            valid = valid and _check_if_valid(words, word)\n",
    "        if word + \"n't\" in words:\n",
    "            valid = valid and _check_if_valid(words, word + \"n't\")\n",
    "        \n",
    "    for word in words:\n",
    "        for suffix in _ENDS_WITH_FILTER:\n",
    "            if word.endswith(suffix):\n",
    "                valid = valid and _check_if_valid(words, word)\n",
    "    if valid:\n",
    "        remaining.append(good)\n",
    "    else:\n",
    "        filtered.append(good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[2, 3].index(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Boys that aren't disturbing Natalie suffer.\",\n",
       " 'A lot of schools that have concealed Adam worry.',\n",
       " \"All pedestrians that wouldn't shock William read.\",\n",
       " 'A doctor who researched the Clintons waves.',\n",
       " \"The girl who doesn't annoy these adults works with Kristin.\",\n",
       " 'Actors who embarrasses Brad long to surrender.',\n",
       " 'That guy that respects those doctors praises some newspaper article.',\n",
       " \"The waiter who isn't disturbing many boys writes.\",\n",
       " 'Some lady that sells some hospitals encourages Vanessa to move.',\n",
       " \"Every association that didn't praise a lot of men badgers Beth to hate Kristin.\",\n",
       " 'Some patients who dislike Kendra negotiate.',\n",
       " 'Some dancer that liked all girls cries.',\n",
       " 'All adults who hire Bruce ascend some slope.',\n",
       " 'A school that could hate these actors testifies.',\n",
       " 'Those ladies who were thinking about Emily break every car.',\n",
       " 'This guy that saw people cares for Claire.',\n",
       " 'The cashiers who distracts Jesus require this lady to praise Diane.',\n",
       " 'A lot of students that can flee from Samantha think about libraries.',\n",
       " \"A lot of teachers who hadn't stunned Vanessa dislike Karla.\",\n",
       " 'A lot of actors that are concealing Wendy neglect to reply.',\n",
       " \"The government that hadn't alarmed these associations finds out who longs to train.\",\n",
       " 'This actor who respected some patients slumps over.',\n",
       " 'The university that had a lot of unicycles alarms Rebecca.',\n",
       " 'The library that was referencing all senators seems to argue.',\n",
       " 'The doctors who should dislike some man pass this restaurant.',\n",
       " 'Every waiter who would visit some banks conceals who assemble.',\n",
       " \"A lot of waiters who didn't dislike Katherine press Pamela to nod.\",\n",
       " 'All teenagers that bother Allison litter.',\n",
       " 'These children that have astounded Carol care for Stacy.',\n",
       " 'The teachers who have distracted a library have clashed.',\n",
       " 'That customer that helped the waiters investigates a library.',\n",
       " \"Many students who don't listen to this lady don't irritate Carrie.\",\n",
       " 'Every university that bothered many actors loves Caroline.',\n",
       " \"That actress who isn't leaving most doctors slumps over.\",\n",
       " 'Waiters that knew Wayne walk through this movie theater.',\n",
       " 'Some students who hide Steven stretch.',\n",
       " 'The pedestrian who healed some hamsters climbs down that ladder.',\n",
       " 'Some library that insulted women expects some university to talk about Brett.',\n",
       " 'Many patients that would approach Patrick continue to work with Marcus.',\n",
       " \"This guy who won't upset universities prods Regina to study.\",\n",
       " 'Pedestrians that shocks Eric ascertain some patients to heal Sonia.',\n",
       " 'Some waitresses who would embarrass Steve clean many skirts.',\n",
       " \"Every customer that isn't irritating many men tours the closet.\",\n",
       " \"Most guests who weren't alarming the legislature weren't negotiating.\",\n",
       " 'Some cashier who listens to some customers appreciates that man.',\n",
       " \"This waitress that wouldn't disturb the drivers approves.\",\n",
       " \"Some boy who wouldn't hide these dancers washes.\",\n",
       " 'Most senators who would praise Maria need to learn.',\n",
       " \"These teachers that hadn't left Harvard University succeed.\",\n",
       " 'This doctor that discusses cashiers cheers.',\n",
       " \"Some actors who haven't disliked that child get fired.\",\n",
       " 'All guests that noticed Phillip sneeze.',\n",
       " 'A lot of companies that were bothering David appreciate many teachers.',\n",
       " 'All waiters who might leave the man flee from Frank.',\n",
       " \"Guests who aren't stunning Sara know who isn't laughing.\",\n",
       " 'The guests who conceal every waiter donate.',\n",
       " \"The association that hasn't disgusted these dancers forgets Julia.\",\n",
       " 'Those senators who respected Brian approach those ladies.',\n",
       " 'This cashier who admires the Borgias spins around.',\n",
       " 'All pedestrians who hired Kimberley break every screen.',\n",
       " 'Some senator who is bothering those waiters is forcing Carolyn to aggravate Peter.',\n",
       " \"A driver who hired those drivers learns who hasn't donated.\",\n",
       " 'This lady who does observe some boys walks through hospitals.',\n",
       " \"This dancer that doesn't boast about some actors climbs down this slope.\",\n",
       " 'Most waitresses who concealed Laura spur Martin to aggravate Kevin.',\n",
       " 'Every lady who hurt these waitresses kisses Linda.',\n",
       " \"The guys that weren't alarming this company happen to brighten.\",\n",
       " 'Some lady that leaves the Lutherans blinks.',\n",
       " \"Some waiter that hasn't toured these libraries dislikes Jill.\",\n",
       " 'Those senators who attacked Christine hide Andrew.',\n",
       " \"That cashier who hadn't respected many waitresses requires Naomi's partners to cook.\",\n",
       " \"All dancers who haven't remembered Ronald sit down.\",\n",
       " 'The birds that have distracted Liam hide.',\n",
       " \"These guests who aren't stunning Scott cope.\",\n",
       " \"The organization that isn't working with most pedestrians describes all dancers.\",\n",
       " \"Every lady who isn't hugging the patients isn't examining many essays.\",\n",
       " 'A lot of ladies that remember Todd go fishing.',\n",
       " 'This university that embarrassed museums forgets who left Andrew.',\n",
       " 'This adult who scares all guests telephones.',\n",
       " 'That patient who hides the Impressionists discusses Ann.',\n",
       " 'Customers that were fleeing from Emily were arguing about this student.',\n",
       " 'A driver that works with all children litters.',\n",
       " 'Libraries that appreciated Danielle talk about Jason.',\n",
       " \"Some waiter who hasn't distracted all pedestrians hasn't arrived at a museum.\",\n",
       " \"Many students that weren't insulting Carrie kiss David.\",\n",
       " 'Most actresses who alarms Sara prompt some woman to swallow.',\n",
       " 'Some governments that had discussed Janet advise Steven to murmur.',\n",
       " \"Waitresses who bores Joseph prod Carlos's mentee to bring some pictures.\",\n",
       " 'A waitress who impressed some governments thinks about who should describe this woman.',\n",
       " \"Some museums that wouldn't hate Colleen worry.\",\n",
       " 'This driver that can dislike children admires a sock.',\n",
       " \"Most actors that don't disgust Sherry don't go fishing.\",\n",
       " \"The children that haven't found Suzanne play.\",\n",
       " 'The teenager who forgets the actors sees.',\n",
       " 'Those waiters who have left David leave.',\n",
       " \"Many teachers who shouldn't respect Jane murmur.\",\n",
       " 'This actor that cured those cashiers paints.',\n",
       " 'This boy that respected these students practices.',\n",
       " \"Some waitress that didn't reveal those people passes every bank.\",\n",
       " 'That driver that sounds like the libraries references Galileo.',\n",
       " 'A lot of senators who had disturbed Ellen profit.',\n",
       " \"This legislature that hadn't embarrassed associations appreciates the Balkans.\",\n",
       " \"A guest that is disgusting girls asks Lissa's oncologists to attack Janice.\",\n",
       " 'The lady that fired most guests longs to wave.',\n",
       " 'Every waitress who complains about some reports resembles prints.',\n",
       " \"Most schools that don't bore Vanessa forfeit.\",\n",
       " \"Many girls that couldn't alarm Victoria anticipate Marla to clean some dress.\",\n",
       " 'A legislature that was criticizing many teachers entreats Raymond to salute.',\n",
       " 'A lot of pedestrians that visited Cheryl salute.',\n",
       " \"These girls who won't help Amy murmur.\",\n",
       " 'This guest who escaped from men boasts about Christine.',\n",
       " 'Every customer who cured the Clintons sees.',\n",
       " 'Most waitresses that can boast about Dan want the eye to open.',\n",
       " 'A dancer that will admire those rabbits wakes up.',\n",
       " \"All pedestrians that wouldn't bother Angela criticize Marcus.\",\n",
       " \"This driver who hasn't referenced these actors blinks.\",\n",
       " 'Many senators who confused Carmen find Lissa.',\n",
       " 'A lot of actors that aggravated Edward see some candles.',\n",
       " \"A driver who hadn't left the Clintons respects Cindy.\",\n",
       " 'That actress who was impressing those libraries entreats that offspring to think about a mushroom.',\n",
       " 'This boy that noticed some actresses reveals Dan.',\n",
       " 'This pedestrian who insulted the teachers gets a job.',\n",
       " \"A girl who wouldn't distract the Impressionists sighs.\",\n",
       " 'The pedestrian that sounds like all hospitals anticipates Natalie to read.',\n",
       " \"The bicycles that hadn't irritated a committee impress Craig.\",\n",
       " 'Senators who have worried Aaron want to clean some ice cream.',\n",
       " 'Some rabbit that was annoying the dancers ascends most stairs.',\n",
       " \"A legislature that wouldn't respect some drivers yearns to remember Rebecca.\",\n",
       " 'Organizations that argued about literature aggravate Candice.',\n",
       " 'A lot of adults who distract Sally sound like Sarah.',\n",
       " 'Every student that had kissed drivers gets a job.',\n",
       " \"The girl who hadn't insulted all waitresses smiles.\",\n",
       " \"That waiter who hadn't described hospitals urges Alice's brothers to come here.\",\n",
       " 'Some teacher who has bored many girls murmurs.',\n",
       " \"The dancers that aren't disturbing Debra hide.\",\n",
       " 'That school that has bothered a lot of girls talks about some boy.',\n",
       " 'That patient who respected the Clintons reveals every high school.',\n",
       " 'Some guy who notices those students wakes up.',\n",
       " \"The customer that hasn't upset the Lutherans hunts.\",\n",
       " 'That waitress that had approached all customers climbs up the ladders.',\n",
       " 'These guys who could leave Julie walk through those mountains.',\n",
       " 'Some customer that escapes from most waitresses wears some shoe.',\n",
       " \"This guy who wasn't noticing children messes up this hospital.\",\n",
       " \"Most horses that aren't boring Elizabeth come here.\",\n",
       " 'These drivers that had left Benjamin write.',\n",
       " 'All hospitals that are firing Emily like to talk to Nicole.',\n",
       " 'This bird that found those hamsters hides.',\n",
       " 'Some waitress who disagrees with teenagers flees from Scott.',\n",
       " 'That dancer that flees from the Impressionists regrets those men reference those movies.',\n",
       " 'Some driver that embarrassed these governments figures out who notice those gates.',\n",
       " 'This pedestrian who finds these children escapes from Scott.',\n",
       " 'All libraries that were aggravating a legislature joke around.',\n",
       " 'The student that stunned many pedestrians figures out who wears these sweaters.',\n",
       " 'A guy that references a lot of customers proves to shatter.',\n",
       " 'Some committee that was disturbing those committees helps Veronica.',\n",
       " 'The organization that has shocked these ladies motivates Katherine to forfeit.',\n",
       " 'This girl who has attacked a lot of drivers lies.',\n",
       " 'A teenager that approaches a lot of men scratches.',\n",
       " 'Dancers who stuns a student compromise.',\n",
       " 'A lot of actresses who did astound this actress sit down.',\n",
       " 'Teenagers who can work with Kathleen observe Tonya.',\n",
       " 'A company that concealed all cashiers forgets about what shrank.',\n",
       " 'All banks that upset Kristen want this river to vaporize.',\n",
       " 'This museum that is bothering a lot of drivers threatens to bother Melinda.',\n",
       " \"The students that hadn't confused Jennifer leave.\",\n",
       " 'Every pedestrian who disgusted all ladies smiles.',\n",
       " \"Some governments that hadn't talked to Paula happen to move.\",\n",
       " 'Most museums that had described Gary want the Clintons to vanish.',\n",
       " 'Many students who took the sock confuse that child.',\n",
       " 'A lot of libraries that listened to the man forget the commentaries.',\n",
       " \"Some ladies who haven't irritated Beverly kiss Sara.\",\n",
       " 'Many boys who dislike Dawn talk.',\n",
       " 'The customers who respected Angela dislike Deanna.',\n",
       " \"Every cashier who wasn't visiting those senators salutes.\",\n",
       " 'That teacher that is alarming the museums cooperates.',\n",
       " 'This customer who has praised all ladies criticizes Rhonda.',\n",
       " 'A shoe that annoys a lot of guys upsets the Borgias.',\n",
       " \"That senator who wasn't boring all customers doubts Joel forgot about who isn't complaining.\",\n",
       " 'This driver that has described many dancers hides.',\n",
       " 'The doctor that has impressed these teachers laughs.',\n",
       " 'A customer who kisses all boys murmurs.',\n",
       " 'These dancers that sound like Debra run around this art gallery.',\n",
       " 'A school that does annoy a lot of customers judges that doctor to exit Spain.',\n",
       " \"These guests that didn't discuss every senator cure Amy.\",\n",
       " 'A boy who has talked about many teenagers bothers to joke around.',\n",
       " 'That waitress who was appreciating a lot of cashiers wears a lot of skirts.',\n",
       " \"This teacher that wouldn't shock these teenagers says George distracted Monet.\",\n",
       " 'The casserole that scares these ladies bakes.',\n",
       " 'A lot of teachers who referenced Eva break all bikes.',\n",
       " \"These actors that haven't investigated Cheryl paint.\",\n",
       " 'Those students that worried Stephen predict Sheila to wash.',\n",
       " 'Most actors who have hurt Dan skate around a hill.',\n",
       " 'A lady who was observing many patients escapes from those hills.',\n",
       " 'The doctor that reveals the Borgias researches this lake.',\n",
       " 'This teacher who bothered the guests drops by the hospitals.',\n",
       " 'A pedestrian who is worrying the Impressionists wins.',\n",
       " 'That teenager that talks about the Borgias tours the library.',\n",
       " 'Many committees that will respect Charles induce Adam to look like most pictures.',\n",
       " 'Some guys who dislike Janet neglect to bother Steve.',\n",
       " 'A lot of waiters that sounded like Beth exercise.',\n",
       " \"Those schools that wouldn't distract a senator discover who isn't helping Gerald.\",\n",
       " 'That teacher that attacked these students forgets who cried.',\n",
       " 'These ladies that are hugging Noah escape from Jennifer.',\n",
       " 'Those shoes that distracted Martin stretch.',\n",
       " \"This adult who didn't astound the doctors gets fired.\",\n",
       " 'Some organization that helps a lot of customers likes Edward.',\n",
       " \"Some teenager that hasn't observed drivers drives to a bank.\",\n",
       " \"The dancer who hasn't embarrassed those guests conceals many gloves.\",\n",
       " \"The cashier that doesn't work with all pedestrians doesn't explain these ladies hate the customer.\",\n",
       " 'Some committee that has known the adults judges the adults to compete.',\n",
       " \"The organization that does know most actors reveals who hasn't kissed Anna.\",\n",
       " 'That teacher that can reference a lot of adults protests.',\n",
       " \"These forks that hadn't alarmed Brett exist.\",\n",
       " 'An actress who had upset all museums fires Helen.',\n",
       " \"Many waitresses who aren't kissing Jennifer cook.\",\n",
       " \"Waitresses that can't disturb Danielle shout.\",\n",
       " \"These waiters that aren't scaring Emily escape from that adult.\",\n",
       " 'This actress who is observing those guys laughs.',\n",
       " 'Every library that is boring some boys describes closets.',\n",
       " \"These teachers that weren't insulting Dawn salute.\",\n",
       " 'Some waitress that left the hospitals neglects to talk to Larry.',\n",
       " 'Those boys that might reference Bruce work hard.',\n",
       " 'Committees that have embarrassed a government have approved.',\n",
       " 'These organizations that insulted Marcus compel Donald to sneeze.',\n",
       " 'Every adult that does know the Lutherans complains about the commentaries.',\n",
       " 'The children that saw Regina think those waitresses mutter.',\n",
       " \"A lot of doctors who won't heal Bill drink.\",\n",
       " 'The waiter who does heal some actors messes up those pants.',\n",
       " \"Some pedestrian that couldn't bother all waitresses paints.\",\n",
       " 'This university that was distracting a lot of actresses appears to talk to Winston Churchill.',\n",
       " 'A student that can approach these pedestrians visits Adam.',\n",
       " \"Some doctor who wasn't hugging some pedestrians kisses Thomas.\",\n",
       " 'Some dancer who would disgust all adults sneezes.',\n",
       " \"A teacher that shouldn't hurt ladies concurs.\",\n",
       " \"Those teenagers that weren't finding Kristen anticipate every couch to crack.\",\n",
       " \"A guest that isn't discussing all schools respects the hill.\",\n",
       " 'Many libraries that scared Edward conceal Dana.',\n",
       " 'Every teacher that will kiss most adults worries.',\n",
       " 'A lot of boys who fire Victoria telephone.',\n",
       " 'The university that disturbs these legislatures tries to compete.',\n",
       " \"The customers that haven't upset Amelia prevent many vases from vanishing.\",\n",
       " 'Those teenagers who complain about Catherine play.',\n",
       " 'Some boys who do discover Julia mess up a lot of pants.',\n",
       " 'These guys that visited this senator drink.',\n",
       " 'That waitress that cared for a lot of patients tours the college campus.',\n",
       " \"The organization that hasn't bothered some universities prods Sherry to complain.\",\n",
       " 'That committee that is criticizing some adults knows Sonia.',\n",
       " \"Some teachers who wouldn't stun Noah lift these skirts.\",\n",
       " 'Every university that alarms many governments explains everything.',\n",
       " 'Every waiter that has discussed children drives to Spain.',\n",
       " 'Some adult who has helped a lot of patients wears this skirt.',\n",
       " 'Patients that will hurt Gerald know the Impressionists.',\n",
       " \"Every waitress who isn't irritating a lot of waitresses drops by most cafes.\",\n",
       " \"Some waitress that isn't listening to these customers prevents those photographs from wasting away.\",\n",
       " \"Some guests who hadn't visited Catherine find Dana.\",\n",
       " \"The waiter that hadn't disturbed all waiters tours this closet.\",\n",
       " 'This dancer who loves women hides away.',\n",
       " 'Some boys that were praising Eric chuckle.',\n",
       " 'Senators who will insult Deanna conceal those teachers.',\n",
       " \"The committee that hadn't talked about some women learns who can't believe Ellen to answer.\",\n",
       " \"A lot of dancers that won't escape from Frank appear to melt.\",\n",
       " 'The adult that hurt a lot of boys complains about those documentaries.',\n",
       " \"That government that had fired the men urges Homer's French teachers to fire Melinda.\",\n",
       " 'Most senators that love Melinda come here.',\n",
       " \"A lot of paintings that aren't looking like some carriage stun Omar.\",\n",
       " 'All patients that alarm Kimberley concur.',\n",
       " 'Some senator that questioned some committees turns out to hate Erin.',\n",
       " 'These organizations that should boast about Rhonda judge the rivers to condense.',\n",
       " \"Some patient who can't exit these museums continues to weep.\",\n",
       " 'The bird that upsets these guests wastes away.',\n",
       " 'Some committees that worry Kendra dislike a lot of women.',\n",
       " \"A lot of associations that can't disturb that government prefer the Impressionists to hate ladies.\",\n",
       " \"Those drivers who hadn't hurt Patricia sigh.\",\n",
       " 'Those boys that worry Dana refuse to correspond.',\n",
       " \"The actress who isn't firing those senators slumps over.\",\n",
       " 'That association that has impressed a lot of men proves to work hard.',\n",
       " 'Most pedestrians who thought about Amelia exercise.',\n",
       " 'This driver who is kissing drivers presses Lisa to upset Benjamin.',\n",
       " \"Some senator who hadn't disliked the actors drives to a lot of rivers.\",\n",
       " 'The company that confuses those committees ascertains the Borgias to retaliate.',\n",
       " 'A lot of girls that have cured Lucille yearn to answer.',\n",
       " \"The customer who hadn't visited all men researches that library.\",\n",
       " 'Those teachers who escaped from Theodore explain everything.',\n",
       " 'Every patient that has returned to many girls departs.',\n",
       " 'That horse that finds many cats falls asleep.',\n",
       " 'These museums that had upset Angela motivate Claire to fall asleep.',\n",
       " 'The libraries that have bothered Andrew oblige Paul to admire this bicycle.',\n",
       " \"A guest who couldn't hug the men goes fishing.\",\n",
       " 'All universities that were distracting Todd fail to flirt.',\n",
       " \"Every government that confuses most organizations investigates who hasn't learned who isn't refusing to help Rose.\",\n",
       " 'All guests who flee from Steven think John turned out to slump over.',\n",
       " \"All cashiers that weren't revealing Kristen cook.\",\n",
       " 'A senator that has hated a lot of teachers weeps.',\n",
       " 'This pedestrian that worried the Lutherans stands up.',\n",
       " 'Every committee that does praise those doctors wonders who sees.',\n",
       " \"Those students who might distract Chad remember who isn't passing this cafe.\",\n",
       " 'All teachers that helped Jason disturb Regina.',\n",
       " 'A lot of teachers that visit a woman boycott Harvard University.',\n",
       " \"The patient who investigates these boys finds out who won't telephone.\",\n",
       " \"The actresses who haven't healed Tonya die.\",\n",
       " 'These guys who can investigate Andrea approve.',\n",
       " 'Most dancers that observe Ann turn out to crack.',\n",
       " 'That committee that disgusted many patients reacts.',\n",
       " 'A waiter that had bothered those people heals Sarah.',\n",
       " \"Many students that aren't aggravating this museum entreat many associations to hate Claire.\",\n",
       " 'This bank that embarrasses some teachers moves.',\n",
       " \"Many dancers who won't return to Samuel escape from some person.\",\n",
       " 'That turtle that annoys many doctors climbs down most hills.',\n",
       " \"The senators who haven't known Tara heal Naomi.\",\n",
       " 'Many lakes that had bothered Kenneth exist.',\n",
       " \"Most waiters who weren't talking to this man appreciate Andrew.\",\n",
       " 'Many teenagers that hate Noah ascertain that mushroom to rot.',\n",
       " 'The guys that were talking to the waitress were learning.',\n",
       " 'A waitress that alarmed these actresses figures out who changed.',\n",
       " \"Many students that haven't discovered Natalie clean all couches.\",\n",
       " 'Many actresses who did find Lori surrender.',\n",
       " 'Some senator who has scared most committees plans to distract Dan.',\n",
       " 'Many girls who hide Veronica benefit.',\n",
       " \"A teacher who hadn't fled from these waiters fixes that bicycle.\",\n",
       " 'Some dancers that were leaving Scott need to return to Gregory.',\n",
       " 'Many cashiers that can discuss Frank slump over.',\n",
       " 'The cashiers who left Brad protest.',\n",
       " \"A senator that wasn't leaving a lot of teachers conceals who weren't noticing Russell.\",\n",
       " \"Every teacher that wasn't finding some hamsters hides.\",\n",
       " \"That dog that can't aggravate those guys wakes up.\",\n",
       " 'Most boys that boasted about Mary bike to those schools.',\n",
       " \"Many girls that didn't disturb Peter long to kiss.\",\n",
       " 'The adult who had listened to those students suffers.',\n",
       " \"The pedestrians who can't discover Bruce pass many museums.\",\n",
       " \"This senator that didn't flee from the Clintons replies.\",\n",
       " 'A cashier that should flee from most patients practices.',\n",
       " 'Many boys that confused William wash.',\n",
       " 'Every girl who admired all women whispers.',\n",
       " 'Waitresses that talked about Caroline stand up.',\n",
       " 'Some driver who watches most actors telephones.',\n",
       " \"These companies that hadn't worried Wendy listen to most guests.\",\n",
       " 'Dancers who know Lori sigh.',\n",
       " \"This waitress who hasn't aggravated those doctors cures Gina.\",\n",
       " 'Some museums that were disgusting Elizabeth fail to hide away.',\n",
       " \"A lot of guests that aren't questioning Ella scare the schools.\",\n",
       " \"Many customers that alarm Alexander encourage Steven's grandmothers to flee from Sonia.\",\n",
       " 'This cashier who admires all adults drops by hospitals.',\n",
       " 'All patients that hurts Steven sing.',\n",
       " 'Many ladies who could hug Lori examine most books.',\n",
       " 'Many waiters who were returning to Victoria study.',\n",
       " \"Those pedestrians that aren't impressing Caroline blink.\",\n",
       " \"A waiter that isn't thinking about many people writes.\",\n",
       " \"Those cashiers who couldn't watch Christine beg every niece of Tiffany to disagree with all doctors.\",\n",
       " \"That waiter that hasn't complained about guests prods Sheila's teacher to perform.\",\n",
       " \"The teenager that shouldn't bore some women coughs.\",\n",
       " 'The bank that should discuss a lot of women urges Aaron to attack Deborah.',\n",
       " 'A lot of senators who have described Kenneth wave.',\n",
       " \"Many customers who couldn't find Timothy long to hunt.\",\n",
       " 'Some organizations that respected Sally oblige that customer to cure Galileo.',\n",
       " 'The guys that annoy the organization reference Keith.',\n",
       " 'The doctor that is approaching actors finds out what slow.',\n",
       " 'Some teachers that left Angela work with Tina.',\n",
       " 'The driver that had shocked the Clintons waves.',\n",
       " 'A doctor that is discovering the Lutherans litters.',\n",
       " 'Some doctors who cured Sonia ask Deanna to perform.',\n",
       " 'The senators that did scare every legislature propose to get a job.',\n",
       " \"These senators who wouldn't like Christina weep.\",\n",
       " 'Some teenagers who have insulted Alexander exercise.',\n",
       " 'Every waitress that is working with many children ascends some steps.',\n",
       " 'Some teenagers that irritated the boy reply.',\n",
       " \"The legislature that wasn't astounding all actresses hopes to boast.\",\n",
       " 'Those customers that talk to that actress spin around.',\n",
       " 'All hospitals that do bother Kirsten do motivate Lissa to remember these waitresses.',\n",
       " 'The actor that had watched many teenagers litters.',\n",
       " 'A lot of dishes that had worried Kayla resemble the photograph.',\n",
       " 'The bank that is disturbing most organizations boasts.',\n",
       " \"Children who would reference this driver advise Karen's daughters to hate these essays.\",\n",
       " \"A waiter that can't appreciate a lot of people slumps over.\",\n",
       " 'Many actors who would cure Steve fail to spin around.',\n",
       " 'These patients that upset Diana explore the mountain.',\n",
       " \"Many customers that haven't approached Dan scan the newspaper article.\",\n",
       " 'Some waitresses who had hired Stacey appreciate Alexander.',\n",
       " 'A doctor that investigated these adults reveals all children.',\n",
       " 'Governments that praise Melissa commission the daughters of this senator to blink.',\n",
       " 'Those dancers that find Dennis walk through a public park.',\n",
       " 'Many girls who should help Becca donate.',\n",
       " 'Some bank that talked to the Lutherans intends to remember Holly.',\n",
       " 'That driver who is scaring dancers boycotts the organization.',\n",
       " 'Many students who have insulted Janet attack Bradley.',\n",
       " 'Some actor who returns to a lot of cashiers scratches.',\n",
       " 'Adults who can disagree with Paula go fishing.',\n",
       " 'The legislatures that might know Danielle investigate who advises this doctor to climb up all slopes.',\n",
       " 'The teacher that leaves a lot of teenagers prevents Russell from boring Tanya.',\n",
       " 'A doctor who questions some committees pressures Robert to shout.',\n",
       " 'Most companies that discussed Brenda prefer a lot of jackets to warp.',\n",
       " \"A hospital that isn't bothering men ascertains Debra to drop by this art gallery.\",\n",
       " \"The actors that haven't referenced Lori prevent every senator from listening to Renee.\",\n",
       " 'Many children that will talk to Danielle worry.',\n",
       " 'Some company that hurts all boys forgets about who was learning.',\n",
       " \"That waiter who won't approach some drivers sees Ella.\",\n",
       " \"Some association that isn't upsetting all governments attempts to donate.\",\n",
       " 'The children who forget Rachel discuss Natalie.',\n",
       " \"That waiter who isn't talking about these men judges Amanda to describe some cafes.\",\n",
       " 'Every government that has disgusted many doctors listens to that woman.',\n",
       " 'A customer that loved waiters urges Curtis to murmur.',\n",
       " 'Those skateboards that had bored Noah disappear.',\n",
       " 'Every adult who will bore some ladies presses this teenager to stun William.',\n",
       " 'Photographs that were worrying Michelle look like the dancer.',\n",
       " \"That girl that hadn't cared for these dancers gets a job.\",\n",
       " 'Some waiter who astounded all ladies stands up.',\n",
       " 'That dancer who praised these dancers doubts all children consider Wayne to hire Steve.',\n",
       " 'That school that did embarrass the Clintons needs to sell the unicycle.',\n",
       " 'This actress that knows most actors forces Suzanne to explore the cafe.',\n",
       " 'Many doctors who did notice Michelle bake.',\n",
       " 'That teenager who is kissing most cashiers breaks many planes.',\n",
       " 'A lot of girls who are arguing about Paul flee from Rachel.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e33c75f76f64ce4b189a385d1bd5f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 15:34:04 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-06-10 15:34:06 INFO: File exists: /afs/cs.stanford.edu/u/ananthag/stanza_resources/en/default.zip\n",
      "2024-06-10 15:34:14 INFO: Finished downloading models and saved to /afs/cs.stanford.edu/u/ananthag/stanza_resources.\n",
      "2024-06-10 15:34:14 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6f132568254222850b32f75e308b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 15:34:20 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus                   |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-06-10 15:34:20 INFO: Using device: cuda\n",
      "2024-06-10 15:34:20 INFO: Loading: tokenize\n",
      "2024-06-10 15:34:20 INFO: Loading: mwt\n",
      "2024-06-10 15:34:20 INFO: Loading: pos\n",
      "2024-06-10 15:34:20 INFO: Loading: lemma\n",
      "2024-06-10 15:34:20 INFO: Loading: constituency\n",
      "2024-06-10 15:34:20 INFO: Loading: depparse\n",
      "2024-06-10 15:34:21 INFO: Loading: sentiment\n",
      "2024-06-10 15:34:21 INFO: Loading: ner\n",
      "2024-06-10 15:34:21 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "stanza.download(\"en\")\n",
    "nlp = stanza.Pipeline(\"en\", tokenize_pretokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The sketch of those trucks has n't hurt Alan .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The', 'sketch', 'of', 'those', 'trucks', 'has', \"n't\", 'hurt', 'Alan', '.'],\n",
       " [2, 8, 5, 5, 2, 8, 8, 0, 8, 8],\n",
       " ['det',\n",
       "  'nsubj',\n",
       "  'case',\n",
       "  'det',\n",
       "  'nmod',\n",
       "  'aux',\n",
       "  'advmod',\n",
       "  'root',\n",
       "  'obj',\n",
       "  'punct'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "heads = []\n",
    "relns = []\n",
    "for dep_edge in doc.sentences[0].dependencies:\n",
    "    words.append(dep_edge[2].text)\n",
    "    heads.append(dep_edge[0].id)\n",
    "    relns.append(dep_edge[1])\n",
    "\n",
    "words, heads, relns\n",
    "# for word, head, reln in zip(words, heads, relns):\n",
    "#     if reln == \"nsubj\":\n",
    "#         for w, h, r in zip(words, heads, relns):\n",
    "#             if w!= word and h == head and r == \"aux\":\n",
    "#                 print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c96bfed700548618d838131ea8c1f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/syntax/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e64ae32252e4d85b4ee214178107dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1aacff9955e4ff795713ea0c385433f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa27c92a7224e1da95291ec83c0cfd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "print(roberta_tokenizer.decode([479]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'T5Tokenizer' object has no attribute 'decoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39m_shift_right(labels)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'T5Tokenizer' object has no attribute 'decoder'"
     ]
    }
   ],
   "source": [
    "model._shift_right(labels)\n",
    "tokenizer.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "folder_path = os.path.join(\n",
    "        \"/scr/biggest/ananthag\",\n",
    "        \"hidden_states\",\n",
    "        \"recogs\",\n",
    "        \"gpt2\",\n",
    "    )\n",
    "\n",
    "class COGSDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.srcs = []\n",
    "        self.tgts = []\n",
    "        self.start_indices = []\n",
    "        total_words = 0\n",
    "        for l in open(f\"ReCOGS/recogs_v2/{split}.tsv\", \"r\").readlines():\n",
    "            text, sparse, _ = l.split(\"\\t\")\n",
    "            self.srcs.append(text)\n",
    "            self.tgts.append(sparse)\n",
    "            self.start_indices.append(total_words)\n",
    "            tokenized_text = tokenizer(text, return_tensors=\"pt\")\n",
    "            total_words += tokenized_text.input_ids.shape[1]\n",
    "\n",
    "        self.hidden_state_cache = np.memmap(\n",
    "            os.path.join(folder_path, f\"{split}.dat\"),\n",
    "            dtype=np.float32,\n",
    "            mode=\"r\",\n",
    "            shape=(total_words, 768),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.start_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_index = self.start_indices[idx]\n",
    "        if idx < 0:\n",
    "            idx = len(self.start_indices) + idx\n",
    "        end_index = self.start_indices[idx + 1] if idx + 1 < len(self.start_indices) else self.hidden_state_cache.shape[0]\n",
    "        print(start_index, end_index)\n",
    "        return self.hidden_state_cache[start_index : end_index], self.srcs, self.tgts[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12\n",
      "12 22\n",
      "('Liam ( 22 ) ; box ( 18 ) ; girl ( 45 ) ; hope ( 27 ) AND agent ( 27 , 22 ) AND ccomp ( 27 , 37 ) AND burn ( 37 ) AND theme ( 37 , 18 ) AND agent ( 37 , 45 )', '* donkey ( 47 ) ; * cookie ( 11 ) ; mother ( 0 ) ; lend ( 22 ) AND agent ( 22 , 47 ) AND theme ( 22 , 11 ) AND recipient ( 22 , 0 )')\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    max_dim_0 = 0\n",
    "    hidden_size = None\n",
    "    for item in batch:\n",
    "        max_dim_0 = max(max_dim_0, item[0].shape[0])\n",
    "        hidden_size = item[0].shape[1] \n",
    "    assert max_dim_0\n",
    "    srcs = []\n",
    "    tgts = []\n",
    "    padded_input = torch.zeros((len(batch), max_dim_0, hidden_size))\n",
    "    for i, item in enumerate(batch):\n",
    "        padded_input[i, : item[0].shape[0]] = torch.as_tensor(item[0])\n",
    "        srcs.append(item[1])\n",
    "        tgts.append(item[2])\n",
    "    return padded_input, tuple(srcs), tuple(tgts)\n",
    "\n",
    "dataloader = DataLoader(COGSDataset(\"dev\"), batch_size=2, shuffle=False, collate_fn=custom_collate_fn)\n",
    "for batch in dataloader:\n",
    "    hidden_states, srcs, tgts = batch\n",
    "    print(tgts)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Liam hoped that a box was burned by a girl .', 'The donkey lended the cookie to a mother .']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import train_recogs\n",
    "import utils\n",
    "\n",
    "dataset = train_recogs.COGSDataset(\"dev\")\n",
    "\n",
    "# model = utils.get_model(\"gpt2\")\n",
    "# tokenizer = utils.get_tokenizer(\"gpt2\")\n",
    "\n",
    "first_srcs = [dataset[0][0], dataset[1][0]]\n",
    "print(first_srcs)\n",
    "\n",
    "inputs = tokenizer(first_srcs, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "torch.eq(outputs.last_hidden_state * inputs.attention_mask.unsqueeze(-1), outputs.hidden_states[-1]).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# all_hidden_states = torch.cat(outputs.hidden_states, dim=0)\n",
    "# print(all_hidden_states.shape)\n",
    "\n",
    "# tokenizer(\"hello world\", return_tensors=\"pt\", padding='max_length', max_length=20)\n",
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22,\n",
       " ('Daniel said that the cat said that a giraffe rented the girl a radio in a swamp on a table .',\n",
       "  'Daniel ( 3 ) ; * cat ( 41 ) ; giraffe ( 27 ) ; * girl ( 31 ) ; radio ( 34 ) ; swamp ( 14 ) ; table ( 28 ) ; say ( 44 ) AND agent ( 44 , 3 ) AND ccomp ( 44 , 45 ) AND say ( 45 ) AND agent ( 45 , 41 ) AND ccomp ( 45 , 42 ) AND rent ( 42 ) AND agent ( 42 , 27 ) AND recipient ( 42 , 31 ) AND theme ( 42 , 34 ) AND nmod . in ( 34 , 14 ) AND nmod . on ( 14 , 28 )'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_token_length = 0\n",
    "max_item = None\n",
    "for item in dataset:\n",
    "    tokenized = tokenizer(item[0], return_tensors=\"pt\")\n",
    "    max_token_length = max(max_token_length, tokenized.input_ids.shape[1])\n",
    "    if max_token_length == tokenized.input_ids.shape[1]:\n",
    "        max_item = item\n",
    "max_token_length, max_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "data = {\n",
    "    'A': 1.5,\n",
    "    'B': 2.0,\n",
    "    'C': 3.5,\n",
    "    'D': 0.5,\n",
    "    'E': 2.5\n",
    "}\n",
    "\n",
    "# Extract keys and values\n",
    "keys = list(data.keys())\n",
    "values = list(data.values())\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 2))\n",
    "\n",
    "# Plot a horizontal line at y=0\n",
    "ax.plot(values, [0] * len(values), 'bo')  # 'bo' for blue color and circle markers\n",
    "ax.hlines(0, min(values) - 0.5, max(values) + 0.5, colors='black', linewidth=1)\n",
    "import numpy as np\n",
    "# Set the x-ticks to the values\n",
    "ax.set_xticks(np.arange(min(values), max(values) + 1, 1))\n",
    "# ax.set_xticklabels(values)\n",
    "\n",
    "# Remove y-axis and add some padding to x-axis\n",
    "ax.yaxis.set_visible(False)\n",
    "plt.xlim(min(values) - 0.5, max(values) + 0.5)\n",
    "\n",
    "# Add labels\n",
    "for key, value in data.items():\n",
    "    ax.text(value, 0.01, key, ha='center', va='bottom')\n",
    "\n",
    "plt.title('1D Line Plot of Key-Value Pairs')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "text = \"Liam ( 54 ) ; Emma ( 37 ) ; * lion ( 2 ) ; donut ( 12 ) ; cot ( 25 ) ; Sebastian ( 31 ) ; hope ( 41 ) AND agent ( 41 , 54 ) AND ccomp ( 41 , 10 ) AND like ( 10 ) AND agent ( 10 , 37 ) AND ccomp ( 10 , 49 ) AND pass ( 49 ) AND agent ( 49 , 2 ) AND theme ( 49 , 12 ) AND recipient ( 49 , 31 ) AND nmod . in ( 12 , 25 )\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 109])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2(text, return_tensors=\"pt\").input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_recogs import COGSDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Paula painted a cake in a closet .',\n",
       " 'Paula ( 8 ) ; cake ( 18 ) ; closet ( 31 ) ; paint ( 22 ) AND agent ( 22 , 8 ) AND theme ( 22 , 18 ) AND nmod . in ( 18 , 31 )',\n",
       " 'prim_to_subj_proper\\n')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class COGSDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.srcs = []\n",
    "        self.tgts = []\n",
    "        self.categoreis = []\n",
    "        self.start_indices = []\n",
    "        total_words = 0\n",
    "        for l in open(f\"ReCOGS/recogs_v2/{split}.tsv\", \"r\").readlines():\n",
    "            text, sparse, category = l.split(\"\\t\")\n",
    "            self.srcs.append(text)\n",
    "            self.tgts.append(sparse)\n",
    "            self.categoreis.append(category)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.srcs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # start_index = self.start_indices[idx]\n",
    "        # if idx < 0:\n",
    "        #     idx = len(self.start_indices) + idx\n",
    "        # end_index = (\n",
    "        #     self.start_indices[idx + 1]\n",
    "        #     if idx + 1 < len(self.start_indices)\n",
    "        #     else len(self.srcs)\n",
    "        # )\n",
    "        return (\n",
    "            self.srcs[idx],\n",
    "            self.tgts[idx],\n",
    "            self.categoreis[idx],\n",
    "        )\n",
    "\n",
    "dataset = COGSDataset(\"gen\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syntax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
