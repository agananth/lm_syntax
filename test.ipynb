{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Head Word Final 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {}\n",
    "for run in api.runs(\n",
    "    path=\"ananthag/Head Word Final 2\"\n",
    "):\n",
    "    runs[run.config['model_name']] = run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37018"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.convert_tokens_to_ids('bite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeGenTokenizerFast(name_or_path='microsoft/phi-2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50257: AddedToken(\"                               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50258: AddedToken(\"                              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50259: AddedToken(\"                             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50260: AddedToken(\"                            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50261: AddedToken(\"                           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50262: AddedToken(\"                          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50263: AddedToken(\"                         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50264: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50265: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50266: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50267: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50268: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50269: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50270: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50271: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50272: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50273: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50274: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50275: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50276: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50277: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50278: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50279: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50280: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50281: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50282: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50283: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50284: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50285: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50286: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50287: AddedToken(\"\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50288: AddedToken(\"\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50289: AddedToken(\"\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50290: AddedToken(\"\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50291: AddedToken(\"\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50292: AddedToken(\"\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50293: AddedToken(\"\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50294: AddedToken(\"\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(4532)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.codegen.tokenization_codegen_fast.CodeGenTokenizerFast"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tok.encode(' adjust', add_prefix_space=False)\n",
    "type(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not tok.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"Hey what's your name\", \"Welcome home\", \"text\" * 20]\n",
    "tok.pad_token_id = tok.eos_token_id\n",
    "inputs_dict = tok.batch_encode_plus(inputs, padding=True, return_tensors='pt')\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "m = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "out = m(**inputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs_dict[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = inputs_dict[\"attention_mask\"]\n",
    "\n",
    "last_non_masked_idx = torch.sum(attn_mask, dim=1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.tensor(\n",
    "            [list(range(inputs.shape[1])) for i in range(inputs.shape[0])]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, position_ids_slice in enumerate(position_ids):\n",
    "    position_ids_slice[last_non_masked_idx[i] :] = position_ids_slice[\n",
    "        last_non_masked_idx[i]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(inputs.shape[1]).repeat(inputs.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[attn_mask] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected index [3, 20] to be smaller than self [3, 20] apart from dimension 0 and to be smaller size than src [3, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_non_masked_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m inputs\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected index [3, 20] to be smaller than self [3, 20] apart from dimension 0 and to be smaller size than src [3, 1]"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs.scatter_(0, 1 - attn_mask, last_non_masked_idx.unsqueeze(1))\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs * attn_mask + (~attn_mask.bool() * last_non_masked_idx.unsqueeze(1).expand(-1, inputs.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10814,   644,   338,   534,  1438,     4,     4,     4,     4,     4,\n",
       "             4,     4,     4,     4,     4,     4,     4,     4,     4,     4],\n",
       "        [14618,  1363,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [ 5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,\n",
       "          5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "from dataset import DepParseDataPickle\n",
    "\n",
    "d = dataset.HeadWordDatasetWithRelns('test', 'gpt2', 12, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2416"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.start_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58223"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.start_indices[2415]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58236, 12, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.hidden_state_cache.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.70710678],\n",
       "       [0.70710678, 1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "X = [[0, 0, 0], [1, 1, 1]]\n",
    "Y = [[1, 0, 0], [1, 1, 0]]\n",
    "cosine_similarity(Y, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ran = np.random.rand(3, 3)\n",
    "ran[np.eye(3, 3, dtype=bool)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "counter = collections.Counter()\n",
    "models = set()\n",
    "with open('surprisals_errata.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        counter[(row['file_name'], row['item_number'])] += 1\n",
    "        models.add(row['model'])\n",
    "\n",
    "len(models)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('reflexive_src_fem.json', '6'), 15),\n",
       " (('reflexive_src_fem.json', '7'), 15),\n",
       " (('reflexive_src_fem.json', '13'), 15),\n",
       " (('reflexive_src_fem.json', '16'), 15),\n",
       " (('reflexive_prep_fem.json', '1'), 15),\n",
       " (('reflexive_prep_fem.json', '2'), 15),\n",
       " (('reflexive_src_fem.json', '1'), 14),\n",
       " (('reflexive_src_fem.json', '5'), 14),\n",
       " (('reflexive_src_fem.json', '15'), 14),\n",
       " (('reflexive_src_fem.json', '19'), 14),\n",
       " (('reflexive_prep_fem.json', '17'), 14),\n",
       " (('fgd-embed3.json', '4'), 14),\n",
       " (('fgd_subject.json', '4'), 14),\n",
       " (('fgd_subject.json', '5'), 14),\n",
       " (('fgd_subject.json', '9'), 14),\n",
       " (('fgd_subject.json', '10'), 14),\n",
       " (('mvrr.json', '5'), 14),\n",
       " (('mvrr_mod.json', '5'), 14),\n",
       " (('number_orc.json', '5'), 14),\n",
       " (('number_orc.json', '17'), 14),\n",
       " (('number_prep.json', '5'), 14),\n",
       " (('number_prep.json', '17'), 14),\n",
       " (('number_src.json', '5'), 14),\n",
       " (('number_src.json', '17'), 14),\n",
       " (('reflexive_orc_fem.json', '1'), 14),\n",
       " (('reflexive_orc_fem.json', '5'), 14),\n",
       " (('reflexive_orc_fem.json', '6'), 14),\n",
       " (('reflexive_orc_fem.json', '7'), 14),\n",
       " (('reflexive_orc_fem.json', '15'), 14),\n",
       " (('reflexive_src_fem.json', '9'), 14),\n",
       " (('fgd_subject.json', '12'), 14),\n",
       " (('reflexive_src_fem.json', '2'), 13),\n",
       " (('reflexive_src_fem.json', '17'), 13),\n",
       " (('reflexive_prep_fem.json', '15'), 13),\n",
       " (('fgd-embed4.json', '15'), 13),\n",
       " (('fgd-embed4.json', '16'), 13),\n",
       " (('number_orc.json', '16'), 13),\n",
       " (('reflexive_orc_fem.json', '11'), 13),\n",
       " (('reflexive_orc_fem.json', '17'), 13),\n",
       " (('fgd_hierarchy.json', '1'), 13),\n",
       " (('reflexive_prep_masc.json', '2'), 13),\n",
       " (('mvrr_mod.json', '16'), 13),\n",
       " (('reflexive_src_masc.json', '9'), 13),\n",
       " (('reflexive_prep_fem.json', '7'), 12),\n",
       " (('reflexive_prep_fem.json', '16'), 12),\n",
       " (('fgd_subject.json', '6'), 12),\n",
       " (('mvrr.json', '14'), 12),\n",
       " (('number_orc.json', '3'), 12),\n",
       " (('reflexive_orc_fem.json', '13'), 12),\n",
       " (('fgd_pp.json', '6'), 12),\n",
       " (('mvrr.json', '20'), 12),\n",
       " (('mvrr.json', '16'), 12),\n",
       " (('fgd-embed3.json', '16'), 11),\n",
       " (('subordination.json', '12'), 11),\n",
       " (('mvrr_mod.json', '19'), 11),\n",
       " (('number_src.json', '3'), 11),\n",
       " (('reflexive_orc_fem.json', '16'), 11),\n",
       " (('reflexive_orc_fem.json', '19'), 11),\n",
       " (('npi_src_any.json', '38'), 11),\n",
       " (('fgd_subject.json', '7'), 11),\n",
       " (('fgd_subject.json', '19'), 11),\n",
       " (('fgd_hierarchy.json', '19'), 10),\n",
       " (('subordination_src-src.json', '9'), 10),\n",
       " (('reflexive_prep_fem.json', '5'), 10),\n",
       " (('fgd-embed3.json', '21'), 10),\n",
       " (('subordination.json', '9'), 10),\n",
       " (('subordination.json', '11'), 10),\n",
       " (('subordination.json', '19'), 10),\n",
       " (('fgd-embed4.json', '3'), 10),\n",
       " (('fgd-embed4.json', '4'), 10),\n",
       " (('fgd-embed4.json', '9'), 10),\n",
       " (('fgd-embed4.json', '21'), 10),\n",
       " (('reflexive_src_masc.json', '19'), 10),\n",
       " (('fgd_object.json', '16'), 10),\n",
       " (('reflexive_src_fem.json', '14'), 10),\n",
       " (('reflexive_prep_fem.json', '9'), 10),\n",
       " (('reflexive_prep_fem.json', '8'), 10),\n",
       " (('fgd_subject.json', '11'), 10),\n",
       " (('fgd_subject.json', '23'), 10),\n",
       " (('reflexive_src_fem.json', '4'), 9),\n",
       " (('reflexive_prep_fem.json', '6'), 9),\n",
       " (('reflexive_prep_fem.json', '13'), 9),\n",
       " (('fgd-embed3.json', '9'), 9),\n",
       " (('subordination.json', '16'), 9),\n",
       " (('fgd_subject.json', '1'), 9),\n",
       " (('mvrr.json', '19'), 9),\n",
       " (('reflexive_src_masc.json', '6'), 9),\n",
       " (('reflexive_src_masc.json', '11'), 9),\n",
       " (('number_src.json', '15'), 9),\n",
       " (('reflexive_orc_fem.json', '8'), 9),\n",
       " (('reflexive_orc_fem.json', '9'), 9),\n",
       " (('fgd_hierarchy.json', '4'), 9),\n",
       " (('fgd_hierarchy.json', '12'), 9),\n",
       " (('fgd-embed3.json', '6'), 9),\n",
       " (('fgd-embed3.json', '8'), 9),\n",
       " (('reflexive_prep_masc.json', '9'), 9),\n",
       " (('fgd-embed4.json', '12'), 9),\n",
       " (('fgd-embed4.json', '8'), 9),\n",
       " (('fgd_hierarchy.json', '7'), 9),\n",
       " (('fgd_hierarchy.json', '11'), 8)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    GPTNeoXTokenizerFast,\n",
    "    LlamaTokenizer,\n",
    "    GPT2Tokenizer,\n",
    "    GemmaTokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaTokenizerFast(name_or_path='google/gemma-2b', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [\"cat\", \"dog\", \"cow\", \"bird\"]\n",
    "model_name = 'google/gemma-2b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast='pythia' in 'google')\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# batch = tokenizer((\"one word\", \"two two two words\", \"three three three three three words\"), padding=True, return_tensors='pt')\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n",
    "# output = model(**batch)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  505,  1573, 50256, 50256, 50256, 50256],\n",
       "        [11545,   734,   734,  2456, 50256, 50256],\n",
       "        [15542,  1115,  1115,  1115,  1115,  2456]]), 'attention_mask': tensor([[1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  505,  1573, 50256, 50256, 50256, 50256],\n",
       "        [11545,   734,   734,  2456, 50256, 50256],\n",
       "        [15542,  1115,  1115,  1115,  1115,  2456]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.attention_mask.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-17.2175, -27.2265, -33.0799], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "targets = torch.cat((batch[\"input_ids\"][:, 1:], torch.tensor([tokenizer.eos_token_id]).expand(batch[\"input_ids\"].shape[0], 1)), dim=1)\n",
    "out = torch.gather(F.log_softmax(output.logits, dim=-1), -1, targets.unsqueeze(-1))\n",
    "(out * batch.attention_mask.unsqueeze(-1)).view(batch.input_ids.shape[0], -1).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['animate_subject_passive.jsonl',\n",
       " 'principle_A_domain_3.jsonl',\n",
       " 'npi_present_1.jsonl',\n",
       " 'wh_vs_that_no_gap_long_distance.jsonl',\n",
       " 'existential_there_quantifiers_1.jsonl',\n",
       " 'determiner_noun_agreement_irregular_1.jsonl',\n",
       " 'ellipsis_n_bar_1.jsonl',\n",
       " 'causative.jsonl',\n",
       " 'wh_questions_object_gap.jsonl',\n",
       " 'transitive.jsonl',\n",
       " 'superlative_quantifiers_1.jsonl',\n",
       " 'tough_vs_raising_2.jsonl',\n",
       " 'principle_A_case_1.jsonl',\n",
       " 'expletive_it_object_raising.jsonl',\n",
       " 'irregular_plural_subject_verb_agreement_2.jsonl',\n",
       " 'principle_A_domain_2.jsonl',\n",
       " 'sentential_negation_npi_scope.jsonl',\n",
       " 'ellipsis_n_bar_2.jsonl',\n",
       " 'determiner_noun_agreement_irregular_2.jsonl',\n",
       " 'left_branch_island_echo_question.jsonl',\n",
       " 'existential_there_quantifiers_2.jsonl',\n",
       " 'wh_island.jsonl',\n",
       " 'superlative_quantifiers_2.jsonl',\n",
       " 'principle_A_domain_1.jsonl',\n",
       " 'irregular_plural_subject_verb_agreement_1.jsonl',\n",
       " 'distractor_agreement_relative_clause.jsonl',\n",
       " 'existential_there_subject_raising.jsonl',\n",
       " 'tough_vs_raising_1.jsonl',\n",
       " 'principle_A_case_2.jsonl',\n",
       " 'coordinate_structure_constraint_object_extraction.jsonl',\n",
       " 'wh_vs_that_no_gap.jsonl',\n",
       " 'anaphor_gender_agreement.jsonl',\n",
       " 'determiner_noun_agreement_with_adj_2.jsonl',\n",
       " 'only_npi_scope.jsonl',\n",
       " 'inchoative.jsonl',\n",
       " 'wh_vs_that_with_gap_long_distance.jsonl',\n",
       " 'npi_present_2.jsonl',\n",
       " 'coordinate_structure_constraint_complex_left_branch.jsonl',\n",
       " 'left_branch_island_simple_question.jsonl',\n",
       " 'determiner_noun_agreement_with_adj_irregular_2.jsonl',\n",
       " 'determiner_noun_agreement_2.jsonl',\n",
       " 'wh_vs_that_with_gap.jsonl',\n",
       " 'regular_plural_subject_verb_agreement_1.jsonl',\n",
       " 'passive_2.jsonl',\n",
       " 'anaphor_number_agreement.jsonl',\n",
       " 'animate_subject_trans.jsonl',\n",
       " 'intransitive.jsonl',\n",
       " 'irregular_past_participle_verbs.jsonl',\n",
       " 'only_npi_licensor_present.jsonl',\n",
       " 'passive_1.jsonl',\n",
       " 'sentential_subject_island.jsonl',\n",
       " 'regular_plural_subject_verb_agreement_2.jsonl',\n",
       " 'principle_A_c_command.jsonl',\n",
       " 'wh_questions_subject_gap.jsonl',\n",
       " 'sentential_negation_npi_licensor_present.jsonl',\n",
       " 'principle_A_reconstruction.jsonl',\n",
       " 'determiner_noun_agreement_with_adjective_1.jsonl',\n",
       " 'drop_argument.jsonl',\n",
       " 'existential_there_object_raising.jsonl',\n",
       " 'irregular_past_participle_adjectives.jsonl',\n",
       " 'adjunct_island.jsonl',\n",
       " 'matrix_question_npi_licensor_present.jsonl',\n",
       " 'distractor_agreement_relational_noun.jsonl',\n",
       " 'complex_NP_island.jsonl',\n",
       " 'wh_questions_subject_gap_long_distance.jsonl',\n",
       " 'determiner_noun_agreement_1.jsonl',\n",
       " 'determiner_noun_agreement_with_adj_irregular_1.jsonl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../blimp/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.4905, -6.7212, -6.6658, -6.3376], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = torch.cat(\n",
    "    (\n",
    "        batch.input_ids[:, 1:],\n",
    "        torch.tensor([[tokenizer.eos_token_id]]).expand(\n",
    "            batch.input_ids.shape[0], 1\n",
    "        ),\n",
    "    ),\n",
    "    dim=-1,\n",
    ")\n",
    "logprobs = torch.gather(\n",
    "    torch.nn.functional.log_softmax(output.logits, dim=-1),\n",
    "    dim=-1,\n",
    "    index=targets.unsqueeze(1),\n",
    ").reshape(-1)\n",
    "logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(logprobs > logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256],\n",
       "        [50256],\n",
       "        [50256],\n",
       "        [50256]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[tokenizer.eos_token_id]]).expand(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92989cfd2446428999e353de76b84785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaConfig {\n",
       "  \"_name_or_path\": \"google/gemma-2b\",\n",
       "  \"architectures\": [\n",
       "    \"GemmaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"head_dim\": 256,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 16384,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"model_type\": \"gemma\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 18,\n",
       "  \"num_key_value_heads\": 1,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 256000\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModelForCausalLM.from_pretrained('google/gemma-2b').config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.41.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191cfe3dc3db42589749e788d9dfce4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db203ad8506467aa419b0af9a628583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733c0e0d90ec490b97f8cbd93917276e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254e59aead874acc995a2ad10194db98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56dc4b34438644c1b297ed0c6e630f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0271308e33a041a7943394d116f80d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "olmo = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-1B-hf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [12092, 1533], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1533], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\" world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n",
      "Pseudoperplexity: -18.2569522857666\n",
      "torch.Size([1, 6])\n",
      "Pseudoperplexity: -30.995452880859375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_pseudoperplexity(model, tokenizer, text):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids']\n",
    "    print(input_ids.shape)\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    # Get the length of the sequence\n",
    "    seq_length = input_ids.size(1)\n",
    "    \n",
    "    # Create batched inputs with each token masked once\n",
    "    masked_input_ids = input_ids.repeat(seq_length, 1)\n",
    "    \n",
    "    masked_attention_mask = attention_mask.repeat(seq_length, 1)\n",
    "    masked_input_ids.fill_diagonal_(tokenizer.mask_token_id)\n",
    "    \n",
    "    # Get the model's predictions for the batched masked inputs\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(masked_input_ids, attention_mask=masked_attention_mask)\n",
    "    \n",
    "    # Extract the logits and compute the log softmax to get log probabilities\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the actual token IDs\n",
    "    actual_token_ids = input_ids[0]\n",
    "\n",
    "    # Extract the log probabilities of the actual tokens\n",
    "    token_log_probs = log_probs[torch.arange(seq_length), torch.arange(seq_length), actual_token_ids]\n",
    "    \n",
    "    # Compute the sum of log probabilities\n",
    "    return token_log_probs.sum().item()\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'FacebookAI/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Example text\n",
    "text = \"The quick brown fox jumped.\"\n",
    "\n",
    "# Compute pseudoperplexity\n",
    "print(f\"Pseudoperplexity: {compute_pseudoperplexity(model, tokenizer, 'He is cool.')}\")\n",
    "print(f\"Pseudoperplexity: {compute_pseudoperplexity(model, tokenizer, 'He are cool.')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40bfb0012ea4b8b85f3ce3d2f4ba977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b276f07a1be4c7a950452c8cc120820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, DebertaV2ForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained('microsoft/deberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta.embeddings.word_embeddings.weight\n",
      "deberta.embeddings.LayerNorm.weight\n",
      "deberta.embeddings.LayerNorm.bias\n",
      "deberta.encoder.layer.0.attention.self.q_bias\n",
      "deberta.encoder.layer.0.attention.self.v_bias\n",
      "deberta.encoder.layer.0.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.0.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.0.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.0.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.0.attention.output.dense.weight\n",
      "deberta.encoder.layer.0.attention.output.dense.bias\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.0.intermediate.dense.weight\n",
      "deberta.encoder.layer.0.intermediate.dense.bias\n",
      "deberta.encoder.layer.0.output.dense.weight\n",
      "deberta.encoder.layer.0.output.dense.bias\n",
      "deberta.encoder.layer.0.output.LayerNorm.weight\n",
      "deberta.encoder.layer.0.output.LayerNorm.bias\n",
      "deberta.encoder.layer.1.attention.self.q_bias\n",
      "deberta.encoder.layer.1.attention.self.v_bias\n",
      "deberta.encoder.layer.1.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.1.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.1.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.1.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.1.attention.output.dense.weight\n",
      "deberta.encoder.layer.1.attention.output.dense.bias\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.1.intermediate.dense.weight\n",
      "deberta.encoder.layer.1.intermediate.dense.bias\n",
      "deberta.encoder.layer.1.output.dense.weight\n",
      "deberta.encoder.layer.1.output.dense.bias\n",
      "deberta.encoder.layer.1.output.LayerNorm.weight\n",
      "deberta.encoder.layer.1.output.LayerNorm.bias\n",
      "deberta.encoder.layer.2.attention.self.q_bias\n",
      "deberta.encoder.layer.2.attention.self.v_bias\n",
      "deberta.encoder.layer.2.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.2.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.2.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.2.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.2.attention.output.dense.weight\n",
      "deberta.encoder.layer.2.attention.output.dense.bias\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.2.intermediate.dense.weight\n",
      "deberta.encoder.layer.2.intermediate.dense.bias\n",
      "deberta.encoder.layer.2.output.dense.weight\n",
      "deberta.encoder.layer.2.output.dense.bias\n",
      "deberta.encoder.layer.2.output.LayerNorm.weight\n",
      "deberta.encoder.layer.2.output.LayerNorm.bias\n",
      "deberta.encoder.layer.3.attention.self.q_bias\n",
      "deberta.encoder.layer.3.attention.self.v_bias\n",
      "deberta.encoder.layer.3.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.3.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.3.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.3.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.3.attention.output.dense.weight\n",
      "deberta.encoder.layer.3.attention.output.dense.bias\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.3.intermediate.dense.weight\n",
      "deberta.encoder.layer.3.intermediate.dense.bias\n",
      "deberta.encoder.layer.3.output.dense.weight\n",
      "deberta.encoder.layer.3.output.dense.bias\n",
      "deberta.encoder.layer.3.output.LayerNorm.weight\n",
      "deberta.encoder.layer.3.output.LayerNorm.bias\n",
      "deberta.encoder.layer.4.attention.self.q_bias\n",
      "deberta.encoder.layer.4.attention.self.v_bias\n",
      "deberta.encoder.layer.4.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.4.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.4.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.4.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.4.attention.output.dense.weight\n",
      "deberta.encoder.layer.4.attention.output.dense.bias\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.4.intermediate.dense.weight\n",
      "deberta.encoder.layer.4.intermediate.dense.bias\n",
      "deberta.encoder.layer.4.output.dense.weight\n",
      "deberta.encoder.layer.4.output.dense.bias\n",
      "deberta.encoder.layer.4.output.LayerNorm.weight\n",
      "deberta.encoder.layer.4.output.LayerNorm.bias\n",
      "deberta.encoder.layer.5.attention.self.q_bias\n",
      "deberta.encoder.layer.5.attention.self.v_bias\n",
      "deberta.encoder.layer.5.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.5.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.5.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.5.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.5.attention.output.dense.weight\n",
      "deberta.encoder.layer.5.attention.output.dense.bias\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.5.intermediate.dense.weight\n",
      "deberta.encoder.layer.5.intermediate.dense.bias\n",
      "deberta.encoder.layer.5.output.dense.weight\n",
      "deberta.encoder.layer.5.output.dense.bias\n",
      "deberta.encoder.layer.5.output.LayerNorm.weight\n",
      "deberta.encoder.layer.5.output.LayerNorm.bias\n",
      "deberta.encoder.layer.6.attention.self.q_bias\n",
      "deberta.encoder.layer.6.attention.self.v_bias\n",
      "deberta.encoder.layer.6.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.6.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.6.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.6.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.6.attention.output.dense.weight\n",
      "deberta.encoder.layer.6.attention.output.dense.bias\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.6.intermediate.dense.weight\n",
      "deberta.encoder.layer.6.intermediate.dense.bias\n",
      "deberta.encoder.layer.6.output.dense.weight\n",
      "deberta.encoder.layer.6.output.dense.bias\n",
      "deberta.encoder.layer.6.output.LayerNorm.weight\n",
      "deberta.encoder.layer.6.output.LayerNorm.bias\n",
      "deberta.encoder.layer.7.attention.self.q_bias\n",
      "deberta.encoder.layer.7.attention.self.v_bias\n",
      "deberta.encoder.layer.7.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.7.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.7.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.7.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.7.attention.output.dense.weight\n",
      "deberta.encoder.layer.7.attention.output.dense.bias\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.7.intermediate.dense.weight\n",
      "deberta.encoder.layer.7.intermediate.dense.bias\n",
      "deberta.encoder.layer.7.output.dense.weight\n",
      "deberta.encoder.layer.7.output.dense.bias\n",
      "deberta.encoder.layer.7.output.LayerNorm.weight\n",
      "deberta.encoder.layer.7.output.LayerNorm.bias\n",
      "deberta.encoder.layer.8.attention.self.q_bias\n",
      "deberta.encoder.layer.8.attention.self.v_bias\n",
      "deberta.encoder.layer.8.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.8.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.8.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.8.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.8.attention.output.dense.weight\n",
      "deberta.encoder.layer.8.attention.output.dense.bias\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.8.intermediate.dense.weight\n",
      "deberta.encoder.layer.8.intermediate.dense.bias\n",
      "deberta.encoder.layer.8.output.dense.weight\n",
      "deberta.encoder.layer.8.output.dense.bias\n",
      "deberta.encoder.layer.8.output.LayerNorm.weight\n",
      "deberta.encoder.layer.8.output.LayerNorm.bias\n",
      "deberta.encoder.layer.9.attention.self.q_bias\n",
      "deberta.encoder.layer.9.attention.self.v_bias\n",
      "deberta.encoder.layer.9.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.9.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.9.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.9.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.9.attention.output.dense.weight\n",
      "deberta.encoder.layer.9.attention.output.dense.bias\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.9.intermediate.dense.weight\n",
      "deberta.encoder.layer.9.intermediate.dense.bias\n",
      "deberta.encoder.layer.9.output.dense.weight\n",
      "deberta.encoder.layer.9.output.dense.bias\n",
      "deberta.encoder.layer.9.output.LayerNorm.weight\n",
      "deberta.encoder.layer.9.output.LayerNorm.bias\n",
      "deberta.encoder.layer.10.attention.self.q_bias\n",
      "deberta.encoder.layer.10.attention.self.v_bias\n",
      "deberta.encoder.layer.10.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.10.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.10.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.10.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.10.attention.output.dense.weight\n",
      "deberta.encoder.layer.10.attention.output.dense.bias\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.10.intermediate.dense.weight\n",
      "deberta.encoder.layer.10.intermediate.dense.bias\n",
      "deberta.encoder.layer.10.output.dense.weight\n",
      "deberta.encoder.layer.10.output.dense.bias\n",
      "deberta.encoder.layer.10.output.LayerNorm.weight\n",
      "deberta.encoder.layer.10.output.LayerNorm.bias\n",
      "deberta.encoder.layer.11.attention.self.q_bias\n",
      "deberta.encoder.layer.11.attention.self.v_bias\n",
      "deberta.encoder.layer.11.attention.self.in_proj.weight\n",
      "deberta.encoder.layer.11.attention.self.pos_proj.weight\n",
      "deberta.encoder.layer.11.attention.self.pos_q_proj.weight\n",
      "deberta.encoder.layer.11.attention.self.pos_q_proj.bias\n",
      "deberta.encoder.layer.11.attention.output.dense.weight\n",
      "deberta.encoder.layer.11.attention.output.dense.bias\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "deberta.encoder.layer.11.intermediate.dense.weight\n",
      "deberta.encoder.layer.11.intermediate.dense.bias\n",
      "deberta.encoder.layer.11.output.dense.weight\n",
      "deberta.encoder.layer.11.output.dense.bias\n",
      "deberta.encoder.layer.11.output.LayerNorm.weight\n",
      "deberta.encoder.layer.11.output.LayerNorm.bias\n",
      "deberta.encoder.rel_embeddings.weight\n",
      "cls.predictions.bias\n",
      "cls.predictions.transform.dense.weight\n",
      "cls.predictions.transform.dense.bias\n",
      "cls.predictions.transform.LayerNorm.weight\n",
      "cls.predictions.transform.LayerNorm.bias\n",
      "cls.predictions.decoder.weight\n",
      "cls.predictions.decoder.bias\n"
     ]
    }
   ],
   "source": [
    "for k in model.state_dict().keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scr/biggest/ananthag\n"
     ]
    }
   ],
   "source": [
    "!echo $HF_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6d219d74254083b5664f333b4609c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e0815223bb4b69b5931bc1b244c3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoModelForMaskedLM\n",
    "\n",
    "roberta = AutoModelForMaskedLM.from_pretrained('FacebookAI/roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1476, -0.0365,  0.0753,  ..., -0.0023,  0.0172, -0.0016],\n",
       "        [ 0.0156,  0.0076, -0.0118,  ..., -0.0022,  0.0081, -0.0156],\n",
       "        [-0.0347, -0.0873, -0.0180,  ...,  0.1174, -0.0098, -0.0355],\n",
       "        ...,\n",
       "        [ 0.0304,  0.0504, -0.0307,  ...,  0.0377,  0.0096,  0.0084],\n",
       "        [ 0.0623, -0.0596,  0.0307,  ..., -0.0920,  0.1080, -0.0183],\n",
       "        [ 0.1259, -0.0145,  0.0332,  ...,  0.0121,  0.0342,  0.0168]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta.lm_head.decoder.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "state_dict = torch.load('/scr/ananthag/hub/models--microsoft--deberta-v3-xsmall/snapshots/4b419818330868dff6a60ad3e6b1c730f8b8c0c6/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta.embeddings.word_embeddings._weight torch.Size([128100, 384])\n",
      "deberta.embeddings.word_embeddings.weight torch.Size([128100, 384])\n",
      "deberta.embeddings.position_embeddings._weight torch.Size([512, 384])\n",
      "deberta.embeddings.position_embeddings.weight torch.Size([512, 384])\n",
      "deberta.embeddings.LayerNorm.weight torch.Size([384])\n",
      "deberta.embeddings.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.0.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.0.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.0.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.1.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.1.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.1.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.2.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.2.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.2.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.3.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.3.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.3.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.4.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.4.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.4.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.5.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.5.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.5.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.6.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.6.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.6.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.7.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.7.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.7.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.8.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.8.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.8.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.9.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.9.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.9.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.10.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.10.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.10.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.11.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.11.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.11.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.rel_embeddings.weight torch.Size([512, 384])\n",
      "deberta.encoder.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.LayerNorm.bias torch.Size([384])\n",
      "lm_predictions.lm_head.bias torch.Size([128100])\n",
      "lm_predictions.lm_head.dense.weight torch.Size([384, 384])\n",
      "lm_predictions.lm_head.dense.bias torch.Size([384])\n",
      "lm_predictions.lm_head.LayerNorm.weight torch.Size([384])\n",
      "lm_predictions.lm_head.LayerNorm.bias torch.Size([384])\n",
      "mask_predictions.dense.weight torch.Size([384, 384])\n",
      "mask_predictions.dense.bias torch.Size([384])\n",
      "mask_predictions.LayerNorm.weight torch.Size([384])\n",
      "mask_predictions.LayerNorm.bias torch.Size([384])\n",
      "mask_predictions.classifier.weight torch.Size([1, 384])\n",
      "mask_predictions.classifier.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for k, v in state_dict.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta.embeddings.word_embeddings.weight torch.Size([128100, 384])\n",
      "deberta.embeddings.LayerNorm.weight torch.Size([384])\n",
      "deberta.embeddings.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.0.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.0.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.0.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.0.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.0.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.1.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.1.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.1.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.1.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.1.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.2.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.2.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.2.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.2.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.2.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.3.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.3.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.3.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.3.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.3.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.4.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.4.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.4.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.4.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.4.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.5.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.5.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.5.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.5.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.5.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.6.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.6.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.6.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.6.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.6.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.7.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.7.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.7.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.7.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.7.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.8.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.8.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.8.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.8.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.8.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.9.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.9.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.9.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.9.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.9.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.10.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.10.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.10.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.10.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.10.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.query_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.key_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.self.value_proj.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.dense.weight torch.Size([384, 384])\n",
      "deberta.encoder.layer.11.attention.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.intermediate.dense.weight torch.Size([1536, 384])\n",
      "deberta.encoder.layer.11.intermediate.dense.bias torch.Size([1536])\n",
      "deberta.encoder.layer.11.output.dense.weight torch.Size([384, 1536])\n",
      "deberta.encoder.layer.11.output.dense.bias torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.layer.11.output.LayerNorm.bias torch.Size([384])\n",
      "deberta.encoder.rel_embeddings.weight torch.Size([512, 384])\n",
      "deberta.encoder.LayerNorm.weight torch.Size([384])\n",
      "deberta.encoder.LayerNorm.bias torch.Size([384])\n",
      "cls.predictions.bias torch.Size([128100])\n",
      "cls.predictions.transform.dense.weight torch.Size([384, 384])\n",
      "cls.predictions.transform.dense.bias torch.Size([384])\n",
      "cls.predictions.transform.LayerNorm.weight torch.Size([384])\n",
      "cls.predictions.transform.LayerNorm.bias torch.Size([384])\n",
      "cls.predictions.decoder.weight torch.Size([128100, 384])\n",
      "cls.predictions.decoder.bias torch.Size([128100])\n"
     ]
    }
   ],
   "source": [
    "for k, v in deberta.state_dict().items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.bias torch.Size([128100])\n",
      "predictions.transform.dense.weight torch.Size([384, 384])\n",
      "predictions.transform.dense.bias torch.Size([384])\n",
      "predictions.transform.LayerNorm.weight torch.Size([384])\n",
      "predictions.transform.LayerNorm.bias torch.Size([384])\n",
      "predictions.decoder.weight torch.Size([128100, 384])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.cls.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ef8c9282c64fb28b318114dc3248e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef13e7f92cf47ffa8d9d0fb53168ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196587104f2f462389b97177784fd537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32099,   296,     1]]), 'attention_mask': tensor([[1, 1, 1]])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello <extra_id_0>\", return_tensors='pt', padding=True)\n",
    "labels = tokenizer(\"<extra_id_0> world\", return_tensors='pt', padding=True).input_ids\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syntax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
