{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Head Word Final 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {}\n",
    "for run in api.runs(\n",
    "    path=\"ananthag/Head Word Final 2\"\n",
    "):\n",
    "    runs[run.config['model_name']] = run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37018"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.convert_tokens_to_ids('bite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeGenTokenizerFast(name_or_path='microsoft/phi-2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50257: AddedToken(\"                               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50258: AddedToken(\"                              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50259: AddedToken(\"                             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50260: AddedToken(\"                            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50261: AddedToken(\"                           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50262: AddedToken(\"                          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50263: AddedToken(\"                         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50264: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50265: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50266: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50267: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50268: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50269: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50270: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50271: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50272: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50273: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50274: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50275: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50276: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50277: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50278: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50279: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50280: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50281: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50282: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50283: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50284: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50285: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50286: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50287: AddedToken(\"\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50288: AddedToken(\"\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50289: AddedToken(\"\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50290: AddedToken(\"\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50291: AddedToken(\"\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50292: AddedToken(\"\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50293: AddedToken(\"\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50294: AddedToken(\"\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(4532)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.codegen.tokenization_codegen_fast.CodeGenTokenizerFast"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tok.encode(' adjust', add_prefix_space=False)\n",
    "type(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not tok.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"Hey what's your name\", \"Welcome home\", \"text\" * 20]\n",
    "tok.pad_token_id = tok.eos_token_id\n",
    "inputs_dict = tok.batch_encode_plus(inputs, padding=True, return_tensors='pt')\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "m = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "out = m(**inputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs_dict[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = inputs_dict[\"attention_mask\"]\n",
    "\n",
    "last_non_masked_idx = torch.sum(attn_mask, dim=1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.tensor(\n",
    "            [list(range(inputs.shape[1])) for i in range(inputs.shape[0])]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, position_ids_slice in enumerate(position_ids):\n",
    "    position_ids_slice[last_non_masked_idx[i] :] = position_ids_slice[\n",
    "        last_non_masked_idx[i]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(inputs.shape[1]).repeat(inputs.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[attn_mask] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected index [3, 20] to be smaller than self [3, 20] apart from dimension 0 and to be smaller size than src [3, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_non_masked_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m inputs\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected index [3, 20] to be smaller than self [3, 20] apart from dimension 0 and to be smaller size than src [3, 1]"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs.scatter_(0, 1 - attn_mask, last_non_masked_idx.unsqueeze(1))\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs * attn_mask + (~attn_mask.bool() * last_non_masked_idx.unsqueeze(1).expand(-1, inputs.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10814,   644,   338,   534,  1438,     4,     4,     4,     4,     4,\n",
       "             4,     4,     4,     4,     4,     4,     4,     4,     4,     4],\n",
       "        [14618,  1363,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [ 5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,\n",
       "          5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239,  5239]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "from dataset import DepParseDataPickle\n",
    "\n",
    "d = dataset.HeadWordDatasetWithRelns('test', 'gpt2', 12, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2416"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.start_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58223"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.start_indices[2415]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58236, 12, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.hidden_state_cache.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.70710678],\n",
       "       [0.70710678, 1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "X = [[0, 0, 0], [1, 1, 1]]\n",
    "Y = [[1, 0, 0], [1, 1, 0]]\n",
    "cosine_similarity(Y, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ran = np.random.rand(3, 3)\n",
    "ran[np.eye(3, 3, dtype=bool)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "counter = collections.Counter()\n",
    "models = set()\n",
    "with open('surprisals_errata.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        counter[(row['file_name'], row['item_number'])] += 1\n",
    "        models.add(row['model'])\n",
    "\n",
    "len(models)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('reflexive_src_fem.json', '6'), 15),\n",
       " (('reflexive_src_fem.json', '7'), 15),\n",
       " (('reflexive_src_fem.json', '13'), 15),\n",
       " (('reflexive_src_fem.json', '16'), 15),\n",
       " (('reflexive_prep_fem.json', '1'), 15),\n",
       " (('reflexive_prep_fem.json', '2'), 15),\n",
       " (('reflexive_src_fem.json', '1'), 14),\n",
       " (('reflexive_src_fem.json', '5'), 14),\n",
       " (('reflexive_src_fem.json', '15'), 14),\n",
       " (('reflexive_src_fem.json', '19'), 14),\n",
       " (('reflexive_prep_fem.json', '17'), 14),\n",
       " (('fgd-embed3.json', '4'), 14),\n",
       " (('fgd_subject.json', '4'), 14),\n",
       " (('fgd_subject.json', '5'), 14),\n",
       " (('fgd_subject.json', '9'), 14),\n",
       " (('fgd_subject.json', '10'), 14),\n",
       " (('mvrr.json', '5'), 14),\n",
       " (('mvrr_mod.json', '5'), 14),\n",
       " (('number_orc.json', '5'), 14),\n",
       " (('number_orc.json', '17'), 14),\n",
       " (('number_prep.json', '5'), 14),\n",
       " (('number_prep.json', '17'), 14),\n",
       " (('number_src.json', '5'), 14),\n",
       " (('number_src.json', '17'), 14),\n",
       " (('reflexive_orc_fem.json', '1'), 14),\n",
       " (('reflexive_orc_fem.json', '5'), 14),\n",
       " (('reflexive_orc_fem.json', '6'), 14),\n",
       " (('reflexive_orc_fem.json', '7'), 14),\n",
       " (('reflexive_orc_fem.json', '15'), 14),\n",
       " (('reflexive_src_fem.json', '9'), 14),\n",
       " (('fgd_subject.json', '12'), 14),\n",
       " (('reflexive_src_fem.json', '2'), 13),\n",
       " (('reflexive_src_fem.json', '17'), 13),\n",
       " (('reflexive_prep_fem.json', '15'), 13),\n",
       " (('fgd-embed4.json', '15'), 13),\n",
       " (('fgd-embed4.json', '16'), 13),\n",
       " (('number_orc.json', '16'), 13),\n",
       " (('reflexive_orc_fem.json', '11'), 13),\n",
       " (('reflexive_orc_fem.json', '17'), 13),\n",
       " (('fgd_hierarchy.json', '1'), 13),\n",
       " (('reflexive_prep_masc.json', '2'), 13),\n",
       " (('mvrr_mod.json', '16'), 13),\n",
       " (('reflexive_src_masc.json', '9'), 13),\n",
       " (('reflexive_prep_fem.json', '7'), 12),\n",
       " (('reflexive_prep_fem.json', '16'), 12),\n",
       " (('fgd_subject.json', '6'), 12),\n",
       " (('mvrr.json', '14'), 12),\n",
       " (('number_orc.json', '3'), 12),\n",
       " (('reflexive_orc_fem.json', '13'), 12),\n",
       " (('fgd_pp.json', '6'), 12),\n",
       " (('mvrr.json', '20'), 12),\n",
       " (('mvrr.json', '16'), 12),\n",
       " (('fgd-embed3.json', '16'), 11),\n",
       " (('subordination.json', '12'), 11),\n",
       " (('mvrr_mod.json', '19'), 11),\n",
       " (('number_src.json', '3'), 11),\n",
       " (('reflexive_orc_fem.json', '16'), 11),\n",
       " (('reflexive_orc_fem.json', '19'), 11),\n",
       " (('npi_src_any.json', '38'), 11),\n",
       " (('fgd_subject.json', '7'), 11),\n",
       " (('fgd_subject.json', '19'), 11),\n",
       " (('fgd_hierarchy.json', '19'), 10),\n",
       " (('subordination_src-src.json', '9'), 10),\n",
       " (('reflexive_prep_fem.json', '5'), 10),\n",
       " (('fgd-embed3.json', '21'), 10),\n",
       " (('subordination.json', '9'), 10),\n",
       " (('subordination.json', '11'), 10),\n",
       " (('subordination.json', '19'), 10),\n",
       " (('fgd-embed4.json', '3'), 10),\n",
       " (('fgd-embed4.json', '4'), 10),\n",
       " (('fgd-embed4.json', '9'), 10),\n",
       " (('fgd-embed4.json', '21'), 10),\n",
       " (('reflexive_src_masc.json', '19'), 10),\n",
       " (('fgd_object.json', '16'), 10),\n",
       " (('reflexive_src_fem.json', '14'), 10),\n",
       " (('reflexive_prep_fem.json', '9'), 10),\n",
       " (('reflexive_prep_fem.json', '8'), 10),\n",
       " (('fgd_subject.json', '11'), 10),\n",
       " (('fgd_subject.json', '23'), 10),\n",
       " (('reflexive_src_fem.json', '4'), 9),\n",
       " (('reflexive_prep_fem.json', '6'), 9),\n",
       " (('reflexive_prep_fem.json', '13'), 9),\n",
       " (('fgd-embed3.json', '9'), 9),\n",
       " (('subordination.json', '16'), 9),\n",
       " (('fgd_subject.json', '1'), 9),\n",
       " (('mvrr.json', '19'), 9),\n",
       " (('reflexive_src_masc.json', '6'), 9),\n",
       " (('reflexive_src_masc.json', '11'), 9),\n",
       " (('number_src.json', '15'), 9),\n",
       " (('reflexive_orc_fem.json', '8'), 9),\n",
       " (('reflexive_orc_fem.json', '9'), 9),\n",
       " (('fgd_hierarchy.json', '4'), 9),\n",
       " (('fgd_hierarchy.json', '12'), 9),\n",
       " (('fgd-embed3.json', '6'), 9),\n",
       " (('fgd-embed3.json', '8'), 9),\n",
       " (('reflexive_prep_masc.json', '9'), 9),\n",
       " (('fgd-embed4.json', '12'), 9),\n",
       " (('fgd-embed4.json', '8'), 9),\n",
       " (('fgd_hierarchy.json', '7'), 9),\n",
       " (('fgd_hierarchy.json', '11'), 8)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    GPTNeoXTokenizerFast,\n",
    "    LlamaTokenizer,\n",
    "    GPT2Tokenizer,\n",
    "    GemmaTokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaTokenizerFast(name_or_path='google/gemma-2b', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [\"cat\", \"dog\", \"cow\", \"bird\"]\n",
    "model_name = 'google/gemma-2b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast='pythia' in 'google')\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# batch = tokenizer((\"one word\", \"two two two words\", \"three three three three three words\"), padding=True, return_tensors='pt')\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n",
    "# output = model(**batch)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  505,  1573, 50256, 50256, 50256, 50256],\n",
       "        [11545,   734,   734,  2456, 50256, 50256],\n",
       "        [15542,  1115,  1115,  1115,  1115,  2456]]), 'attention_mask': tensor([[1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  505,  1573, 50256, 50256, 50256, 50256],\n",
       "        [11545,   734,   734,  2456, 50256, 50256],\n",
       "        [15542,  1115,  1115,  1115,  1115,  2456]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.attention_mask.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-17.2175, -27.2265, -33.0799], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "targets = torch.cat((batch[\"input_ids\"][:, 1:], torch.tensor([tokenizer.eos_token_id]).expand(batch[\"input_ids\"].shape[0], 1)), dim=1)\n",
    "out = torch.gather(F.log_softmax(output.logits, dim=-1), -1, targets.unsqueeze(-1))\n",
    "(out * batch.attention_mask.unsqueeze(-1)).view(batch.input_ids.shape[0], -1).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['animate_subject_passive.jsonl',\n",
       " 'principle_A_domain_3.jsonl',\n",
       " 'npi_present_1.jsonl',\n",
       " 'wh_vs_that_no_gap_long_distance.jsonl',\n",
       " 'existential_there_quantifiers_1.jsonl',\n",
       " 'determiner_noun_agreement_irregular_1.jsonl',\n",
       " 'ellipsis_n_bar_1.jsonl',\n",
       " 'causative.jsonl',\n",
       " 'wh_questions_object_gap.jsonl',\n",
       " 'transitive.jsonl',\n",
       " 'superlative_quantifiers_1.jsonl',\n",
       " 'tough_vs_raising_2.jsonl',\n",
       " 'principle_A_case_1.jsonl',\n",
       " 'expletive_it_object_raising.jsonl',\n",
       " 'irregular_plural_subject_verb_agreement_2.jsonl',\n",
       " 'principle_A_domain_2.jsonl',\n",
       " 'sentential_negation_npi_scope.jsonl',\n",
       " 'ellipsis_n_bar_2.jsonl',\n",
       " 'determiner_noun_agreement_irregular_2.jsonl',\n",
       " 'left_branch_island_echo_question.jsonl',\n",
       " 'existential_there_quantifiers_2.jsonl',\n",
       " 'wh_island.jsonl',\n",
       " 'superlative_quantifiers_2.jsonl',\n",
       " 'principle_A_domain_1.jsonl',\n",
       " 'irregular_plural_subject_verb_agreement_1.jsonl',\n",
       " 'distractor_agreement_relative_clause.jsonl',\n",
       " 'existential_there_subject_raising.jsonl',\n",
       " 'tough_vs_raising_1.jsonl',\n",
       " 'principle_A_case_2.jsonl',\n",
       " 'coordinate_structure_constraint_object_extraction.jsonl',\n",
       " 'wh_vs_that_no_gap.jsonl',\n",
       " 'anaphor_gender_agreement.jsonl',\n",
       " 'determiner_noun_agreement_with_adj_2.jsonl',\n",
       " 'only_npi_scope.jsonl',\n",
       " 'inchoative.jsonl',\n",
       " 'wh_vs_that_with_gap_long_distance.jsonl',\n",
       " 'npi_present_2.jsonl',\n",
       " 'coordinate_structure_constraint_complex_left_branch.jsonl',\n",
       " 'left_branch_island_simple_question.jsonl',\n",
       " 'determiner_noun_agreement_with_adj_irregular_2.jsonl',\n",
       " 'determiner_noun_agreement_2.jsonl',\n",
       " 'wh_vs_that_with_gap.jsonl',\n",
       " 'regular_plural_subject_verb_agreement_1.jsonl',\n",
       " 'passive_2.jsonl',\n",
       " 'anaphor_number_agreement.jsonl',\n",
       " 'animate_subject_trans.jsonl',\n",
       " 'intransitive.jsonl',\n",
       " 'irregular_past_participle_verbs.jsonl',\n",
       " 'only_npi_licensor_present.jsonl',\n",
       " 'passive_1.jsonl',\n",
       " 'sentential_subject_island.jsonl',\n",
       " 'regular_plural_subject_verb_agreement_2.jsonl',\n",
       " 'principle_A_c_command.jsonl',\n",
       " 'wh_questions_subject_gap.jsonl',\n",
       " 'sentential_negation_npi_licensor_present.jsonl',\n",
       " 'principle_A_reconstruction.jsonl',\n",
       " 'determiner_noun_agreement_with_adjective_1.jsonl',\n",
       " 'drop_argument.jsonl',\n",
       " 'existential_there_object_raising.jsonl',\n",
       " 'irregular_past_participle_adjectives.jsonl',\n",
       " 'adjunct_island.jsonl',\n",
       " 'matrix_question_npi_licensor_present.jsonl',\n",
       " 'distractor_agreement_relational_noun.jsonl',\n",
       " 'complex_NP_island.jsonl',\n",
       " 'wh_questions_subject_gap_long_distance.jsonl',\n",
       " 'determiner_noun_agreement_1.jsonl',\n",
       " 'determiner_noun_agreement_with_adj_irregular_1.jsonl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../blimp/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.4905, -6.7212, -6.6658, -6.3376], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = torch.cat(\n",
    "    (\n",
    "        batch.input_ids[:, 1:],\n",
    "        torch.tensor([[tokenizer.eos_token_id]]).expand(\n",
    "            batch.input_ids.shape[0], 1\n",
    "        ),\n",
    "    ),\n",
    "    dim=-1,\n",
    ")\n",
    "logprobs = torch.gather(\n",
    "    torch.nn.functional.log_softmax(output.logits, dim=-1),\n",
    "    dim=-1,\n",
    "    index=targets.unsqueeze(1),\n",
    ").reshape(-1)\n",
    "logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(logprobs > logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256],\n",
       "        [50256],\n",
       "        [50256],\n",
       "        [50256]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[tokenizer.eos_token_id]]).expand(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92989cfd2446428999e353de76b84785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaConfig {\n",
       "  \"_name_or_path\": \"google/gemma-2b\",\n",
       "  \"architectures\": [\n",
       "    \"GemmaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"head_dim\": 256,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 16384,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"model_type\": \"gemma\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 18,\n",
       "  \"num_key_value_heads\": 1,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 256000\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModelForCausalLM.from_pretrained('google/gemma-2b').config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syntax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
